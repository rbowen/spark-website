<!DOCTYPE html ><html><head><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" name="viewport"/><title>Spark 4.0.0-preview1 ScalaDoc  - org.apache.spark.sql.Dataset</title><meta content="Spark 4.0.0 - preview1 ScalaDoc - org.apache.spark.sql.Dataset" name="description"/><meta content="Spark 4.0.0 preview1 ScalaDoc org.apache.spark.sql.Dataset" name="keywords"/><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><link href="../../../../lib/index.css" media="screen" type="text/css" rel="stylesheet"/><link href="../../../../lib/template.css" media="screen" type="text/css" rel="stylesheet"/><link href="../../../../lib/print.css" media="print" type="text/css" rel="stylesheet"/><link href="../../../../lib/diagrams.css" media="screen" type="text/css" rel="stylesheet" id="diagrams-css"/><script type="text/javascript" src="../../../../lib/jquery.min.js"></script><script type="text/javascript" src="../../../../lib/index.js"></script><script type="text/javascript" src="../../../../index.js"></script><script type="text/javascript" src="../../../../lib/scheduler.js"></script><script type="text/javascript" src="../../../../lib/template.js"></script><script type="text/javascript">/* this variable can be used by the JS to determine the path to the root document */
var toRoot = '../../../../';</script></head><body><div id="search"><span id="doc-title">Spark 4.0.0-preview1 ScalaDoc<span id="doc-version"></span></span> <span class="close-results"><span class="left">&lt;</span> Back</span><div id="textfilter"><span class="input"><input autocapitalize="none" placeholder="Search" id="index-input" type="text" accesskey="/"/><i class="clear material-icons"></i><i id="search-icon" class="material-icons"></i></span></div></div><div id="search-results"><div id="search-progress"><div id="progress-fill"></div></div><div id="results-content"><div id="entity-results"></div><div id="member-results"></div></div></div><div id="content-scroll-container" style="-webkit-overflow-scrolling: touch;"><div id="content-container" style="-webkit-overflow-scrolling: touch;"><div id="subpackage-spacer"><div id="packages"><h1>Packages</h1><ul><li class="indented0 " name="_root_.root" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="_root_" class="anchorToMember"></a><a id="root:_root_" class="anchorToMember"></a> <span class="permalink"><a href="../../../../index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../../../../index.html" title=""><span class="name">root</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../../../index.html" name="_root_" id="_root_" class="extype">root</a></dd></dl></div></li><li class="indented1 " name="_root_.org" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="org" class="anchorToMember"></a><a id="org:org" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../../../index.html" title=""><span class="name">org</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../../../index.html" name="_root_" id="_root_" class="extype">root</a></dd></dl></div></li><li class="indented2 " name="org.apache" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="apache" class="anchorToMember"></a><a id="apache:apache" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../../index.html" title=""><span class="name">apache</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../../index.html" name="org" id="org" class="extype">org</a></dd></dl></div></li><li class="indented3 " name="org.apache.spark" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="spark" class="anchorToMember"></a><a id="spark:spark" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../index.html" title="Core Spark functionality."><span class="name">spark</span></a></span><p class="shortcomment cmt">Core Spark functionality.</p><div class="fullcomment"><div class="comment cmt"><p>Core Spark functionality. <a href="../SparkContext.html" name="org.apache.spark.SparkContext" id="org.apache.spark.SparkContext" class="extype">org.apache.spark.SparkContext</a> serves as the main entry point to
Spark, while <a href="../rdd/RDD.html" name="org.apache.spark.rdd.RDD" id="org.apache.spark.rdd.RDD" class="extype">org.apache.spark.rdd.RDD</a> is the data type representing a distributed collection,
and provides most parallel operations.</p><p>In addition, <a href="../rdd/PairRDDFunctions.html" name="org.apache.spark.rdd.PairRDDFunctions" id="org.apache.spark.rdd.PairRDDFunctions" class="extype">org.apache.spark.rdd.PairRDDFunctions</a> contains operations available only on RDDs
of key-value pairs, such as <code>groupByKey</code> and <code>join</code>; <a href="../rdd/DoubleRDDFunctions.html" name="org.apache.spark.rdd.DoubleRDDFunctions" id="org.apache.spark.rdd.DoubleRDDFunctions" class="extype">org.apache.spark.rdd.DoubleRDDFunctions</a>
contains operations available only on RDDs of Doubles; and
<a href="../rdd/SequenceFileRDDFunctions.html" name="org.apache.spark.rdd.SequenceFileRDDFunctions" id="org.apache.spark.rdd.SequenceFileRDDFunctions" class="extype">org.apache.spark.rdd.SequenceFileRDDFunctions</a> contains operations available on RDDs that can
be saved as SequenceFiles. These operations are automatically available on any RDD of the right
type (e.g. RDD[(Int, Int)] through implicit conversions.</p><p>Java programmers should reference the <a href="../api/java/index.html" name="org.apache.spark.api.java" id="org.apache.spark.api.java" class="extype">org.apache.spark.api.java</a> package
for Spark programming APIs in Java.</p><p>Classes and methods marked with <span class="experimental badge" style="float: none;">
Experimental</span> are user-facing features which have not been officially adopted by the
Spark project. These are subject to change or removal in minor releases.</p><p>Classes and methods marked with <span class="developer badge" style="float: none;">
Developer API</span> are intended for advanced users want to extend Spark through lower
level interfaces. These are subject to changes or removal in minor releases.
</p></div><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../index.html" name="org.apache" id="org.apache" class="extype">apache</a></dd></dl></div></li><li class="indented4 " name="org.apache.spark.sql" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="sql" class="anchorToMember"></a><a id="sql:sql" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="index.html" title="Allows the execution of relational queries, including those expressed in SQL using Spark."><span class="name">sql</span></a></span><p class="shortcomment cmt">Allows the execution of relational queries, including those expressed in SQL using Spark.</p><div class="fullcomment"><div class="comment cmt"><p>Allows the execution of relational queries, including those expressed in SQL using Spark.
</p></div><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../index.html" name="org.apache.spark" id="org.apache.spark" class="extype">spark</a></dd></dl></div></li><li class="indented5 " name="org.apache.spark.sql.api" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="api" class="anchorToMember"></a><a id="api:api" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/api/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="api/index.html" title="Contains API classes that are specific to a single language (i.e."><span class="name">api</span></a></span><p class="shortcomment cmt">Contains API classes that are specific to a single language (i.e.</p><div class="fullcomment"><div class="comment cmt"><p>Contains API classes that are specific to a single language (i.e. Java).
</p></div><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a></dd></dl></div></li><li class="indented5 " name="org.apache.spark.sql.artifact" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="artifact" class="anchorToMember"></a><a id="artifact:artifact" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/artifact/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="artifact/index.html" title=""><span class="name">artifact</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a></dd></dl></div></li><li class="indented5 " name="org.apache.spark.sql.avro" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="avro" class="anchorToMember"></a><a id="avro:avro" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/avro/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="avro/index.html" title=""><span class="name">avro</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a></dd></dl></div></li><li class="indented5 " name="org.apache.spark.sql.catalog" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="catalog" class="anchorToMember"></a><a id="catalog:catalog" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/catalog/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="catalog/index.html" title=""><span class="name">catalog</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a></dd></dl></div></li><li class="indented5 " name="org.apache.spark.sql.catalyst" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="catalyst" class="anchorToMember"></a><a id="catalyst:catalyst" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/catalyst/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="catalyst/index.html" title=""><span class="name">catalyst</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a></dd></dl></div></li><li class="indented5 " name="org.apache.spark.sql.columnar" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="columnar" class="anchorToMember"></a><a id="columnar:columnar" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/columnar/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="columnar/index.html" title=""><span class="name">columnar</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a></dd></dl></div></li><li class="indented5 " name="org.apache.spark.sql.connector" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="connector" class="anchorToMember"></a><a id="connector:connector" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/connector/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="connector/index.html" title=""><span class="name">connector</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a></dd></dl></div></li><li class="indented5 " name="org.apache.spark.sql.expressions" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="expressions" class="anchorToMember"></a><a id="expressions:expressions" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/expressions/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="expressions/index.html" title=""><span class="name">expressions</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a></dd></dl></div></li><li class="indented5 " name="org.apache.spark.sql.jdbc" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="jdbc" class="anchorToMember"></a><a id="jdbc:jdbc" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/jdbc/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="jdbc/index.html" title=""><span class="name">jdbc</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a></dd></dl></div></li><li class="indented5 " name="org.apache.spark.sql.sources" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="sources" class="anchorToMember"></a><a id="sources:sources" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/sources/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="sources/index.html" title="A set of APIs for adding data sources to Spark SQL."><span class="name">sources</span></a></span><p class="shortcomment cmt">A set of APIs for adding data sources to Spark SQL.</p><div class="fullcomment"><div class="comment cmt"><p>A set of APIs for adding data sources to Spark SQL.
</p></div><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a></dd></dl></div></li><li class="indented5 " name="org.apache.spark.sql.streaming" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="streaming" class="anchorToMember"></a><a id="streaming:streaming" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/streaming/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="streaming/index.html" title=""><span class="name">streaming</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a></dd></dl></div></li><li class="indented5 " name="org.apache.spark.sql.types" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="types" class="anchorToMember"></a><a id="types:types" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/types/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="types/index.html" title="Contains a type system for attributes produced by relations, including complex types like structs, arrays and maps."><span class="name">types</span></a></span><p class="shortcomment cmt">Contains a type system for attributes produced by relations, including complex types like
structs, arrays and maps.</p><div class="fullcomment"><div class="comment cmt"><p>Contains a type system for attributes produced by relations, including complex types like
structs, arrays and maps.
</p></div><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a></dd></dl></div></li><li class="indented5 " name="org.apache.spark.sql.util" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="util" class="anchorToMember"></a><a id="util:util" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/util/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="util/index.html" title=""><span class="name">util</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a></dd></dl></div></li><li class="indented5 " name="org.apache.spark.sql.vectorized" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="vectorized" class="anchorToMember"></a><a id="vectorized:vectorized" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/vectorized/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="vectorized/index.html" title=""><span class="name">vectorized</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a></dd></dl></div></li><li class="current-entities indented4"><span class="separator"></span> <a href="AnalysisException.html" title="Thrown when a query fails to analyze, usually because the query itself is invalid." class="class"></a><a href="AnalysisException.html" title="Thrown when a query fails to analyze, usually because the query itself is invalid.">AnalysisException</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="Column.html" title="A column that will be computed based on the data in a DataFrame." class="class"></a><a href="Column.html" title="A column that will be computed based on the data in a DataFrame.">Column</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="ColumnName.html" title="A convenient class used for constructing schema." class="class"></a><a href="ColumnName.html" title="A convenient class used for constructing schema.">ColumnName</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="CreateTableWriter.html" title="Trait to restrict calls to create and replace operations." class="trait"></a><a href="CreateTableWriter.html" title="Trait to restrict calls to create and replace operations.">CreateTableWriter</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="DataFrameNaFunctions.html" title="Functionality for working with missing data in DataFrames." class="class"></a><a href="DataFrameNaFunctions.html" title="Functionality for working with missing data in DataFrames.">DataFrameNaFunctions</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="DataFrameReader.html" title="Interface used to load a Dataset from external storage systems (e.g." class="class"></a><a href="DataFrameReader.html" title="Interface used to load a Dataset from external storage systems (e.g.">DataFrameReader</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="DataFrameStatFunctions.html" title="Statistic functions for DataFrames." class="class"></a><a href="DataFrameStatFunctions.html" title="Statistic functions for DataFrames.">DataFrameStatFunctions</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="DataFrameWriter.html" title="Interface used to write a Dataset to external storage systems (e.g." class="class"></a><a href="DataFrameWriter.html" title="Interface used to write a Dataset to external storage systems (e.g.">DataFrameWriter</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="DataFrameWriterV2.html" title="Interface used to write a org.apache.spark.sql.Dataset to external storage using the v2 API." class="class"></a><a href="DataFrameWriterV2.html" title="Interface used to write a org.apache.spark.sql.Dataset to external storage using the v2 API.">DataFrameWriterV2</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="DataSourceRegistration.html" title="Functions for registering user-defined data sources." class="class"></a><a href="DataSourceRegistration.html" title="Functions for registering user-defined data sources.">DataSourceRegistration</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="" title="A Dataset is a strongly typed collection of domain-specific objects that can be transformed in parallel using functional or relational operations." class="class"></a><a href="" title="A Dataset is a strongly typed collection of domain-specific objects that can be transformed in parallel using functional or relational operations.">Dataset</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="DatasetHolder.html" title="A container for a Dataset, used for implicit conversions in Scala." class="class"></a><a href="DatasetHolder.html" title="A container for a Dataset, used for implicit conversions in Scala.">DatasetHolder</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="Encoder.html" title="Used to convert a JVM object of type T to and from the internal Spark SQL representation." class="trait"></a><a href="Encoder.html" title="Used to convert a JVM object of type T to and from the internal Spark SQL representation.">Encoder</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="Encoders$.html" title="Methods for creating an Encoder." class="object"></a><a href="Encoders$.html" title="Methods for creating an Encoder.">Encoders</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="ExperimentalMethods.html" title=":: Experimental :: Holder for experimental methods for the bravest." class="class"></a><a href="ExperimentalMethods.html" title=":: Experimental :: Holder for experimental methods for the bravest.">ExperimentalMethods</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="ExtendedExplainGenerator.html" title="A trait for a session extension to implement that provides addition explain plan information." class="trait"></a><a href="ExtendedExplainGenerator.html" title="A trait for a session extension to implement that provides addition explain plan information.">ExtendedExplainGenerator</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="ForeachWriter.html" title="The abstract class for writing custom logic to process data generated by a query." class="class"></a><a href="ForeachWriter.html" title="The abstract class for writing custom logic to process data generated by a query.">ForeachWriter</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="KeyValueGroupedDataset.html" title="A Dataset has been logically grouped by a user specified grouping key." class="class"></a><a href="KeyValueGroupedDataset.html" title="A Dataset has been logically grouped by a user specified grouping key.">KeyValueGroupedDataset</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="LowPrioritySQLImplicits.html" title="Lower priority implicit methods for converting Scala objects into Datasets." class="trait"></a><a href="LowPrioritySQLImplicits.html" title="Lower priority implicit methods for converting Scala objects into Datasets.">LowPrioritySQLImplicits</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="MergeIntoWriter.html" title="MergeIntoWriter provides methods to define and execute merge actions based on specified conditions." class="class"></a><a href="MergeIntoWriter.html" title="MergeIntoWriter provides methods to define and execute merge actions based on specified conditions.">MergeIntoWriter</a></li><li class="current-entities indented4"><a href="Observation$.html" title="(Scala-specific) Create instances of Observation via Scala apply." class="object"></a> <a href="Observation.html" title="Helper class to simplify usage of Dataset.observe(String, Column, Column*):" class="class"></a><a href="Observation.html" title="Helper class to simplify usage of Dataset.observe(String, Column, Column*):">Observation</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="ObservationBase.html" title="Helper class to simplify usage of Dataset.observe(String, Column, Column*):" class="class"></a><a href="ObservationBase.html" title="Helper class to simplify usage of Dataset.observe(String, Column, Column*):">ObservationBase</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="RelationalGroupedDataset.html" title="A set of methods for aggregations on a DataFrame, created by groupBy, cube or rollup (and also pivot)." class="class"></a><a href="RelationalGroupedDataset.html" title="A set of methods for aggregations on a DataFrame, created by groupBy, cube or rollup (and also pivot).">RelationalGroupedDataset</a></li><li class="current-entities indented4"><a href="Row$.html" title="" class="object"></a> <a href="Row.html" title="Represents one row of output from a relational operator." class="trait"></a><a href="Row.html" title="Represents one row of output from a relational operator.">Row</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="RowFactory.html" title="A factory class used to construct Row objects." class="class"></a><a href="RowFactory.html" title="A factory class used to construct Row objects.">RowFactory</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="RuntimeConfig.html" title="Runtime configuration interface for Spark." class="class"></a><a href="RuntimeConfig.html" title="Runtime configuration interface for Spark.">RuntimeConfig</a></li><li class="current-entities indented4"><a href="SQLContext$.html" title="This SQLContext object contains utility functions to create a singleton SQLContext instance, or to get the created SQLContext instance." class="object"></a> <a href="SQLContext.html" title="The entry point for working with structured data (rows and columns) in Spark 1.x." class="class"></a><a href="SQLContext.html" title="The entry point for working with structured data (rows and columns) in Spark 1.x.">SQLContext</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="SQLImplicits.html" title="A collection of implicit methods for converting common Scala objects into Datasets." class="class"></a><a href="SQLImplicits.html" title="A collection of implicit methods for converting common Scala objects into Datasets.">SQLImplicits</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="SaveMode.html" title="SaveMode is used to specify the expected behavior of saving a DataFrame to a data source." class="class"></a><a href="SaveMode.html" title="SaveMode is used to specify the expected behavior of saving a DataFrame to a data source.">SaveMode</a></li><li class="current-entities indented4"><a href="SparkSession$.html" title="" class="object"></a> <a href="SparkSession.html" title="The entry point to programming Spark with the Dataset and DataFrame API." class="class"></a><a href="SparkSession.html" title="The entry point to programming Spark with the Dataset and DataFrame API.">SparkSession</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="SparkSessionExtensions.html" title=":: Experimental :: Holder for injection points to the SparkSession." class="class"></a><a href="SparkSessionExtensions.html" title=":: Experimental :: Holder for injection points to the SparkSession.">SparkSessionExtensions</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="SparkSessionExtensionsProvider.html" title=":: Unstable ::" class="trait"></a><a href="SparkSessionExtensionsProvider.html" title=":: Unstable ::">SparkSessionExtensionsProvider</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="TypedColumn.html" title="A Column where an Encoder has been given for the expected input and return type." class="class"></a><a href="TypedColumn.html" title="A Column where an Encoder has been given for the expected input and return type.">TypedColumn</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="UDFRegistration.html" title="Functions for registering user-defined functions." class="class"></a><a href="UDFRegistration.html" title="Functions for registering user-defined functions.">UDFRegistration</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="UDTFRegistration.html" title="Functions for registering user-defined table functions." class="class"></a><a href="UDTFRegistration.html" title="Functions for registering user-defined table functions.">UDTFRegistration</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="WhenMatched.html" title="A class for defining actions to be taken when matching rows in a DataFrame during a merge operation." class="class"></a><a href="WhenMatched.html" title="A class for defining actions to be taken when matching rows in a DataFrame during a merge operation.">WhenMatched</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="WhenNotMatched.html" title="A class for defining actions to be taken when no matching rows are found in a DataFrame during a merge operation." class="class"></a><a href="WhenNotMatched.html" title="A class for defining actions to be taken when no matching rows are found in a DataFrame during a merge operation.">WhenNotMatched</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="WhenNotMatchedBySource.html" title="A class for defining actions to be performed when there is no match by source during a merge operation in a MergeIntoWriter." class="class"></a><a href="WhenNotMatchedBySource.html" title="A class for defining actions to be performed when there is no match by source during a merge operation in a MergeIntoWriter.">WhenNotMatchedBySource</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="WriteConfigMethods.html" title="Configuration methods common to create/replace operations and insert/overwrite operations." class="trait"></a><a href="WriteConfigMethods.html" title="Configuration methods common to create/replace operations and insert/overwrite operations.">WriteConfigMethods</a></li><li class="current-entities indented4"><span class="separator"></span> <a href="functions$.html" title="Commonly used functions available for DataFrame operations." class="object"></a><a href="functions$.html" title="Commonly used functions available for DataFrame operations.">functions</a></li></ul></div></div><div id="content"><body class="class type"><div id="definition"><div class="big-circle class">c</div><p id="owner"><a href="../../../index.html" name="org" id="org" class="extype">org</a>.<a href="../../index.html" name="org.apache" id="org.apache" class="extype">apache</a>.<a href="../index.html" name="org.apache.spark" id="org.apache.spark" class="extype">spark</a>.<a href="index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a></p><h1>Dataset<span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html" title="Permalink"><i class="material-icons"></i></a></span></h1><h3><span class="morelinks"></span></h3></div><h4 id="signature" class="signature"><span class="modifier_kind"><span class="modifier"></span> <span class="kind">class</span></span> <span class="symbol"><span class="name">Dataset</span><span class="tparams">[<span name="T">T</span>]</span><span class="result"> extends <span name="scala.Serializable" class="extype">Serializable</span></span></span></h4><div id="comment" class="fullcommenttop"><div class="comment cmt"><p>A Dataset is a strongly typed collection of domain-specific objects that can be transformed
in parallel using functional or relational operations. Each Dataset also has an untyped view
called a <code>DataFrame</code>, which is a Dataset of <a href="Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>.</p><p>Operations available on Datasets are divided into transformations and actions. Transformations
are the ones that produce new Datasets, and actions are the ones that trigger computation and
return results. Example transformations include map, filter, select, and aggregate (<code>groupBy</code>).
Example actions count, show, or writing data out to file systems.</p><p>Datasets are "lazy", i.e. computations are only triggered when an action is invoked. Internally,
a Dataset represents a logical plan that describes the computation required to produce the data.
When an action is invoked, Spark's query optimizer optimizes the logical plan and generates a
physical plan for efficient execution in a parallel and distributed manner. To explore the
logical plan as well as optimized physical plan, use the <code>explain</code> function.</p><p>To efficiently support domain-specific objects, an <a href="Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a> is required. The encoder maps
the domain specific type <code>T</code> to Spark's internal type system. For example, given a class <code>Person</code>
with two fields, <code>name</code> (string) and <code>age</code> (int), an encoder is used to tell Spark to generate
code at runtime to serialize the <code>Person</code> object into a binary structure. This binary structure
often has much lower memory footprint as well as are optimized for efficiency in data processing
(e.g. in a columnar format). To understand the internal binary representation for data, use the
<code>schema</code> function.</p><p>There are typically two ways to create a Dataset. The most common way is by pointing Spark
to some files on storage systems, using the <code>read</code> function available on a <code>SparkSession</code>.</p><pre><span class="kw">val</span> people = spark.read.parquet(<span class="lit">"..."</span>).as[Person]  <span class="cmt">// Scala</span>
Dataset&lt;Person&gt; people = spark.read().parquet(<span class="lit">"..."</span>).as(Encoders.bean(Person.<span class="kw">class</span>)); <span class="cmt">// Java</span></pre><p>Datasets can also be created through transformations available on existing Datasets. For example,
the following creates a new Dataset by applying a filter on the existing one:</p><pre><span class="kw">val</span> names = people.map(_.name)  <span class="cmt">// in Scala; names is a Dataset[String]</span>
Dataset&lt;<span class="std">String</span>&gt; names = people.map(
  (MapFunction&lt;Person, <span class="std">String</span>&gt;) p -&gt; p.name, Encoders.STRING()); <span class="cmt">// Java</span></pre><p>Dataset operations can also be untyped, through various domain-specific-language (DSL)
functions defined in: Dataset (this class), <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>, and <a href="functions$.html" name="org.apache.spark.sql.functions" id="org.apache.spark.sql.functions" class="extype">functions</a>. These operations
are very similar to the operations available in the data frame abstraction in R or Python.</p><p>To select a column from the Dataset, use <code>apply</code> method in Scala and <code>col</code> in Java.</p><pre><span class="kw">val</span> ageCol = people(<span class="lit">"age"</span>)  <span class="cmt">// in Scala</span>
Column ageCol = people.col(<span class="lit">"age"</span>); <span class="cmt">// in Java</span></pre><p>Note that the <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a> type can also be manipulated through its various functions.</p><pre><span class="cmt">// The following creates a new column that increases everybody's age by 10.</span>
people(<span class="lit">"age"</span>) + <span class="num">10</span>  <span class="cmt">// in Scala</span>
people.col(<span class="lit">"age"</span>).plus(<span class="num">10</span>);  <span class="cmt">// in Java</span></pre><p>A more concrete example in Scala:</p><pre><span class="cmt">// To create Dataset[Row] using SparkSession</span>
<span class="kw">val</span> people = spark.read.parquet(<span class="lit">"..."</span>)
<span class="kw">val</span> department = spark.read.parquet(<span class="lit">"..."</span>)

people.filter(<span class="lit">"age &gt; 30"</span>)
  .join(department, people(<span class="lit">"deptId"</span>) === department(<span class="lit">"id"</span>))
  .groupBy(department(<span class="lit">"name"</span>), people(<span class="lit">"gender"</span>))
  .agg(avg(people(<span class="lit">"salary"</span>)), max(people(<span class="lit">"age"</span>)))</pre><p>and in Java:</p><pre><span class="cmt">// To create Dataset&lt;Row&gt; using SparkSession</span>
Dataset&lt;Row&gt; people = spark.read().parquet(<span class="lit">"..."</span>);
Dataset&lt;Row&gt; department = spark.read().parquet(<span class="lit">"..."</span>);

people.filter(people.col(<span class="lit">"age"</span>).gt(<span class="num">30</span>))
  .join(department, people.col(<span class="lit">"deptId"</span>).equalTo(department.col(<span class="lit">"id"</span>)))
  .groupBy(department.col(<span class="lit">"name"</span>), people.col(<span class="lit">"gender"</span>))
  .agg(avg(people.col(<span class="lit">"salary"</span>)), max(people.col(<span class="lit">"age"</span>)));</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Stable</span><span class="args">()</span> </dd><dt>Source</dt><dd><a href="https://github.com/apache/spark/tree/v4.0.0-preview1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala" target="_blank">Dataset.scala</a></dd><dt>Since</dt><dd><p>1.6.0</p></dd></dl><div class="toggleContainer"><div class="toggle block"><span>Linear Supertypes</span><div class="superTypes hiddenContent"><a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/io/Serializable.html#java.io.Serializable" name="java.io.Serializable" id="java.io.Serializable" class="extype">Serializable</a>, <span name="scala.AnyRef" class="extype">AnyRef</span>, <span name="scala.Any" class="extype">Any</span></div></div></div></div><div id="mbrsel"><div class="toggle"></div><div id="memberfilter"><i class="material-icons arrow"></i><span class="input"><input placeholder="Filter all members" id="mbrsel-input" type="text" accesskey="/"/></span><i class="clear material-icons"></i></div><div id="filterby"><div id="order"><span class="filtertype">Ordering</span><ol><li class="group out"><span>Grouped</span></li><li class="alpha in"><span>Alphabetic</span></li><li class="inherit out"><span>By Inheritance</span></li></ol></div><div class="ancestors"><span class="filtertype">Inherited<br/></span><ol id="linearization"><li class="in" name="org.apache.spark.sql.Dataset"><span>Dataset</span></li><li class="in" name="java.io.Serializable"><span>Serializable</span></li><li class="in" name="scala.AnyRef"><span>AnyRef</span></li><li class="in" name="scala.Any"><span>Any</span></li></ol></div><div class="ancestors"><span class="filtertype"></span><ol><li class="hideall out"><span>Hide All</span></li><li class="showall in"><span>Show All</span></li></ol></div><div id="visbl"><span class="filtertype">Visibility</span><ol><li class="public in"><span>Public</span></li><li class="protected out"><span>Protected</span></li></ol></div></div></div><div id="template"><div id="allMembers"><div id="constructors" class="members"><h3>Instance Constructors</h3><ol><li class="indented0 " name="org.apache.spark.sql.Dataset#&lt;init&gt;" group="Ungrouped" fullComment="no" data-isabs="false" visbl="pub"><a id="&lt;init&gt;(sqlContext:org.apache.spark.sql.SQLContext,logicalPlan:org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,encoder:org.apache.spark.sql.Encoder[T]):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="&lt;init&gt;:Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#&lt;init&gt;(sqlContext:org.apache.spark.sql.SQLContext,logicalPlan:org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,encoder:org.apache.spark.sql.Encoder[T]):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">new</span></span> <span class="symbol"><span class="name">Dataset</span><span class="params">(<span name="sqlContext">sqlContext: <a href="SQLContext.html" name="org.apache.spark.sql.SQLContext" id="org.apache.spark.sql.SQLContext" class="extype">SQLContext</a></span>, <span name="logicalPlan">logicalPlan: <span name="org.apache.spark.sql.catalyst.plans.logical.LogicalPlan" class="extype">LogicalPlan</span></span>, <span name="encoder">encoder: <a href="Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span>)</span></span></li><li class="indented0 " name="org.apache.spark.sql.Dataset#&lt;init&gt;" group="Ungrouped" fullComment="no" data-isabs="false" visbl="pub"><a id="&lt;init&gt;(sparkSession:org.apache.spark.sql.SparkSession,logicalPlan:org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,encoder:org.apache.spark.sql.Encoder[T]):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="&lt;init&gt;:Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#&lt;init&gt;(sparkSession:org.apache.spark.sql.SparkSession,logicalPlan:org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,encoder:org.apache.spark.sql.Encoder[T]):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">new</span></span> <span class="symbol"><span class="name">Dataset</span><span class="params">(<span name="sparkSession">sparkSession: <a href="SparkSession.html" name="org.apache.spark.sql.SparkSession" id="org.apache.spark.sql.SparkSession" class="extype">SparkSession</a></span>, <span name="logicalPlan">logicalPlan: <span name="org.apache.spark.sql.catalyst.plans.logical.LogicalPlan" class="extype">LogicalPlan</span></span>, <span name="encoder">encoder: <a href="Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span>)</span></span></li></ol></div><div class="values members"><h3>Value Members</h3><ol><li class="indented0 " name="scala.AnyRef#!=" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="!=(x$1:Any):Boolean" class="anchorToMember"></a><a id="!=(Any):Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#!=(x$1:Any):Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name" title="gt4s: $bang$eq">!=</span><span class="params">(<span name="arg0">arg0: <span name="scala.Any" class="extype">Any</span></span>)</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div></li><li class="indented0 " name="scala.AnyRef###" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="##:Int" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html###:Int" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name" title="gt4s: $hash$hash">##</span><span class="result">: <span name="scala.Int" class="extype">Int</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div></li><li class="indented0 " name="scala.AnyRef#==" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="==(x$1:Any):Boolean" class="anchorToMember"></a><a id="==(Any):Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#==(x$1:Any):Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name" title="gt4s: $eq$eq">==</span><span class="params">(<span name="arg0">arg0: <span name="scala.Any" class="extype">Any</span></span>)</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#agg" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="agg(expr:org.apache.spark.sql.Column,exprs:org.apache.spark.sql.Column*):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="agg(Column,Column*):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#agg(expr:org.apache.spark.sql.Column,exprs:org.apache.spark.sql.Column*):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">agg</span><span class="params">(<span name="expr">expr: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>, <span name="exprs">exprs: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Aggregates on the entire Dataset without groups.</p><div class="fullcomment"><div class="comment cmt"><p>Aggregates on the entire Dataset without groups.</p><pre><span class="cmt">// ds.agg(...) is a shorthand for ds.groupBy().agg(...)</span>
ds.agg(max($<span class="lit">"age"</span>), avg($<span class="lit">"salary"</span>))
ds.groupBy().agg(max($<span class="lit">"age"</span>), avg($<span class="lit">"salary"</span>))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#agg" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="agg(exprs:java.util.Map[String,String]):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="agg(Map[String,String]):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#agg(exprs:java.util.Map[String,String]):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">agg</span><span class="params">(<span name="exprs">exprs: <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/Map.html#java.util.Map" name="java.util.Map" id="java.util.Map" class="extype">Map</a>[<span name="scala.Predef.String" class="extype">String</span>, <span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">(Java-specific) Aggregates on the entire Dataset without groups.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific) Aggregates on the entire Dataset without groups.</p><pre><span class="cmt">// ds.agg(...) is a shorthand for ds.groupBy().agg(...)</span>
ds.agg(<span class="std">Map</span>(<span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>, <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>))
ds.groupBy().agg(<span class="std">Map</span>(<span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>, <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>))</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#agg" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="agg(exprs:Map[String,String]):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="agg(Map[String,String]):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#agg(exprs:Map[String,String]):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">agg</span><span class="params">(<span name="exprs">exprs: <span name="scala.Predef.Map" class="extype">Map</span>[<span name="scala.Predef.String" class="extype">String</span>, <span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">(Scala-specific) Aggregates on the entire Dataset without groups.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Aggregates on the entire Dataset without groups.</p><pre><span class="cmt">// ds.agg(...) is a shorthand for ds.groupBy().agg(...)</span>
ds.agg(<span class="std">Map</span>(<span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>, <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>))
ds.groupBy().agg(<span class="std">Map</span>(<span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>, <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>))</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#agg" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="agg(aggExpr:(String,String),aggExprs:(String,String)*):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="agg((String,String),(String,String)*):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#agg(aggExpr:(String,String),aggExprs:(String,String)*):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">agg</span><span class="params">(<span name="aggExpr">aggExpr: (<span name="scala.Predef.String" class="extype">String</span>, <span name="scala.Predef.String" class="extype">String</span>)</span>, <span name="aggExprs">aggExprs: (<span name="scala.Predef.String" class="extype">String</span>, <span name="scala.Predef.String" class="extype">String</span>)*</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">(Scala-specific) Aggregates on the entire Dataset without groups.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Aggregates on the entire Dataset without groups.</p><pre><span class="cmt">// ds.agg(...) is a shorthand for ds.groupBy().agg(...)</span>
ds.agg(<span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>, <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>)
ds.groupBy().agg(<span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>, <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>)</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#alias" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="alias(alias:Symbol):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="alias(Symbol):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#alias(alias:Symbol):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">alias</span><span class="params">(<span name="alias">alias: <span name="scala.Symbol" class="extype">Symbol</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">(Scala-specific) Returns a new Dataset with an alias set.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Returns a new Dataset with an alias set. Same as <code>as</code>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#alias" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="alias(alias:String):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="alias(String):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#alias(alias:String):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">alias</span><span class="params">(<span name="alias">alias: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset with an alias set.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with an alias set. Same as <code>as</code>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#apply" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="apply(colName:String):org.apache.spark.sql.Column" class="anchorToMember"></a><a id="apply(String):Column" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#apply(colName:String):org.apache.spark.sql.Column" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">apply</span><span class="params">(<span name="colName">colName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span></span><p class="shortcomment cmt">Selects column based on the column name and returns it as a <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>.</p><div class="fullcomment"><div class="comment cmt"><p>Selects column based on the column name and returns it as a <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>The column name can also reference to a nested column like <code>a.b</code>.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#as" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="as(alias:Symbol):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="as(Symbol):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#as(alias:Symbol):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">as</span><span class="params">(<span name="alias">alias: <span name="scala.Symbol" class="extype">Symbol</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">(Scala-specific) Returns a new Dataset with an alias set.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Returns a new Dataset with an alias set.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#as" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="as(alias:String):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="as(String):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#as(alias:String):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">as</span><span class="params">(<span name="alias">alias: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset with an alias set.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with an alias set.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#as" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="as[U](implicitevidence$2:org.apache.spark.sql.Encoder[U]):org.apache.spark.sql.Dataset[U]" class="anchorToMember"></a><a id="as[U](Encoder[U]):Dataset[U]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#as[U](implicitevidence$2:org.apache.spark.sql.Encoder[U]):org.apache.spark.sql.Dataset[U]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">as</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span class="implicit">implicit </span><span name="arg0">arg0: <a href="Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.Dataset.as.U" class="extype">U</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.as.U" class="extype">U</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset where each record has been mapped on to the specified type.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset where each record has been mapped on to the specified type. The
method used to map columns depend on the type of <code>U</code>:</p><ul><li>When <code>U</code> is a class, fields for the class will be mapped to columns of the same name
  (case sensitivity is determined by <code>spark.sql.caseSensitive</code>).</li><li>When <code>U</code> is a tuple, the columns will be mapped by ordinal (i.e. the first column will
  be assigned to <code>_1</code>).</li><li>When <code>U</code> is a primitive type (i.e. String, Int, etc), then the first column of the
  <code>DataFrame</code> will be used.</li></ul><p>If the schema of the Dataset does not match the desired <code>U</code> type, you can use <code>select</code>
along with <code>alias</code> or <code>as</code> to rearrange or rename as required.</p><p>Note that <code>as[]</code> only changes the view of the data that is passed into typed operations,
such as <code>map()</code>, and does not eagerly project away any columns that are not present in
the specified class.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="scala.Any#asInstanceOf" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="asInstanceOf[T0]:T0" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#asInstanceOf[T0]:T0" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">asInstanceOf</span><span class="tparams">[<span name="T0">T0</span>]</span><span class="result">: <span name="scala.Any.asInstanceOf.T0" class="extype">T0</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>Any</dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#cache" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="cache():Dataset.this.type" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#cache():Dataset.this.type" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">cache</span><span class="params">()</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>.this.type</span></span><p class="shortcomment cmt">Persist this Dataset with the default storage level (<code>MEMORY_AND_DISK</code>).</p><div class="fullcomment"><div class="comment cmt"><p>Persist this Dataset with the default storage level (<code>MEMORY_AND_DISK</code>).
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#checkpoint" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="checkpoint(eager:Boolean):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="checkpoint(Boolean):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#checkpoint(eager:Boolean):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">checkpoint</span><span class="params">(<span name="eager">eager: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a checkpointed version of this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the
logical plan of this Dataset, which is especially useful in iterative algorithms where the
plan may grow exponentially. It will be saved to files inside the checkpoint
directory set with <code>SparkContext#setCheckpointDir</code>.
</p></div><dl class="paramcmts block"><dt class="param">eager</dt><dd class="cmt"><p>Whether to checkpoint this dataframe immediately</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.1.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>When checkpoint is used with eager = false, the final data that is checkpointed after
      the first action may be different from the data that was used during the job due to
      non-determinism of the underlying operation and retries. If checkpoint is used to achieve
      saving a deterministic snapshot of the data, eager = true should be used. Otherwise,
      it is only deterministic after the first execution, after the checkpoint was finalized.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#checkpoint" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="checkpoint():org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="checkpoint():Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#checkpoint():org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">checkpoint</span><span class="params">()</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Eagerly checkpoint a Dataset and return the new Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate
the logical plan of this Dataset, which is especially useful in iterative algorithms where the
plan may grow exponentially. It will be saved to files inside the checkpoint
directory set with <code>SparkContext#setCheckpointDir</code>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.1.0</p></dd></dl></div></li><li class="indented0 " name="scala.AnyRef#clone" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="prt"><a id="clone():Object" class="anchorToMember"></a><a id="clone():AnyRef" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#clone():Object" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">clone</span><span class="params">()</span><span class="result">: <span name="scala.AnyRef" class="extype">AnyRef</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Attributes</dt><dd>protected[<span name="java.lang" class="extype">lang</span>] </dd><dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd><span class="name">@throws</span><span class="args">(<span><span class="defval">classOf[java.lang.CloneNotSupportedException]</span></span>)</span> <span class="name">@IntrinsicCandidate</span><span class="args">()</span> <span class="name">@native</span><span class="args">()</span> </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#coalesce" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="coalesce(numPartitions:Int):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="coalesce(Int):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#coalesce(numPartitions:Int):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">coalesce</span><span class="params">(<span name="numPartitions">numPartitions: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset that has exactly <code>numPartitions</code> partitions, when the fewer partitions
are requested.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset that has exactly <code>numPartitions</code> partitions, when the fewer partitions
are requested. If a larger number of partitions is requested, it will stay at the current
number of partitions. Similar to coalesce defined on an <code>RDD</code>, this operation results in
a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not
be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions.</p><p>However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,
this may result in your computation taking place on fewer nodes than
you like (e.g. one node in the case of numPartitions = 1). To avoid this,
you can call repartition. This will add a shuffle step, but means the
current upstream partitions will be executed in parallel (per whatever
the current partitioning is).
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#col" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="col(colName:String):org.apache.spark.sql.Column" class="anchorToMember"></a><a id="col(String):Column" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#col(colName:String):org.apache.spark.sql.Column" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">col</span><span class="params">(<span name="colName">colName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span></span><p class="shortcomment cmt">Selects column based on the column name and returns it as a <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>.</p><div class="fullcomment"><div class="comment cmt"><p>Selects column based on the column name and returns it as a <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>The column name can also reference to a nested column like <code>a.b</code>.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#colRegex" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="colRegex(colName:String):org.apache.spark.sql.Column" class="anchorToMember"></a><a id="colRegex(String):Column" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#colRegex(colName:String):org.apache.spark.sql.Column" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">colRegex</span><span class="params">(<span name="colName">colName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span></span><p class="shortcomment cmt">Selects column based on the column name specified as a regex and returns it as <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>.</p><div class="fullcomment"><div class="comment cmt"><p>Selects column based on the column name specified as a regex and returns it as <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>.</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#collect" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="collect():Array[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#collect():Array[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">collect</span><span class="params">()</span><span class="result">: <span name="scala.Array" class="extype">Array</span>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns an array that contains all rows in this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns an array that contains all rows in this Dataset.</p><p>Running collect requires moving all the data into the application's driver process, and
doing so on a very large dataset can crash the driver process with OutOfMemoryError.</p><p>For Java API, use <a href="#collectAsList():java.util.List[T]" name="org.apache.spark.sql.Dataset#collectAsList" id="org.apache.spark.sql.Dataset#collectAsList" class="extmbr">collectAsList</a>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#collectAsList" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="collectAsList():java.util.List[T]" class="anchorToMember"></a><a id="collectAsList():List[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#collectAsList():java.util.List[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">collectAsList</span><span class="params">()</span><span class="result">: <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/List.html#java.util.List" name="java.util.List" id="java.util.List" class="extype">List</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a Java list that contains all rows in this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a Java list that contains all rows in this Dataset.</p><p>Running collect requires moving all the data into the application's driver process, and
doing so on a very large dataset can crash the driver process with OutOfMemoryError.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#columns" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="columns:Array[String]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#columns:Array[String]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">columns</span><span class="result">: <span name="scala.Array" class="extype">Array</span>[<span name="scala.Predef.String" class="extype">String</span>]</span></span><p class="shortcomment cmt">Returns all column names as an array.</p><div class="fullcomment"><div class="comment cmt"><p>Returns all column names as an array.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#count" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="count():Long" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#count():Long" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">count</span><span class="params">()</span><span class="result">: <span name="scala.Long" class="extype">Long</span></span></span><p class="shortcomment cmt">Returns the number of rows in the Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the number of rows in the Dataset.</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#createGlobalTempView" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="createGlobalTempView(viewName:String):Unit" class="anchorToMember"></a><a id="createGlobalTempView(String):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#createGlobalTempView(viewName:String):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">createGlobalTempView</span><span class="params">(<span name="viewName">viewName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Creates a global temporary view using the given name.</p><div class="fullcomment"><div class="comment cmt"><p>Creates a global temporary view using the given name. The lifetime of this
temporary view is tied to this Spark application.</p><p>Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
i.e. it will be automatically dropped when the application terminates. It's tied to a system
preserved database <code>global_temp</code>, and we must use the qualified name to refer a global temp
view, e.g. <code>SELECT * FROM global_temp.view1</code>.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@throws</span><span class="args">(<span><span class="defval">scala.this.throws.&lt;init&gt;$default$1[org.apache.spark.sql.AnalysisException]</span></span>)</span> </dd><dt>Since</dt><dd><p>2.1.0</p></dd><dt>Exceptions thrown</dt><dd><span class="cmt"><p><a href="AnalysisException.html" name="org.apache.spark.sql.AnalysisException" id="org.apache.spark.sql.AnalysisException" class="extype"><code>AnalysisException</code></a> if the view name is invalid or already exists</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#createOrReplaceGlobalTempView" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="createOrReplaceGlobalTempView(viewName:String):Unit" class="anchorToMember"></a><a id="createOrReplaceGlobalTempView(String):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#createOrReplaceGlobalTempView(viewName:String):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">createOrReplaceGlobalTempView</span><span class="params">(<span name="viewName">viewName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Creates or replaces a global temporary view using the given name.</p><div class="fullcomment"><div class="comment cmt"><p>Creates or replaces a global temporary view using the given name. The lifetime of this
temporary view is tied to this Spark application.</p><p>Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
i.e. it will be automatically dropped when the application terminates. It's tied to a system
preserved database <code>global_temp</code>, and we must use the qualified name to refer a global temp
view, e.g. <code>SELECT * FROM global_temp.view1</code>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.2.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#createOrReplaceTempView" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="createOrReplaceTempView(viewName:String):Unit" class="anchorToMember"></a><a id="createOrReplaceTempView(String):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#createOrReplaceTempView(viewName:String):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">createOrReplaceTempView</span><span class="params">(<span name="viewName">viewName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Creates a local temporary view using the given name.</p><div class="fullcomment"><div class="comment cmt"><p>Creates a local temporary view using the given name. The lifetime of this
temporary view is tied to the <a href="SparkSession.html" name="org.apache.spark.sql.SparkSession" id="org.apache.spark.sql.SparkSession" class="extype">SparkSession</a> that was used to create this Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#createTempView" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="createTempView(viewName:String):Unit" class="anchorToMember"></a><a id="createTempView(String):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#createTempView(viewName:String):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">createTempView</span><span class="params">(<span name="viewName">viewName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Creates a local temporary view using the given name.</p><div class="fullcomment"><div class="comment cmt"><p>Creates a local temporary view using the given name. The lifetime of this
temporary view is tied to the <a href="SparkSession.html" name="org.apache.spark.sql.SparkSession" id="org.apache.spark.sql.SparkSession" class="extype">SparkSession</a> that was used to create this Dataset.</p><p>Local temporary view is session-scoped. Its lifetime is the lifetime of the session that
created it, i.e. it will be automatically dropped when the session terminates. It's not
tied to any databases, i.e. we can't use <code>db1.view1</code> to reference a local temporary view.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@throws</span><span class="args">(<span><span class="defval">scala.this.throws.&lt;init&gt;$default$1[org.apache.spark.sql.AnalysisException]</span></span>)</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Exceptions thrown</dt><dd><span class="cmt"><p><a href="AnalysisException.html" name="org.apache.spark.sql.AnalysisException" id="org.apache.spark.sql.AnalysisException" class="extype"><code>AnalysisException</code></a> if the view name is invalid or already exists</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#crossJoin" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="crossJoin(right:org.apache.spark.sql.Dataset[_]):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="crossJoin(Dataset[_]):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#crossJoin(right:org.apache.spark.sql.Dataset[_]):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">crossJoin</span><span class="params">(<span name="right">right: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[_]</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Explicit cartesian join with another <code>DataFrame</code>.</p><div class="fullcomment"><div class="comment cmt"><p>Explicit cartesian join with another <code>DataFrame</code>.
</p></div><dl class="paramcmts block"><dt class="param">right</dt><dd class="cmt"><p>Right side of the join operation.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.1.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>Cartesian joins are very expensive without an extra filter that can be pushed down.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#cube" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="cube(col1:String,cols:String*):org.apache.spark.sql.RelationalGroupedDataset" class="anchorToMember"></a><a id="cube(String,String*):RelationalGroupedDataset" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#cube(col1:String,cols:String*):org.apache.spark.sql.RelationalGroupedDataset" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">cube</span><span class="params">(<span name="col1">col1: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="cols">cols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.RelationalGroupedDataset" id="org.apache.spark.sql.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a></span></span><p class="shortcomment cmt">Create a multi-dimensional cube for the current Dataset using the specified columns,
so we can run aggregation on them.</p><div class="fullcomment"><div class="comment cmt"><p>Create a multi-dimensional cube for the current Dataset using the specified columns,
so we can run aggregation on them.
See <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.RelationalGroupedDataset" id="org.apache.spark.sql.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a> for all the available aggregate functions.</p><p>This is a variant of cube that can only group by existing columns using column names
(i.e. cannot construct expressions).</p><pre><span class="cmt">// Compute the average for all numeric columns cubed by department and group.</span>
ds.cube(<span class="lit">"department"</span>, <span class="lit">"group"</span>).avg()

<span class="cmt">// Compute the max age and average salary, cubed by department and gender.</span>
ds.cube($<span class="lit">"department"</span>, $<span class="lit">"gender"</span>).agg(<span class="std">Map</span>(
  <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>,
  <span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>
))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#cube" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="cube(cols:org.apache.spark.sql.Column*):org.apache.spark.sql.RelationalGroupedDataset" class="anchorToMember"></a><a id="cube(Column*):RelationalGroupedDataset" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#cube(cols:org.apache.spark.sql.Column*):org.apache.spark.sql.RelationalGroupedDataset" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">cube</span><span class="params">(<span name="cols">cols: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.RelationalGroupedDataset" id="org.apache.spark.sql.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a></span></span><p class="shortcomment cmt">Create a multi-dimensional cube for the current Dataset using the specified columns,
so we can run aggregation on them.</p><div class="fullcomment"><div class="comment cmt"><p>Create a multi-dimensional cube for the current Dataset using the specified columns,
so we can run aggregation on them.
See <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.RelationalGroupedDataset" id="org.apache.spark.sql.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a> for all the available aggregate functions.</p><pre><span class="cmt">// Compute the average for all numeric columns cubed by department and group.</span>
ds.cube($<span class="lit">"department"</span>, $<span class="lit">"group"</span>).avg()

<span class="cmt">// Compute the max age and average salary, cubed by department and gender.</span>
ds.cube($<span class="lit">"department"</span>, $<span class="lit">"gender"</span>).agg(<span class="std">Map</span>(
  <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>,
  <span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>
))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#describe" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="describe(cols:String*):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="describe(String*):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#describe(cols:String*):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">describe</span><span class="params">(<span name="cols">cols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Computes basic statistics for numeric and string columns, including count, mean, stddev, min,
and max.</p><div class="fullcomment"><div class="comment cmt"><p>Computes basic statistics for numeric and string columns, including count, mean, stddev, min,
and max. If no columns are given, this function computes statistics for all numerical or
string columns.</p><p>This function is meant for exploratory data analysis, as we make no guarantee about the
backward compatibility of the schema of the resulting Dataset. If you want to
programmatically compute summary statistics, use the <code>agg</code> function instead.</p><pre>ds.describe(<span class="lit">"age"</span>, <span class="lit">"height"</span>).show()

<span class="cmt">// output:</span>
<span class="cmt">// summary age   height</span>
<span class="cmt">// count   10.0  10.0</span>
<span class="cmt">// mean    53.3  178.05</span>
<span class="cmt">// stddev  11.6  15.7</span>
<span class="cmt">// min     18.0  163.0</span>
<span class="cmt">// max     92.0  192.0</span></pre><p>Use <a href="#summary(statistics:String*):org.apache.spark.sql.DataFrame" name="org.apache.spark.sql.Dataset#summary" id="org.apache.spark.sql.Dataset#summary" class="extmbr">summary</a> for expanded statistics and control over which statistics to compute.
</p></div><dl class="paramcmts block"><dt class="param">cols</dt><dd class="cmt"><p>Columns to compute statistics on.</p></dd></dl><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#distinct" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="distinct():org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="distinct():Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#distinct():org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">distinct</span><span class="params">()</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset that contains only the unique rows from this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset that contains only the unique rows from this Dataset.
This is an alias for <code>dropDuplicates</code>.</p><p>Note that for a streaming <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>, this method returns distinct rows only once
regardless of the output mode, which the behavior may not be same with <code>DISTINCT</code> in SQL
against streaming <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>Equality checking is performed directly on the encoded representation of the data
and thus is not affected by a custom <code>equals</code> function defined on <code>T</code>.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#drop" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="drop(col:org.apache.spark.sql.Column,cols:org.apache.spark.sql.Column*):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="drop(Column,Column*):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#drop(col:org.apache.spark.sql.Column,cols:org.apache.spark.sql.Column*):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">drop</span><span class="params">(<span name="col">col: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>, <span name="cols">cols: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Returns a new Dataset with columns dropped.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with columns dropped.</p><p>This method can only be used to drop top level columns.
This is a no-op if the Dataset doesn't have a columns
with an equivalent expression.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.4.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#drop" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="drop(col:org.apache.spark.sql.Column):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="drop(Column):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#drop(col:org.apache.spark.sql.Column):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">drop</span><span class="params">(<span name="col">col: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Returns a new Dataset with column dropped.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with column dropped.</p><p>This method can only be used to drop top level column.
This version of drop accepts a <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a> rather than a name.
This is a no-op if the Dataset doesn't have a column
with an equivalent expression.</p><p>Note: <code>drop(col(colName))</code> has different semantic with <code>drop(colName)</code>,
please refer to <code>Dataset#drop(colName: String)</code>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#drop" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="drop(colNames:String*):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="drop(String*):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#drop(colNames:String*):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">drop</span><span class="params">(<span name="colNames">colNames: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Returns a new Dataset with columns dropped.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with columns dropped.
This is a no-op if schema doesn't contain column name(s).</p><p>This method can only be used to drop top level columns. the colName string is treated literally
without further interpretation.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#drop" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="drop(colName:String):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="drop(String):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#drop(colName:String):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">drop</span><span class="params">(<span name="colName">colName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Returns a new Dataset with a column dropped.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain
column name.</p><p>This method can only be used to drop top level columns. the colName string is treated
literally without further interpretation.</p><p>Note: <code>drop(colName)</code> has different semantic with <code>drop(col(colName))</code>, for example:
1, multi column have the same colName:</p><pre><span class="kw">val</span> df1 = spark.range(<span class="num">0</span>, <span class="num">2</span>).withColumn(<span class="lit">"key1"</span>, lit(<span class="num">1</span>))
<span class="kw">val</span> df2 = spark.range(<span class="num">0</span>, <span class="num">2</span>).withColumn(<span class="lit">"key2"</span>, lit(<span class="num">2</span>))
<span class="kw">val</span> df3 = df1.join(df2)

df3.show
<span class="cmt">// +---+----+---+----+</span>
<span class="cmt">// | id|key1| id|key2|</span>
<span class="cmt">// +---+----+---+----+</span>
<span class="cmt">// |  0|   1|  0|   2|</span>
<span class="cmt">// |  0|   1|  1|   2|</span>
<span class="cmt">// |  1|   1|  0|   2|</span>
<span class="cmt">// |  1|   1|  1|   2|</span>
<span class="cmt">// +---+----+---+----+</span>

df3.drop(<span class="lit">"id"</span>).show()
<span class="cmt">// output: the two 'id' columns are both dropped.</span>
<span class="cmt">// |key1|key2|</span>
<span class="cmt">// +----+----+</span>
<span class="cmt">// |   1|   2|</span>
<span class="cmt">// |   1|   2|</span>
<span class="cmt">// |   1|   2|</span>
<span class="cmt">// |   1|   2|</span>
<span class="cmt">// +----+----+</span>

df3.drop(col(<span class="lit">"id"</span>)).show()
<span class="cmt">// ...AnalysisException: [AMBIGUOUS_REFERENCE] Reference `id` is ambiguous...</span></pre><p>2, colName contains special characters, like dot.</p><pre><span class="kw">val</span> df = spark.range(<span class="num">0</span>, <span class="num">2</span>).withColumn(<span class="lit">"a.b.c"</span>, lit(<span class="num">1</span>))

df.show()
<span class="cmt">// +---+-----+</span>
<span class="cmt">// | id|a.b.c|</span>
<span class="cmt">// +---+-----+</span>
<span class="cmt">// |  0|    1|</span>
<span class="cmt">// |  1|    1|</span>
<span class="cmt">// +---+-----+</span>

df.drop(<span class="lit">"a.b.c"</span>).show()
<span class="cmt">// +---+</span>
<span class="cmt">// | id|</span>
<span class="cmt">// +---+</span>
<span class="cmt">// |  0|</span>
<span class="cmt">// |  1|</span>
<span class="cmt">// +---+</span>

df.drop(col(<span class="lit">"a.b.c"</span>)).show()
<span class="cmt">// no column match the expression 'a.b.c'</span>
<span class="cmt">// +---+-----+</span>
<span class="cmt">// | id|a.b.c|</span>
<span class="cmt">// +---+-----+</span>
<span class="cmt">// |  0|    1|</span>
<span class="cmt">// |  1|    1|</span>
<span class="cmt">// +---+-----+</span></pre></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#dropDuplicates" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="dropDuplicates(col1:String,cols:String*):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="dropDuplicates(String,String*):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#dropDuplicates(col1:String,cols:String*):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">dropDuplicates</span><span class="params">(<span name="col1">col1: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="cols">cols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a> with duplicate rows removed, considering only
the subset of columns.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a> with duplicate rows removed, considering only
the subset of columns.</p><p>For a static batch <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>, it just drops duplicate rows. For a streaming <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>, it
will keep all data across triggers as intermediate state to drop duplicates rows. You can use
<a href="#withWatermark(eventTime:String,delayThreshold:String):org.apache.spark.sql.Dataset[T]" name="org.apache.spark.sql.Dataset#withWatermark" id="org.apache.spark.sql.Dataset#withWatermark" class="extmbr">withWatermark</a> to limit how late the duplicate data can be and system will accordingly limit
the state. In addition, too late data older than watermark will be dropped to avoid any
possibility of duplicates.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#dropDuplicates" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="dropDuplicates(colNames:Array[String]):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="dropDuplicates(Array[String]):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#dropDuplicates(colNames:Array[String]):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">dropDuplicates</span><span class="params">(<span name="colNames">colNames: <span name="scala.Array" class="extype">Array</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset with duplicate rows removed, considering only
the subset of columns.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with duplicate rows removed, considering only
the subset of columns.</p><p>For a static batch <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>, it just drops duplicate rows. For a streaming <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>, it
will keep all data across triggers as intermediate state to drop duplicates rows. You can use
<a href="#withWatermark(eventTime:String,delayThreshold:String):org.apache.spark.sql.Dataset[T]" name="org.apache.spark.sql.Dataset#withWatermark" id="org.apache.spark.sql.Dataset#withWatermark" class="extmbr">withWatermark</a> to limit how late the duplicate data can be and system will accordingly limit
the state. In addition, too late data older than watermark will be dropped to avoid any
possibility of duplicates.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#dropDuplicates" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="dropDuplicates(colNames:Seq[String]):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="dropDuplicates(Seq[String]):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#dropDuplicates(colNames:Seq[String]):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">dropDuplicates</span><span class="params">(<span name="colNames">colNames: <span name="scala.Seq" class="extype">Seq</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">(Scala-specific) Returns a new Dataset with duplicate rows removed, considering only
the subset of columns.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Returns a new Dataset with duplicate rows removed, considering only
the subset of columns.</p><p>For a static batch <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>, it just drops duplicate rows. For a streaming <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>, it
will keep all data across triggers as intermediate state to drop duplicates rows. You can use
<a href="#withWatermark(eventTime:String,delayThreshold:String):org.apache.spark.sql.Dataset[T]" name="org.apache.spark.sql.Dataset#withWatermark" id="org.apache.spark.sql.Dataset#withWatermark" class="extmbr">withWatermark</a> to limit how late the duplicate data can be and system will accordingly limit
the state. In addition, too late data older than watermark will be dropped to avoid any
possibility of duplicates.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#dropDuplicates" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="dropDuplicates():org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="dropDuplicates():Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#dropDuplicates():org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">dropDuplicates</span><span class="params">()</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset that contains only the unique rows from this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset that contains only the unique rows from this Dataset.
This is an alias for <code>distinct</code>.</p><p>For a static batch <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>, it just drops duplicate rows. For a streaming <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>, it
will keep all data across triggers as intermediate state to drop duplicates rows. You can use
<a href="#withWatermark(eventTime:String,delayThreshold:String):org.apache.spark.sql.Dataset[T]" name="org.apache.spark.sql.Dataset#withWatermark" id="org.apache.spark.sql.Dataset#withWatermark" class="extmbr">withWatermark</a> to limit how late the duplicate data can be and system will accordingly limit
the state. In addition, too late data older than watermark will be dropped to avoid any
possibility of duplicates.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#dropDuplicatesWithinWatermark" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="dropDuplicatesWithinWatermark(col1:String,cols:String*):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="dropDuplicatesWithinWatermark(String,String*):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#dropDuplicatesWithinWatermark(col1:String,cols:String*):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">dropDuplicatesWithinWatermark</span><span class="params">(<span name="col1">col1: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="cols">cols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset with duplicates rows removed, considering only the subset of columns,
within watermark.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with duplicates rows removed, considering only the subset of columns,
within watermark.</p><p>This only works with streaming <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>, and watermark for the input <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a> must be
set via <a href="#withWatermark(eventTime:String,delayThreshold:String):org.apache.spark.sql.Dataset[T]" name="org.apache.spark.sql.Dataset#withWatermark" id="org.apache.spark.sql.Dataset#withWatermark" class="extmbr">withWatermark</a>.</p><p>For a streaming <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>, this will keep all data across triggers as intermediate state
to drop duplicated rows. The state will be kept to guarantee the semantic, "Events are
deduplicated as long as the time distance of earliest and latest events are smaller than the
delay threshold of watermark." Users are encouraged to set the delay threshold of watermark
longer than max timestamp differences among duplicated events.</p><p>Note: too late data older than watermark will be dropped.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.5.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#dropDuplicatesWithinWatermark" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="dropDuplicatesWithinWatermark(colNames:Array[String]):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="dropDuplicatesWithinWatermark(Array[String]):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#dropDuplicatesWithinWatermark(colNames:Array[String]):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">dropDuplicatesWithinWatermark</span><span class="params">(<span name="colNames">colNames: <span name="scala.Array" class="extype">Array</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset with duplicates rows removed, considering only the subset of columns,
within watermark.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with duplicates rows removed, considering only the subset of columns,
within watermark.</p><p>This only works with streaming <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>, and watermark for the input <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a> must be
set via <a href="#withWatermark(eventTime:String,delayThreshold:String):org.apache.spark.sql.Dataset[T]" name="org.apache.spark.sql.Dataset#withWatermark" id="org.apache.spark.sql.Dataset#withWatermark" class="extmbr">withWatermark</a>.</p><p>For a streaming <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>, this will keep all data across triggers as intermediate state
to drop duplicated rows. The state will be kept to guarantee the semantic, "Events are
deduplicated as long as the time distance of earliest and latest events are smaller than the
delay threshold of watermark." Users are encouraged to set the delay threshold of watermark
longer than max timestamp differences among duplicated events.</p><p>Note: too late data older than watermark will be dropped.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.5.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#dropDuplicatesWithinWatermark" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="dropDuplicatesWithinWatermark(colNames:Seq[String]):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="dropDuplicatesWithinWatermark(Seq[String]):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#dropDuplicatesWithinWatermark(colNames:Seq[String]):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">dropDuplicatesWithinWatermark</span><span class="params">(<span name="colNames">colNames: <span name="scala.Seq" class="extype">Seq</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset with duplicates rows removed, considering only the subset of columns,
within watermark.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with duplicates rows removed, considering only the subset of columns,
within watermark.</p><p>This only works with streaming <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>, and watermark for the input <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a> must be
set via <a href="#withWatermark(eventTime:String,delayThreshold:String):org.apache.spark.sql.Dataset[T]" name="org.apache.spark.sql.Dataset#withWatermark" id="org.apache.spark.sql.Dataset#withWatermark" class="extmbr">withWatermark</a>.</p><p>For a streaming <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>, this will keep all data across triggers as intermediate state
to drop duplicated rows. The state will be kept to guarantee the semantic, "Events are
deduplicated as long as the time distance of earliest and latest events are smaller than the
delay threshold of watermark." Users are encouraged to set the delay threshold of watermark
longer than max timestamp differences among duplicated events.</p><p>Note: too late data older than watermark will be dropped.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.5.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#dropDuplicatesWithinWatermark" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="dropDuplicatesWithinWatermark():org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="dropDuplicatesWithinWatermark():Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#dropDuplicatesWithinWatermark():org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">dropDuplicatesWithinWatermark</span><span class="params">()</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset with duplicates rows removed, within watermark.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with duplicates rows removed, within watermark.</p><p>This only works with streaming <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>, and watermark for the input <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a> must be
set via <a href="#withWatermark(eventTime:String,delayThreshold:String):org.apache.spark.sql.Dataset[T]" name="org.apache.spark.sql.Dataset#withWatermark" id="org.apache.spark.sql.Dataset#withWatermark" class="extmbr">withWatermark</a>.</p><p>For a streaming <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>, this will keep all data across triggers as intermediate state
to drop duplicated rows. The state will be kept to guarantee the semantic, "Events are
deduplicated as long as the time distance of earliest and latest events are smaller than the
delay threshold of watermark." Users are encouraged to set the delay threshold of watermark
longer than max timestamp differences among duplicated events.</p><p>Note: too late data older than watermark will be dropped.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.5.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#dtypes" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="dtypes:Array[(String,String)]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#dtypes:Array[(String,String)]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">dtypes</span><span class="result">: <span name="scala.Array" class="extype">Array</span>[(<span name="scala.Predef.String" class="extype">String</span>, <span name="scala.Predef.String" class="extype">String</span>)]</span></span><p class="shortcomment cmt">Returns all column names and their data types as an array.</p><div class="fullcomment"><div class="comment cmt"><p>Returns all column names and their data types as an array.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#encoder" group="Ungrouped" fullComment="no" data-isabs="false" visbl="pub"><a id="encoder:org.apache.spark.sql.Encoder[T]" class="anchorToMember"></a><a id="encoder:Encoder[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#encoder:org.apache.spark.sql.Encoder[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">val</span></span> <span class="symbol"><span class="name">encoder</span><span class="result">: <a href="Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span></li><li class="indented0 " name="scala.AnyRef#eq" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="eq(x$1:AnyRef):Boolean" class="anchorToMember"></a><a id="eq(AnyRef):Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#eq(x$1:AnyRef):Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">eq</span><span class="params">(<span name="arg0">arg0: <span name="scala.AnyRef" class="extype">AnyRef</span></span>)</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef</dd></dl></div></li><li class="indented0 " name="scala.AnyRef#equals" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="equals(x$1:Object):Boolean" class="anchorToMember"></a><a id="equals(AnyRef):Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#equals(x$1:Object):Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">equals</span><span class="params">(<span name="arg0">arg0: <span name="scala.AnyRef" class="extype">AnyRef</span></span>)</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#except" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="except(other:org.apache.spark.sql.Dataset[T]):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="except(Dataset[T]):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#except(other:org.apache.spark.sql.Dataset[T]):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">except</span><span class="params">(<span name="other">other: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset containing rows in this Dataset but not in another Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset containing rows in this Dataset but not in another Dataset.
This is equivalent to <code>EXCEPT DISTINCT</code> in SQL.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>Equality checking is performed directly on the encoded representation of the data
and thus is not affected by a custom <code>equals</code> function defined on <code>T</code>.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#exceptAll" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="exceptAll(other:org.apache.spark.sql.Dataset[T]):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="exceptAll(Dataset[T]):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#exceptAll(other:org.apache.spark.sql.Dataset[T]):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">exceptAll</span><span class="params">(<span name="other">other: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset containing rows in this Dataset but not in another Dataset while
preserving the duplicates.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset containing rows in this Dataset but not in another Dataset while
preserving the duplicates.
This is equivalent to <code>EXCEPT ALL</code> in SQL.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.4.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>Equality checking is performed directly on the encoded representation of the data
and thus is not affected by a custom <code>equals</code> function defined on <code>T</code>. Also as standard in
SQL, this function resolves columns by position (not by name).</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#explain" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="explain():Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#explain():Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">explain</span><span class="params">()</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Prints the physical plan to the console for debugging purposes.</p><div class="fullcomment"><div class="comment cmt"><p>Prints the physical plan to the console for debugging purposes.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#explain" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="explain(extended:Boolean):Unit" class="anchorToMember"></a><a id="explain(Boolean):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#explain(extended:Boolean):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">explain</span><span class="params">(<span name="extended">extended: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Prints the plans (logical and physical) to the console for debugging purposes.</p><div class="fullcomment"><div class="comment cmt"><p>Prints the plans (logical and physical) to the console for debugging purposes.
</p></div><dl class="paramcmts block"><dt class="param">extended</dt><dd class="cmt"><p>default <code>false</code>. If <code>false</code>, prints only the physical plan.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#explain" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="explain(mode:String):Unit" class="anchorToMember"></a><a id="explain(String):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#explain(mode:String):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">explain</span><span class="params">(<span name="mode">mode: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Prints the plans (logical and physical) with a format specified by a given explain mode.</p><div class="fullcomment"><div class="comment cmt"><p>Prints the plans (logical and physical) with a format specified by a given explain mode.
</p></div><dl class="paramcmts block"><dt class="param">mode</dt><dd class="cmt"><p>specifies the expected output format of plans.</p><ul><li><code>simple</code> Print only a physical plan.</li><li><code>extended</code>: Print both logical and physical plans.</li><li><code>codegen</code>: Print a physical plan and generated codes if they are
                available.</li><li><code>cost</code>: Print a logical plan and statistics if they are available.</li><li><code>formatted</code>: Split explain output into two sections: a physical plan outline
                and node details.</li></ul></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>3.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#filter" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="filter(func:org.apache.spark.api.java.function.FilterFunction[T]):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="filter(FilterFunction[T]):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#filter(func:org.apache.spark.api.java.function.FilterFunction[T]):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">filter</span><span class="params">(<span name="func">func: <a href="../api/java/function/FilterFunction.html" name="org.apache.spark.api.java.function.FilterFunction" id="org.apache.spark.api.java.function.FilterFunction" class="extype">FilterFunction</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">(Java-specific)
Returns a new Dataset that only contains elements where <code>func</code> returns <code>true</code>.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific)
Returns a new Dataset that only contains elements where <code>func</code> returns <code>true</code>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#filter" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="filter(func:T=&gt;Boolean):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="filter((T)=&gt;Boolean):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#filter(func:T=&gt;Boolean):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">filter</span><span class="params">(<span name="func">func: (<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>) =&gt; <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">(Scala-specific)
Returns a new Dataset that only contains elements where <code>func</code> returns <code>true</code>.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific)
Returns a new Dataset that only contains elements where <code>func</code> returns <code>true</code>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#filter" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="filter(conditionExpr:String):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="filter(String):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#filter(conditionExpr:String):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">filter</span><span class="params">(<span name="conditionExpr">conditionExpr: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Filters rows using the given SQL expression.</p><div class="fullcomment"><div class="comment cmt"><p>Filters rows using the given SQL expression.</p><pre>peopleDs.filter(<span class="lit">"age &gt; 15"</span>)</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#filter" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="filter(condition:org.apache.spark.sql.Column):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="filter(Column):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#filter(condition:org.apache.spark.sql.Column):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">filter</span><span class="params">(<span name="condition">condition: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Filters rows using the given condition.</p><div class="fullcomment"><div class="comment cmt"><p>Filters rows using the given condition.</p><pre><span class="cmt">// The following are equivalent:</span>
peopleDs.filter($<span class="lit">"age"</span> &gt; <span class="num">15</span>)
peopleDs.where($<span class="lit">"age"</span> &gt; <span class="num">15</span>)</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#first" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="first():T" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#first():T" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">first</span><span class="params">()</span><span class="result">: <span name="org.apache.spark.sql.Dataset.T" class="extype">T</span></span></span><p class="shortcomment cmt">Returns the first row.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the first row. Alias for head().</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#flatMap" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="flatMap[U](f:org.apache.spark.api.java.function.FlatMapFunction[T,U],encoder:org.apache.spark.sql.Encoder[U]):org.apache.spark.sql.Dataset[U]" class="anchorToMember"></a><a id="flatMap[U](FlatMapFunction[T,U],Encoder[U]):Dataset[U]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#flatMap[U](f:org.apache.spark.api.java.function.FlatMapFunction[T,U],encoder:org.apache.spark.sql.Encoder[U]):org.apache.spark.sql.Dataset[U]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">flatMap</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="f">f: <a href="../api/java/function/FlatMapFunction.html" name="org.apache.spark.api.java.function.FlatMapFunction" id="org.apache.spark.api.java.function.FlatMapFunction" class="extype">FlatMapFunction</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.flatMap.U" class="extype">U</span>]</span>, <span name="encoder">encoder: <a href="Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.Dataset.flatMap.U" class="extype">U</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.flatMap.U" class="extype">U</span>]</span></span><p class="shortcomment cmt">(Java-specific)
Returns a new Dataset by first applying a function to all elements of this Dataset,
and then flattening the results.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific)
Returns a new Dataset by first applying a function to all elements of this Dataset,
and then flattening the results.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#flatMap" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="flatMap[U](func:T=&gt;IterableOnce[U])(implicitevidence$8:org.apache.spark.sql.Encoder[U]):org.apache.spark.sql.Dataset[U]" class="anchorToMember"></a><a id="flatMap[U]((T)=&gt;IterableOnce[U])(Encoder[U]):Dataset[U]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#flatMap[U](func:T=&gt;IterableOnce[U])(implicitevidence$8:org.apache.spark.sql.Encoder[U]):org.apache.spark.sql.Dataset[U]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">flatMap</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="func">func: (<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>) =&gt; <span name="scala.IterableOnce" class="extype">IterableOnce</span>[<span name="org.apache.spark.sql.Dataset.flatMap.U" class="extype">U</span>]</span>)</span><span class="params">(<span class="implicit">implicit </span><span name="arg0">arg0: <a href="Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.Dataset.flatMap.U" class="extype">U</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.flatMap.U" class="extype">U</span>]</span></span><p class="shortcomment cmt">(Scala-specific)
Returns a new Dataset by first applying a function to all elements of this Dataset,
and then flattening the results.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific)
Returns a new Dataset by first applying a function to all elements of this Dataset,
and then flattening the results.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#foreach" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="foreach(func:org.apache.spark.api.java.function.ForeachFunction[T]):Unit" class="anchorToMember"></a><a id="foreach(ForeachFunction[T]):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#foreach(func:org.apache.spark.api.java.function.ForeachFunction[T]):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">foreach</span><span class="params">(<span name="func">func: <a href="../api/java/function/ForeachFunction.html" name="org.apache.spark.api.java.function.ForeachFunction" id="org.apache.spark.api.java.function.ForeachFunction" class="extype">ForeachFunction</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">(Java-specific)
Runs <code>func</code> on each element of this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific)
Runs <code>func</code> on each element of this Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#foreach" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="foreach(f:T=&gt;Unit):Unit" class="anchorToMember"></a><a id="foreach((T)=&gt;Unit):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#foreach(f:T=&gt;Unit):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">foreach</span><span class="params">(<span name="f">f: (<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>) =&gt; <span name="scala.Unit" class="extype">Unit</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Applies a function <code>f</code> to all rows.</p><div class="fullcomment"><div class="comment cmt"><p>Applies a function <code>f</code> to all rows.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#foreachPartition" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="foreachPartition(func:org.apache.spark.api.java.function.ForeachPartitionFunction[T]):Unit" class="anchorToMember"></a><a id="foreachPartition(ForeachPartitionFunction[T]):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#foreachPartition(func:org.apache.spark.api.java.function.ForeachPartitionFunction[T]):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">foreachPartition</span><span class="params">(<span name="func">func: <a href="../api/java/function/ForeachPartitionFunction.html" name="org.apache.spark.api.java.function.ForeachPartitionFunction" id="org.apache.spark.api.java.function.ForeachPartitionFunction" class="extype">ForeachPartitionFunction</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">(Java-specific)
Runs <code>func</code> on each partition of this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific)
Runs <code>func</code> on each partition of this Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#foreachPartition" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="foreachPartition(f:Iterator[T]=&gt;Unit):Unit" class="anchorToMember"></a><a id="foreachPartition((Iterator[T])=&gt;Unit):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#foreachPartition(f:Iterator[T]=&gt;Unit):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">foreachPartition</span><span class="params">(<span name="f">f: (<span name="scala.Iterator" class="extype">Iterator</span>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]) =&gt; <span name="scala.Unit" class="extype">Unit</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Applies a function <code>f</code> to each partition of this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Applies a function <code>f</code> to each partition of this Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="scala.AnyRef#getClass" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="getClass():Class[_]" class="anchorToMember"></a><a id="getClass():Class[_&lt;:AnyRef]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#getClass():Class[_]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">getClass</span><span class="params">()</span><span class="result">: <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/lang/Class.html#java.lang.Class" name="java.lang.Class" id="java.lang.Class" class="extype">Class</a>[_ &lt;: <span name="scala.AnyRef" class="extype">AnyRef</span>]</span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef → Any</dd><dt>Annotations</dt><dd><span class="name">@IntrinsicCandidate</span><span class="args">()</span> <span class="name">@native</span><span class="args">()</span> </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#groupBy" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="groupBy(col1:String,cols:String*):org.apache.spark.sql.RelationalGroupedDataset" class="anchorToMember"></a><a id="groupBy(String,String*):RelationalGroupedDataset" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#groupBy(col1:String,cols:String*):org.apache.spark.sql.RelationalGroupedDataset" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">groupBy</span><span class="params">(<span name="col1">col1: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="cols">cols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.RelationalGroupedDataset" id="org.apache.spark.sql.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a></span></span><p class="shortcomment cmt">Groups the Dataset using the specified columns, so that we can run aggregation on them.</p><div class="fullcomment"><div class="comment cmt"><p>Groups the Dataset using the specified columns, so that we can run aggregation on them.
See <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.RelationalGroupedDataset" id="org.apache.spark.sql.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a> for all the available aggregate functions.</p><p>This is a variant of groupBy that can only group by existing columns using column names
(i.e. cannot construct expressions).</p><pre><span class="cmt">// Compute the average for all numeric columns grouped by department.</span>
ds.groupBy(<span class="lit">"department"</span>).avg()

<span class="cmt">// Compute the max age and average salary, grouped by department and gender.</span>
ds.groupBy($<span class="lit">"department"</span>, $<span class="lit">"gender"</span>).agg(<span class="std">Map</span>(
  <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>,
  <span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>
))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#groupBy" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="groupBy(cols:org.apache.spark.sql.Column*):org.apache.spark.sql.RelationalGroupedDataset" class="anchorToMember"></a><a id="groupBy(Column*):RelationalGroupedDataset" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#groupBy(cols:org.apache.spark.sql.Column*):org.apache.spark.sql.RelationalGroupedDataset" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">groupBy</span><span class="params">(<span name="cols">cols: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.RelationalGroupedDataset" id="org.apache.spark.sql.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a></span></span><p class="shortcomment cmt">Groups the Dataset using the specified columns, so we can run aggregation on them.</p><div class="fullcomment"><div class="comment cmt"><p>Groups the Dataset using the specified columns, so we can run aggregation on them. See
<a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.RelationalGroupedDataset" id="org.apache.spark.sql.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a> for all the available aggregate functions.</p><pre><span class="cmt">// Compute the average for all numeric columns grouped by department.</span>
ds.groupBy($<span class="lit">"department"</span>).avg()

<span class="cmt">// Compute the max age and average salary, grouped by department and gender.</span>
ds.groupBy($<span class="lit">"department"</span>, $<span class="lit">"gender"</span>).agg(<span class="std">Map</span>(
  <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>,
  <span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>
))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#groupByKey" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="groupByKey[K](func:org.apache.spark.api.java.function.MapFunction[T,K],encoder:org.apache.spark.sql.Encoder[K]):org.apache.spark.sql.KeyValueGroupedDataset[K,T]" class="anchorToMember"></a><a id="groupByKey[K](MapFunction[T,K],Encoder[K]):KeyValueGroupedDataset[K,T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#groupByKey[K](func:org.apache.spark.api.java.function.MapFunction[T,K],encoder:org.apache.spark.sql.Encoder[K]):org.apache.spark.sql.KeyValueGroupedDataset[K,T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">groupByKey</span><span class="tparams">[<span name="K">K</span>]</span><span class="params">(<span name="func">func: <a href="../api/java/function/MapFunction.html" name="org.apache.spark.api.java.function.MapFunction" id="org.apache.spark.api.java.function.MapFunction" class="extype">MapFunction</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.groupByKey.K" class="extype">K</span>]</span>, <span name="encoder">encoder: <a href="Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.Dataset.groupByKey.K" class="extype">K</span>]</span>)</span><span class="result">: <a href="KeyValueGroupedDataset.html" name="org.apache.spark.sql.KeyValueGroupedDataset" id="org.apache.spark.sql.KeyValueGroupedDataset" class="extype">KeyValueGroupedDataset</a>[<span name="org.apache.spark.sql.Dataset.groupByKey.K" class="extype">K</span>, <span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">(Java-specific)
Returns a <a href="KeyValueGroupedDataset.html" name="org.apache.spark.sql.KeyValueGroupedDataset" id="org.apache.spark.sql.KeyValueGroupedDataset" class="extype">KeyValueGroupedDataset</a> where the data is grouped by the given key <code>func</code>.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific)
Returns a <a href="KeyValueGroupedDataset.html" name="org.apache.spark.sql.KeyValueGroupedDataset" id="org.apache.spark.sql.KeyValueGroupedDataset" class="extype">KeyValueGroupedDataset</a> where the data is grouped by the given key <code>func</code>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#groupByKey" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="groupByKey[K](func:T=&gt;K)(implicitevidence$3:org.apache.spark.sql.Encoder[K]):org.apache.spark.sql.KeyValueGroupedDataset[K,T]" class="anchorToMember"></a><a id="groupByKey[K]((T)=&gt;K)(Encoder[K]):KeyValueGroupedDataset[K,T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#groupByKey[K](func:T=&gt;K)(implicitevidence$3:org.apache.spark.sql.Encoder[K]):org.apache.spark.sql.KeyValueGroupedDataset[K,T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">groupByKey</span><span class="tparams">[<span name="K">K</span>]</span><span class="params">(<span name="func">func: (<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>) =&gt; <span name="org.apache.spark.sql.Dataset.groupByKey.K" class="extype">K</span></span>)</span><span class="params">(<span class="implicit">implicit </span><span name="arg0">arg0: <a href="Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.Dataset.groupByKey.K" class="extype">K</span>]</span>)</span><span class="result">: <a href="KeyValueGroupedDataset.html" name="org.apache.spark.sql.KeyValueGroupedDataset" id="org.apache.spark.sql.KeyValueGroupedDataset" class="extype">KeyValueGroupedDataset</a>[<span name="org.apache.spark.sql.Dataset.groupByKey.K" class="extype">K</span>, <span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">(Scala-specific)
Returns a <a href="KeyValueGroupedDataset.html" name="org.apache.spark.sql.KeyValueGroupedDataset" id="org.apache.spark.sql.KeyValueGroupedDataset" class="extype">KeyValueGroupedDataset</a> where the data is grouped by the given key <code>func</code>.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific)
Returns a <a href="KeyValueGroupedDataset.html" name="org.apache.spark.sql.KeyValueGroupedDataset" id="org.apache.spark.sql.KeyValueGroupedDataset" class="extype">KeyValueGroupedDataset</a> where the data is grouped by the given key <code>func</code>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#groupingSets" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="groupingSets(groupingSets:Seq[Seq[org.apache.spark.sql.Column]],cols:org.apache.spark.sql.Column*):org.apache.spark.sql.RelationalGroupedDataset" class="anchorToMember"></a><a id="groupingSets(Seq[Seq[Column]],Column*):RelationalGroupedDataset" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#groupingSets(groupingSets:Seq[Seq[org.apache.spark.sql.Column]],cols:org.apache.spark.sql.Column*):org.apache.spark.sql.RelationalGroupedDataset" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">groupingSets</span><span class="params">(<span name="groupingSets">groupingSets: <span name="scala.Seq" class="extype">Seq</span>[<span name="scala.Seq" class="extype">Seq</span>[<a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]]</span>, <span name="cols">cols: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.RelationalGroupedDataset" id="org.apache.spark.sql.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a></span></span><p class="shortcomment cmt">Create multi-dimensional aggregation for the current Dataset using the specified grouping sets,
so we can run aggregation on them.</p><div class="fullcomment"><div class="comment cmt"><p>Create multi-dimensional aggregation for the current Dataset using the specified grouping sets,
so we can run aggregation on them.
See <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.RelationalGroupedDataset" id="org.apache.spark.sql.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a> for all the available aggregate functions.</p><pre><span class="cmt">// Compute the average for all numeric columns group by specific grouping sets.</span>
ds.groupingSets(<span class="std">Seq</span>(<span class="std">Seq</span>($<span class="lit">"department"</span>, $<span class="lit">"group"</span>), <span class="std">Seq</span>()), $<span class="lit">"department"</span>, $<span class="lit">"group"</span>).avg()

<span class="cmt">// Compute the max age and average salary, group by specific grouping sets.</span>
ds.groupingSets(<span class="std">Seq</span>($<span class="lit">"department"</span>, $<span class="lit">"gender"</span>), <span class="std">Seq</span>()), $<span class="lit">"department"</span>, $<span class="lit">"group"</span>).agg(<span class="std">Map</span>(
  <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>,
  <span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>
))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>4.0.0</p></dd></dl></div></li><li class="indented0 " name="scala.AnyRef#hashCode" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="hashCode():Int" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#hashCode():Int" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">hashCode</span><span class="params">()</span><span class="result">: <span name="scala.Int" class="extype">Int</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef → Any</dd><dt>Annotations</dt><dd><span class="name">@IntrinsicCandidate</span><span class="args">()</span> <span class="name">@native</span><span class="args">()</span> </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#head" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="head():T" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#head():T" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">head</span><span class="params">()</span><span class="result">: <span name="org.apache.spark.sql.Dataset.T" class="extype">T</span></span></span><p class="shortcomment cmt">Returns the first row.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the first row.</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#head" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="head(n:Int):Array[T]" class="anchorToMember"></a><a id="head(Int):Array[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#head(n:Int):Array[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">head</span><span class="params">(<span name="n">n: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <span name="scala.Array" class="extype">Array</span>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns the first <code>n</code> rows.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the first <code>n</code> rows.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>this method should only be used if the resulting array is expected to be small, as
all the data is loaded into the driver's memory.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#hint" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="hint(name:String,parameters:Any*):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="hint(String,Any*):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#hint(name:String,parameters:Any*):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">hint</span><span class="params">(<span name="name">name: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="parameters">parameters: <span name="scala.Any" class="extype">Any</span>*</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Specifies some hint on the current Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Specifies some hint on the current Dataset. As an example, the following code specifies
that one of the plan can be broadcasted:</p><pre>df1.join(df2.hint(<span class="lit">"broadcast"</span>))</pre><p>the following code specifies that this dataset could be rebalanced with given number of
partitions:</p><pre>df1.hint(<span class="lit">"rebalance"</span>, <span class="num">10</span>)</pre></div><dl class="paramcmts block"><dt class="param">name</dt><dd class="cmt"><p>the name of the hint</p></dd><dt class="param">parameters</dt><dd class="cmt"><p>the parameters of the hint, all the parameters should be a <code>Column</code> or
                  <code>Expression</code> or <code>Symbol</code> or could be converted into a <code>Literal</code></p></dd></dl><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.2.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#inputFiles" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="inputFiles:Array[String]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#inputFiles:Array[String]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">inputFiles</span><span class="result">: <span name="scala.Array" class="extype">Array</span>[<span name="scala.Predef.String" class="extype">String</span>]</span></span><p class="shortcomment cmt">Returns a best-effort snapshot of the files that compose this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a best-effort snapshot of the files that compose this Dataset. This method simply
asks each constituent BaseRelation for its respective files and takes the union of all results.
Depending on the source relations, this may not find all input files. Duplicates are removed.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#intersect" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="intersect(other:org.apache.spark.sql.Dataset[T]):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="intersect(Dataset[T]):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#intersect(other:org.apache.spark.sql.Dataset[T]):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">intersect</span><span class="params">(<span name="other">other: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset containing rows only in both this Dataset and another Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset containing rows only in both this Dataset and another Dataset.
This is equivalent to <code>INTERSECT</code> in SQL.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>Equality checking is performed directly on the encoded representation of the data
and thus is not affected by a custom <code>equals</code> function defined on <code>T</code>.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#intersectAll" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="intersectAll(other:org.apache.spark.sql.Dataset[T]):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="intersectAll(Dataset[T]):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#intersectAll(other:org.apache.spark.sql.Dataset[T]):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">intersectAll</span><span class="params">(<span name="other">other: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset containing rows only in both this Dataset and another Dataset while
preserving the duplicates.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset containing rows only in both this Dataset and another Dataset while
preserving the duplicates.
This is equivalent to <code>INTERSECT ALL</code> in SQL.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.4.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>Equality checking is performed directly on the encoded representation of the data
and thus is not affected by a custom <code>equals</code> function defined on <code>T</code>. Also as standard
in SQL, this function resolves columns by position (not by name).</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#isEmpty" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="isEmpty:Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#isEmpty:Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">isEmpty</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><p class="shortcomment cmt">Returns true if the <code>Dataset</code> is empty.</p><div class="fullcomment"><div class="comment cmt"><p>Returns true if the <code>Dataset</code> is empty.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.4.0</p></dd></dl></div></li><li class="indented0 " name="scala.Any#isInstanceOf" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="isInstanceOf[T0]:Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#isInstanceOf[T0]:Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">isInstanceOf</span><span class="tparams">[<span name="T0">T0</span>]</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>Any</dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#isLocal" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="isLocal:Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#isLocal:Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">isLocal</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><p class="shortcomment cmt">Returns true if the <code>collect</code> and <code>take</code> methods can be run locally
(without any Spark executors).</p><div class="fullcomment"><div class="comment cmt"><p>Returns true if the <code>collect</code> and <code>take</code> methods can be run locally
(without any Spark executors).
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#isStreaming" group="streaming" fullComment="yes" data-isabs="false" visbl="pub"><a id="isStreaming:Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#isStreaming:Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">isStreaming</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><p class="shortcomment cmt">Returns true if this Dataset contains one or more sources that continuously
return data as it arrives.</p><div class="fullcomment"><div class="comment cmt"><p>Returns true if this Dataset contains one or more sources that continuously
return data as it arrives. A Dataset that reads data from a streaming source
must be executed as a <code>StreamingQuery</code> using the <code>start()</code> method in
<code>DataStreamWriter</code>. Methods that return a single answer, e.g. <code>count()</code> or
<code>collect()</code>, will throw an <a href="AnalysisException.html" name="org.apache.spark.sql.AnalysisException" id="org.apache.spark.sql.AnalysisException" class="extype">AnalysisException</a> when there is a streaming
source present.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#javaRDD" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="javaRDD:org.apache.spark.api.java.JavaRDD[T]" class="anchorToMember"></a><a id="javaRDD:JavaRDD[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#javaRDD:org.apache.spark.api.java.JavaRDD[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">javaRDD</span><span class="result">: <a href="../api/java/JavaRDD.html" name="org.apache.spark.api.java.JavaRDD" id="org.apache.spark.api.java.JavaRDD" class="extype">JavaRDD</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns the content of the Dataset as a <code>JavaRDD</code> of <code>T</code>s.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the content of the Dataset as a <code>JavaRDD</code> of <code>T</code>s.</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#join" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="join(right:org.apache.spark.sql.Dataset[_],joinExprs:org.apache.spark.sql.Column,joinType:String):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="join(Dataset[_],Column,String):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#join(right:org.apache.spark.sql.Dataset[_],joinExprs:org.apache.spark.sql.Column,joinType:String):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">join</span><span class="params">(<span name="right">right: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[_]</span>, <span name="joinExprs">joinExprs: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>, <span name="joinType">joinType: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Join with another <code>DataFrame</code>, using the given join expression.</p><div class="fullcomment"><div class="comment cmt"><p>Join with another <code>DataFrame</code>, using the given join expression. The following performs
a full outer join between <code>df1</code> and <code>df2</code>.</p><pre><span class="cmt">// Scala:</span>
<span class="kw">import</span> org.apache.spark.sql.functions._
df1.join(df2, $<span class="lit">"df1Key"</span> === $<span class="lit">"df2Key"</span>, <span class="lit">"outer"</span>)

<span class="cmt">// Java:</span>
<span class="kw">import</span> static org.apache.spark.sql.functions.*;
df1.join(df2, col(<span class="lit">"df1Key"</span>).equalTo(col(<span class="lit">"df2Key"</span>)), <span class="lit">"outer"</span>);</pre></div><dl class="paramcmts block"><dt class="param">right</dt><dd class="cmt"><p>Right side of the join.</p></dd><dt class="param">joinExprs</dt><dd class="cmt"><p>Join expression.</p></dd><dt class="param">joinType</dt><dd class="cmt"><p>Type of join to perform. Default <code>inner</code>. Must be one of:
                <code>inner</code>, <code>cross</code>, <code>outer</code>, <code>full</code>, <code>fullouter</code>, <code>full_outer</code>, <code>left</code>,
                <code>leftouter</code>, <code>left_outer</code>, <code>right</code>, <code>rightouter</code>, <code>right_outer</code>,
                <code>semi</code>, <code>leftsemi</code>, <code>left_semi</code>, <code>anti</code>, <code>leftanti</code>, left_anti<code>.</code></p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#join" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="join(right:org.apache.spark.sql.Dataset[_],joinExprs:org.apache.spark.sql.Column):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="join(Dataset[_],Column):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#join(right:org.apache.spark.sql.Dataset[_],joinExprs:org.apache.spark.sql.Column):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">join</span><span class="params">(<span name="right">right: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[_]</span>, <span name="joinExprs">joinExprs: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Inner join with another <code>DataFrame</code>, using the given join expression.</p><div class="fullcomment"><div class="comment cmt"><p>Inner join with another <code>DataFrame</code>, using the given join expression.</p><pre><span class="cmt">// The following two are equivalent:</span>
df1.join(df2, $<span class="lit">"df1Key"</span> === $<span class="lit">"df2Key"</span>)
df1.join(df2).where($<span class="lit">"df1Key"</span> === $<span class="lit">"df2Key"</span>)</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#join" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="join(right:org.apache.spark.sql.Dataset[_],usingColumns:Seq[String],joinType:String):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="join(Dataset[_],Seq[String],String):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#join(right:org.apache.spark.sql.Dataset[_],usingColumns:Seq[String],joinType:String):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">join</span><span class="params">(<span name="right">right: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[_]</span>, <span name="usingColumns">usingColumns: <span name="scala.Seq" class="extype">Seq</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>, <span name="joinType">joinType: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">(Scala-specific) Equi-join with another <code>DataFrame</code> using the given columns.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Equi-join with another <code>DataFrame</code> using the given columns. A cross join
with a predicate is specified as an inner join. If you would explicitly like to perform a
cross join use the <code>crossJoin</code> method.</p><p>Different from other join functions, the join columns will only appear once in the output,
i.e. similar to SQL's <code>JOIN USING</code> syntax.
</p></div><dl class="paramcmts block"><dt class="param">right</dt><dd class="cmt"><p>Right side of the join operation.</p></dd><dt class="param">usingColumns</dt><dd class="cmt"><p>Names of the columns to join on. This columns must exist on both sides.</p></dd><dt class="param">joinType</dt><dd class="cmt"><p>Type of join to perform. Default <code>inner</code>. Must be one of:
                <code>inner</code>, <code>cross</code>, <code>outer</code>, <code>full</code>, <code>fullouter</code>, <code>full_outer</code>, <code>left</code>,
                <code>leftouter</code>, <code>left_outer</code>, <code>right</code>, <code>rightouter</code>, <code>right_outer</code>,
                <code>semi</code>, <code>leftsemi</code>, <code>left_semi</code>, <code>anti</code>, <code>leftanti</code>, <code>left_anti</code>.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>If you perform a self-join using this function without aliasing the input
<code>DataFrame</code>s, you will NOT be able to reference any columns after the join, since
there is no way to disambiguate which side of the join you would like to reference.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#join" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="join(right:org.apache.spark.sql.Dataset[_],usingColumns:Array[String],joinType:String):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="join(Dataset[_],Array[String],String):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#join(right:org.apache.spark.sql.Dataset[_],usingColumns:Array[String],joinType:String):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">join</span><span class="params">(<span name="right">right: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[_]</span>, <span name="usingColumns">usingColumns: <span name="scala.Array" class="extype">Array</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>, <span name="joinType">joinType: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">(Java-specific) Equi-join with another <code>DataFrame</code> using the given columns.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific) Equi-join with another <code>DataFrame</code> using the given columns. See the
Scala-specific overload for more details.
</p></div><dl class="paramcmts block"><dt class="param">right</dt><dd class="cmt"><p>Right side of the join operation.</p></dd><dt class="param">usingColumns</dt><dd class="cmt"><p>Names of the columns to join on. This columns must exist on both sides.</p></dd><dt class="param">joinType</dt><dd class="cmt"><p>Type of join to perform. Default <code>inner</code>. Must be one of:
                <code>inner</code>, <code>cross</code>, <code>outer</code>, <code>full</code>, <code>fullouter</code>, <code>full_outer</code>, <code>left</code>,
                <code>leftouter</code>, <code>left_outer</code>, <code>right</code>, <code>rightouter</code>, <code>right_outer</code>,
                <code>semi</code>, <code>leftsemi</code>, <code>left_semi</code>, <code>anti</code>, <code>leftanti</code>, left_anti<code>.</code></p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#join" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="join(right:org.apache.spark.sql.Dataset[_],usingColumn:String,joinType:String):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="join(Dataset[_],String,String):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#join(right:org.apache.spark.sql.Dataset[_],usingColumn:String,joinType:String):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">join</span><span class="params">(<span name="right">right: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[_]</span>, <span name="usingColumn">usingColumn: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="joinType">joinType: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Equi-join with another <code>DataFrame</code> using the given column.</p><div class="fullcomment"><div class="comment cmt"><p>Equi-join with another <code>DataFrame</code> using the given column. A cross join with a predicate
is specified as an inner join. If you would explicitly like to perform a cross join use the
<code>crossJoin</code> method.</p><p>Different from other join functions, the join column will only appear once in the output,
i.e. similar to SQL's <code>JOIN USING</code> syntax.
</p></div><dl class="paramcmts block"><dt class="param">right</dt><dd class="cmt"><p>Right side of the join operation.</p></dd><dt class="param">usingColumn</dt><dd class="cmt"><p>Name of the column to join on. This column must exist on both sides.</p></dd><dt class="param">joinType</dt><dd class="cmt"><p>Type of join to perform. Default <code>inner</code>. Must be one of:
                <code>inner</code>, <code>cross</code>, <code>outer</code>, <code>full</code>, <code>fullouter</code>, <code>full_outer</code>, <code>left</code>,
                <code>leftouter</code>, <code>left_outer</code>, <code>right</code>, <code>rightouter</code>, <code>right_outer</code>,
                <code>semi</code>, <code>leftsemi</code>, <code>left_semi</code>, <code>anti</code>, <code>leftanti</code>, left_anti<code>.</code></p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>If you perform a self-join using this function without aliasing the input
<code>DataFrame</code>s, you will NOT be able to reference any columns after the join, since
there is no way to disambiguate which side of the join you would like to reference.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#join" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="join(right:org.apache.spark.sql.Dataset[_],usingColumns:Seq[String]):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="join(Dataset[_],Seq[String]):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#join(right:org.apache.spark.sql.Dataset[_],usingColumns:Seq[String]):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">join</span><span class="params">(<span name="right">right: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[_]</span>, <span name="usingColumns">usingColumns: <span name="scala.Seq" class="extype">Seq</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">(Scala-specific) Inner equi-join with another <code>DataFrame</code> using the given columns.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Inner equi-join with another <code>DataFrame</code> using the given columns.</p><p>Different from other join functions, the join columns will only appear once in the output,
i.e. similar to SQL's <code>JOIN USING</code> syntax.</p><pre><span class="cmt">// Joining df1 and df2 using the columns "user_id" and "user_name"</span>
df1.join(df2, <span class="std">Seq</span>(<span class="lit">"user_id"</span>, <span class="lit">"user_name"</span>))</pre></div><dl class="paramcmts block"><dt class="param">right</dt><dd class="cmt"><p>Right side of the join operation.</p></dd><dt class="param">usingColumns</dt><dd class="cmt"><p>Names of the columns to join on. This columns must exist on both sides.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>If you perform a self-join using this function without aliasing the input
<code>DataFrame</code>s, you will NOT be able to reference any columns after the join, since
there is no way to disambiguate which side of the join you would like to reference.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#join" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="join(right:org.apache.spark.sql.Dataset[_],usingColumns:Array[String]):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="join(Dataset[_],Array[String]):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#join(right:org.apache.spark.sql.Dataset[_],usingColumns:Array[String]):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">join</span><span class="params">(<span name="right">right: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[_]</span>, <span name="usingColumns">usingColumns: <span name="scala.Array" class="extype">Array</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">(Java-specific) Inner equi-join with another <code>DataFrame</code> using the given columns.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific) Inner equi-join with another <code>DataFrame</code> using the given columns. See the
Scala-specific overload for more details.
</p></div><dl class="paramcmts block"><dt class="param">right</dt><dd class="cmt"><p>Right side of the join operation.</p></dd><dt class="param">usingColumns</dt><dd class="cmt"><p>Names of the columns to join on. This columns must exist on both sides.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#join" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="join(right:org.apache.spark.sql.Dataset[_],usingColumn:String):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="join(Dataset[_],String):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#join(right:org.apache.spark.sql.Dataset[_],usingColumn:String):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">join</span><span class="params">(<span name="right">right: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[_]</span>, <span name="usingColumn">usingColumn: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Inner equi-join with another <code>DataFrame</code> using the given column.</p><div class="fullcomment"><div class="comment cmt"><p>Inner equi-join with another <code>DataFrame</code> using the given column.</p><p>Different from other join functions, the join column will only appear once in the output,
i.e. similar to SQL's <code>JOIN USING</code> syntax.</p><pre><span class="cmt">// Joining df1 and df2 using the column "user_id"</span>
df1.join(df2, <span class="lit">"user_id"</span>)</pre></div><dl class="paramcmts block"><dt class="param">right</dt><dd class="cmt"><p>Right side of the join operation.</p></dd><dt class="param">usingColumn</dt><dd class="cmt"><p>Name of the column to join on. This column must exist on both sides.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>If you perform a self-join using this function without aliasing the input
<code>DataFrame</code>s, you will NOT be able to reference any columns after the join, since
there is no way to disambiguate which side of the join you would like to reference.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#join" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="join(right:org.apache.spark.sql.Dataset[_]):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="join(Dataset[_]):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#join(right:org.apache.spark.sql.Dataset[_]):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">join</span><span class="params">(<span name="right">right: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[_]</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Join with another <code>DataFrame</code>.</p><div class="fullcomment"><div class="comment cmt"><p>Join with another <code>DataFrame</code>.</p><p>Behaves as an INNER JOIN and requires a subsequent join predicate.
</p></div><dl class="paramcmts block"><dt class="param">right</dt><dd class="cmt"><p>Right side of the join operation.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#joinWith" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="joinWith[U](other:org.apache.spark.sql.Dataset[U],condition:org.apache.spark.sql.Column):org.apache.spark.sql.Dataset[(T,U)]" class="anchorToMember"></a><a id="joinWith[U](Dataset[U],Column):Dataset[(T,U)]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#joinWith[U](other:org.apache.spark.sql.Dataset[U],condition:org.apache.spark.sql.Column):org.apache.spark.sql.Dataset[(T,U)]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">joinWith</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="other">other: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.joinWith.U" class="extype">U</span>]</span>, <span name="condition">condition: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[(<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.joinWith.U" class="extype">U</span>)]</span></span><p class="shortcomment cmt">Using inner equi-join to join this Dataset returning a <code>Tuple2</code> for each pair
where <code>condition</code> evaluates to true.</p><div class="fullcomment"><div class="comment cmt"><p>Using inner equi-join to join this Dataset returning a <code>Tuple2</code> for each pair
where <code>condition</code> evaluates to true.
</p></div><dl class="paramcmts block"><dt class="param">other</dt><dd class="cmt"><p>Right side of the join.</p></dd><dt class="param">condition</dt><dd class="cmt"><p>Join expression.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#joinWith" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="joinWith[U](other:org.apache.spark.sql.Dataset[U],condition:org.apache.spark.sql.Column,joinType:String):org.apache.spark.sql.Dataset[(T,U)]" class="anchorToMember"></a><a id="joinWith[U](Dataset[U],Column,String):Dataset[(T,U)]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#joinWith[U](other:org.apache.spark.sql.Dataset[U],condition:org.apache.spark.sql.Column,joinType:String):org.apache.spark.sql.Dataset[(T,U)]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">joinWith</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="other">other: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.joinWith.U" class="extype">U</span>]</span>, <span name="condition">condition: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>, <span name="joinType">joinType: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[(<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.joinWith.U" class="extype">U</span>)]</span></span><p class="shortcomment cmt">Joins this Dataset returning a <code>Tuple2</code> for each pair where <code>condition</code> evaluates to
true.</p><div class="fullcomment"><div class="comment cmt"><p>Joins this Dataset returning a <code>Tuple2</code> for each pair where <code>condition</code> evaluates to
true.</p><p>This is similar to the relation <code>join</code> function with one important difference in the
result schema. Since <code>joinWith</code> preserves objects present on either side of the join, the
result schema is similarly nested into a tuple under the column names <code>_1</code> and <code>_2</code>.</p><p>This type of join can be useful both for preserving type-safety with the original object
types as well as working with relational data where either side of the join has column
names in common.
</p></div><dl class="paramcmts block"><dt class="param">other</dt><dd class="cmt"><p>Right side of the join.</p></dd><dt class="param">condition</dt><dd class="cmt"><p>Join expression.</p></dd><dt class="param">joinType</dt><dd class="cmt"><p>Type of join to perform. Default <code>inner</code>. Must be one of:
                <code>inner</code>, <code>cross</code>, <code>outer</code>, <code>full</code>, <code>fullouter</code>,<code>full_outer</code>, <code>left</code>,
                <code>leftouter</code>, <code>left_outer</code>, <code>right</code>, <code>rightouter</code>, <code>right_outer</code>.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#limit" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="limit(n:Int):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="limit(Int):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#limit(n:Int):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">limit</span><span class="params">(<span name="n">n: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset by taking the first <code>n</code> rows.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by taking the first <code>n</code> rows. The difference between this function
and <code>head</code> is that <code>head</code> is an action and returns an array (by triggering query execution)
while <code>limit</code> returns a new Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#localCheckpoint" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="localCheckpoint(eager:Boolean):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="localCheckpoint(Boolean):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#localCheckpoint(eager:Boolean):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">localCheckpoint</span><span class="params">(<span name="eager">eager: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Locally checkpoints a Dataset and return the new Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate
the logical plan of this Dataset, which is especially useful in iterative algorithms where the
plan may grow exponentially. Local checkpoints are written to executor storage and despite
potentially faster they are unreliable and may compromise job completion.
</p></div><dl class="paramcmts block"><dt class="param">eager</dt><dd class="cmt"><p>Whether to checkpoint this dataframe immediately</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.3.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>When checkpoint is used with eager = false, the final data that is checkpointed after
      the first action may be different from the data that was used during the job due to
      non-determinism of the underlying operation and retries. If checkpoint is used to achieve
      saving a deterministic snapshot of the data, eager = true should be used. Otherwise,
      it is only deterministic after the first execution, after the checkpoint was finalized.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#localCheckpoint" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="localCheckpoint():org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="localCheckpoint():Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#localCheckpoint():org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">localCheckpoint</span><span class="params">()</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Eagerly locally checkpoints a Dataset and return the new Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be
used to truncate the logical plan of this Dataset, which is especially useful in iterative
algorithms where the plan may grow exponentially. Local checkpoints are written to executor
storage and despite potentially faster they are unreliable and may compromise job completion.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#map" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="map[U](func:org.apache.spark.api.java.function.MapFunction[T,U],encoder:org.apache.spark.sql.Encoder[U]):org.apache.spark.sql.Dataset[U]" class="anchorToMember"></a><a id="map[U](MapFunction[T,U],Encoder[U]):Dataset[U]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#map[U](func:org.apache.spark.api.java.function.MapFunction[T,U],encoder:org.apache.spark.sql.Encoder[U]):org.apache.spark.sql.Dataset[U]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">map</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="func">func: <a href="../api/java/function/MapFunction.html" name="org.apache.spark.api.java.function.MapFunction" id="org.apache.spark.api.java.function.MapFunction" class="extype">MapFunction</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.map.U" class="extype">U</span>]</span>, <span name="encoder">encoder: <a href="Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.Dataset.map.U" class="extype">U</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.map.U" class="extype">U</span>]</span></span><p class="shortcomment cmt">(Java-specific)
Returns a new Dataset that contains the result of applying <code>func</code> to each element.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific)
Returns a new Dataset that contains the result of applying <code>func</code> to each element.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#map" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="map[U](func:T=&gt;U)(implicitevidence$6:org.apache.spark.sql.Encoder[U]):org.apache.spark.sql.Dataset[U]" class="anchorToMember"></a><a id="map[U]((T)=&gt;U)(Encoder[U]):Dataset[U]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#map[U](func:T=&gt;U)(implicitevidence$6:org.apache.spark.sql.Encoder[U]):org.apache.spark.sql.Dataset[U]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">map</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="func">func: (<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>) =&gt; <span name="org.apache.spark.sql.Dataset.map.U" class="extype">U</span></span>)</span><span class="params">(<span class="implicit">implicit </span><span name="arg0">arg0: <a href="Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.Dataset.map.U" class="extype">U</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.map.U" class="extype">U</span>]</span></span><p class="shortcomment cmt">(Scala-specific)
Returns a new Dataset that contains the result of applying <code>func</code> to each element.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific)
Returns a new Dataset that contains the result of applying <code>func</code> to each element.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#mapPartitions" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="mapPartitions[U](f:org.apache.spark.api.java.function.MapPartitionsFunction[T,U],encoder:org.apache.spark.sql.Encoder[U]):org.apache.spark.sql.Dataset[U]" class="anchorToMember"></a><a id="mapPartitions[U](MapPartitionsFunction[T,U],Encoder[U]):Dataset[U]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#mapPartitions[U](f:org.apache.spark.api.java.function.MapPartitionsFunction[T,U],encoder:org.apache.spark.sql.Encoder[U]):org.apache.spark.sql.Dataset[U]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">mapPartitions</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="f">f: <a href="../api/java/function/MapPartitionsFunction.html" name="org.apache.spark.api.java.function.MapPartitionsFunction" id="org.apache.spark.api.java.function.MapPartitionsFunction" class="extype">MapPartitionsFunction</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.mapPartitions.U" class="extype">U</span>]</span>, <span name="encoder">encoder: <a href="Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.Dataset.mapPartitions.U" class="extype">U</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.mapPartitions.U" class="extype">U</span>]</span></span><p class="shortcomment cmt">(Java-specific)
Returns a new Dataset that contains the result of applying <code>f</code> to each partition.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific)
Returns a new Dataset that contains the result of applying <code>f</code> to each partition.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#mapPartitions" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="mapPartitions[U](func:Iterator[T]=&gt;Iterator[U])(implicitevidence$7:org.apache.spark.sql.Encoder[U]):org.apache.spark.sql.Dataset[U]" class="anchorToMember"></a><a id="mapPartitions[U]((Iterator[T])=&gt;Iterator[U])(Encoder[U]):Dataset[U]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#mapPartitions[U](func:Iterator[T]=&gt;Iterator[U])(implicitevidence$7:org.apache.spark.sql.Encoder[U]):org.apache.spark.sql.Dataset[U]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">mapPartitions</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="func">func: (<span name="scala.Iterator" class="extype">Iterator</span>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]) =&gt; <span name="scala.Iterator" class="extype">Iterator</span>[<span name="org.apache.spark.sql.Dataset.mapPartitions.U" class="extype">U</span>]</span>)</span><span class="params">(<span class="implicit">implicit </span><span name="arg0">arg0: <a href="Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.Dataset.mapPartitions.U" class="extype">U</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.mapPartitions.U" class="extype">U</span>]</span></span><p class="shortcomment cmt">(Scala-specific)
Returns a new Dataset that contains the result of applying <code>func</code> to each partition.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific)
Returns a new Dataset that contains the result of applying <code>func</code> to each partition.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#melt" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="melt(ids:Array[org.apache.spark.sql.Column],variableColumnName:String,valueColumnName:String):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="melt(Array[Column],String,String):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#melt(ids:Array[org.apache.spark.sql.Column],variableColumnName:String,valueColumnName:String):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">melt</span><span class="params">(<span name="ids">ids: <span name="scala.Array" class="extype">Array</span>[<a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>, <span name="variableColumnName">variableColumnName: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="valueColumnName">valueColumnName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns set.</p><div class="fullcomment"><div class="comment cmt"><p>Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns set.
This is the reverse to <code>groupBy(...).pivot(...).agg(...)</code>, except for the aggregation,
which cannot be reversed. This is an alias for <code>unpivot</code>.
</p></div><dl class="paramcmts block"><dt class="param">ids</dt><dd class="cmt"><p>Id columns</p></dd><dt class="param">variableColumnName</dt><dd class="cmt"><p>Name of the variable column</p></dd><dt class="param">valueColumnName</dt><dd class="cmt"><p>Name of the value column</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd><dt>See also</dt><dd><span class="cmt"><p><code>org.apache.spark.sql.Dataset.unpivot(Array, Array, String, String)</code>
This is equivalent to calling <code>Dataset#unpivot(Array, Array, String, String)</code>
where <code>values</code> is set to all non-id columns that exist in the DataFrame.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#melt" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="melt(ids:Array[org.apache.spark.sql.Column],values:Array[org.apache.spark.sql.Column],variableColumnName:String,valueColumnName:String):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="melt(Array[Column],Array[Column],String,String):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#melt(ids:Array[org.apache.spark.sql.Column],values:Array[org.apache.spark.sql.Column],variableColumnName:String,valueColumnName:String):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">melt</span><span class="params">(<span name="ids">ids: <span name="scala.Array" class="extype">Array</span>[<a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>, <span name="values">values: <span name="scala.Array" class="extype">Array</span>[<a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>, <span name="variableColumnName">variableColumnName: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="valueColumnName">valueColumnName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns set.</p><div class="fullcomment"><div class="comment cmt"><p>Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns set.
This is the reverse to <code>groupBy(...).pivot(...).agg(...)</code>, except for the aggregation,
which cannot be reversed. This is an alias for <code>unpivot</code>.
</p></div><dl class="paramcmts block"><dt class="param">ids</dt><dd class="cmt"><p>Id columns</p></dd><dt class="param">values</dt><dd class="cmt"><p>Value columns to unpivot</p></dd><dt class="param">variableColumnName</dt><dd class="cmt"><p>Name of the variable column</p></dd><dt class="param">valueColumnName</dt><dd class="cmt"><p>Name of the value column</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd><dt>See also</dt><dd><span class="cmt"><p><code>org.apache.spark.sql.Dataset.unpivot(Array, Array, String, String)</code></p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#mergeInto" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="mergeInto(table:String,condition:org.apache.spark.sql.Column):org.apache.spark.sql.MergeIntoWriter[T]" class="anchorToMember"></a><a id="mergeInto(String,Column):MergeIntoWriter[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#mergeInto(table:String,condition:org.apache.spark.sql.Column):org.apache.spark.sql.MergeIntoWriter[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">mergeInto</span><span class="params">(<span name="table">table: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="condition">condition: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>)</span><span class="result">: <a href="MergeIntoWriter.html" name="org.apache.spark.sql.MergeIntoWriter" id="org.apache.spark.sql.MergeIntoWriter" class="extype">MergeIntoWriter</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Merges a set of updates, insertions, and deletions based on a source table into
a target table.</p><div class="fullcomment"><div class="comment cmt"><p>Merges a set of updates, insertions, and deletions based on a source table into
a target table.</p><p>Scala Examples:</p><pre>spark.table(<span class="lit">"source"</span>)
  .mergeInto(<span class="lit">"target"</span>, $<span class="lit">"source.id"</span> === $<span class="lit">"target.id"</span>)
  .whenMatched($<span class="lit">"salary"</span> === <span class="num">100</span>)
  .delete()
  .whenNotMatched()
  .insertAll()
  .whenNotMatchedBySource($<span class="lit">"salary"</span> === <span class="num">100</span>)
  .update(<span class="std">Map</span>(
    <span class="lit">"salary"</span> -&gt; lit(<span class="num">200</span>)
  ))
  .merge()</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>4.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#metadataColumn" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="metadataColumn(colName:String):org.apache.spark.sql.Column" class="anchorToMember"></a><a id="metadataColumn(String):Column" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#metadataColumn(colName:String):org.apache.spark.sql.Column" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">metadataColumn</span><span class="params">(<span name="colName">colName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span></span><p class="shortcomment cmt">Selects a metadata column based on its logical column name, and returns it as a <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>.</p><div class="fullcomment"><div class="comment cmt"><p>Selects a metadata column based on its logical column name, and returns it as a <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>.</p><p>A metadata column can be accessed this way even if the underlying data source defines a data
column with a conflicting name.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.5.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#na" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="na:org.apache.spark.sql.DataFrameNaFunctions" class="anchorToMember"></a><a id="na:DataFrameNaFunctions" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#na:org.apache.spark.sql.DataFrameNaFunctions" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">na</span><span class="result">: <a href="DataFrameNaFunctions.html" name="org.apache.spark.sql.DataFrameNaFunctions" id="org.apache.spark.sql.DataFrameNaFunctions" class="extype">DataFrameNaFunctions</a></span></span><p class="shortcomment cmt">Returns a <a href="DataFrameNaFunctions.html" name="org.apache.spark.sql.DataFrameNaFunctions" id="org.apache.spark.sql.DataFrameNaFunctions" class="extype">DataFrameNaFunctions</a> for working with missing data.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a <a href="DataFrameNaFunctions.html" name="org.apache.spark.sql.DataFrameNaFunctions" id="org.apache.spark.sql.DataFrameNaFunctions" class="extype">DataFrameNaFunctions</a> for working with missing data.</p><pre><span class="cmt">// Dropping rows containing any null values.</span>
ds.na.drop()</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="scala.AnyRef#ne" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="ne(x$1:AnyRef):Boolean" class="anchorToMember"></a><a id="ne(AnyRef):Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#ne(x$1:AnyRef):Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">ne</span><span class="params">(<span name="arg0">arg0: <span name="scala.AnyRef" class="extype">AnyRef</span></span>)</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef</dd></dl></div></li><li class="indented0 " name="scala.AnyRef#notify" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="notify():Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#notify():Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">notify</span><span class="params">()</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd><span class="name">@IntrinsicCandidate</span><span class="args">()</span> <span class="name">@native</span><span class="args">()</span> </dd></dl></div></li><li class="indented0 " name="scala.AnyRef#notifyAll" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="notifyAll():Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#notifyAll():Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">notifyAll</span><span class="params">()</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd><span class="name">@IntrinsicCandidate</span><span class="args">()</span> <span class="name">@native</span><span class="args">()</span> </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#observe" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="observe(observation:org.apache.spark.sql.Observation,expr:org.apache.spark.sql.Column,exprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="observe(Observation,Column,Column*):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#observe(observation:org.apache.spark.sql.Observation,expr:org.apache.spark.sql.Column,exprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">observe</span><span class="params">(<span name="observation">observation: <a href="Observation.html" name="org.apache.spark.sql.Observation" id="org.apache.spark.sql.Observation" class="extype">Observation</a></span>, <span name="expr">expr: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>, <span name="exprs">exprs: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Observe (named) metrics through an <code>org.apache.spark.sql.Observation</code> instance.</p><div class="fullcomment"><div class="comment cmt"><p>Observe (named) metrics through an <code>org.apache.spark.sql.Observation</code> instance.
This is equivalent to calling <code>observe(String, Column, Column*)</code> but does not require
adding <code>org.apache.spark.sql.util.QueryExecutionListener</code> to the spark session.
This method does not support streaming datasets.</p><p>A user can retrieve the metrics by accessing <code>org.apache.spark.sql.Observation.get</code>.</p><pre><span class="cmt">// Observe row count (rows) and highest id (maxid) in the Dataset while writing it</span>
<span class="kw">val</span> observation = Observation(<span class="lit">"my_metrics"</span>)
<span class="kw">val</span> observed_ds = ds.observe(observation, count(lit(<span class="num">1</span>)).as(<span class="lit">"rows"</span>), max($<span class="lit">"id"</span>).as(<span class="lit">"maxid"</span>))
observed_ds.write.parquet(<span class="lit">"ds.parquet"</span>)
<span class="kw">val</span> metrics = observation.get</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.3.0</p></dd><dt>Exceptions thrown</dt><dd><span class="cmt"><p><span name="IllegalArgumentException" class="extype"><code>IllegalArgumentException</code></span> If this is a streaming Dataset (this.isStreaming == true)</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#observe" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="observe(name:String,expr:org.apache.spark.sql.Column,exprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="observe(String,Column,Column*):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#observe(name:String,expr:org.apache.spark.sql.Column,exprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">observe</span><span class="params">(<span name="name">name: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="expr">expr: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>, <span name="exprs">exprs: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Define (named) metrics to observe on the Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Define (named) metrics to observe on the Dataset. This method returns an 'observed' Dataset
that returns the same result as the input, with the following guarantees:</p><ul><li>It will compute the defined aggregates (metrics) on all the data that is flowing through
  the Dataset at that point.</li><li>It will report the value of the defined aggregate columns as soon as we reach a completion
  point. A completion point is either the end of a query (batch mode) or the end of a streaming
  epoch. The value of the aggregates only reflects the data processed since the previous
  completion point.</li></ul><p>Please note that continuous execution is currently not supported.</p><p>The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or
more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that
contain references to the input Dataset's columns must always be wrapped in an aggregate
function.</p><p>A user can observe these metrics by either adding
<a href="streaming/StreamingQueryListener.html" name="org.apache.spark.sql.streaming.StreamingQueryListener" id="org.apache.spark.sql.streaming.StreamingQueryListener" class="extype">org.apache.spark.sql.streaming.StreamingQueryListener</a> or a
<a href="util/QueryExecutionListener.html" name="org.apache.spark.sql.util.QueryExecutionListener" id="org.apache.spark.sql.util.QueryExecutionListener" class="extype">org.apache.spark.sql.util.QueryExecutionListener</a> to the spark session.</p><pre><span class="cmt">// Monitor the metrics using a listener.</span>
spark.streams.addListener(<span class="kw">new</span> StreamingQueryListener() {
  <span class="kw">override</span> <span class="kw">def</span> onQueryStarted(event: QueryStartedEvent): <span class="std">Unit</span> = {}
  <span class="kw">override</span> <span class="kw">def</span> onQueryProgress(event: QueryProgressEvent): <span class="std">Unit</span> = {
    event.progress.observedMetrics.asScala.get(<span class="lit">"my_event"</span>).foreach { row <span class="kw">=&gt;</span>
      <span class="cmt">// Trigger if the number of errors exceeds 5 percent</span>
      <span class="kw">val</span> num_rows = row.getAs[<span class="std">Long</span>](<span class="lit">"rc"</span>)
      <span class="kw">val</span> num_error_rows = row.getAs[<span class="std">Long</span>](<span class="lit">"erc"</span>)
      <span class="kw">val</span> ratio = num_error_rows.toDouble / num_rows
      <span class="kw">if</span> (ratio &gt; <span class="num">0.05</span>) {
        <span class="cmt">// Trigger alert</span>
      }
    }
  }
  <span class="kw">override</span> <span class="kw">def</span> onQueryTerminated(event: QueryTerminatedEvent): <span class="std">Unit</span> = {}
})
<span class="cmt">// Observe row count (rc) and error row count (erc) in the streaming Dataset</span>
<span class="kw">val</span> observed_ds = ds.observe(<span class="lit">"my_event"</span>, count(lit(<span class="num">1</span>)).as(<span class="lit">"rc"</span>), count($<span class="lit">"error"</span>).as(<span class="lit">"erc"</span>))
observed_ds.writeStream.format(<span class="lit">"..."</span>).start()</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#offset" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="offset(n:Int):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="offset(Int):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#offset(n:Int):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">offset</span><span class="params">(<span name="n">n: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset by skipping the first <code>n</code> rows.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by skipping the first <code>n</code> rows.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#orderBy" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="orderBy(sortExprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="orderBy(Column*):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#orderBy(sortExprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">orderBy</span><span class="params">(<span name="sortExprs">sortExprs: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset sorted by the given expressions.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset sorted by the given expressions.
This is an alias of the <code>sort</code> function.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#orderBy" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="orderBy(sortCol:String,sortCols:String*):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="orderBy(String,String*):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#orderBy(sortCol:String,sortCols:String*):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">orderBy</span><span class="params">(<span name="sortCol">sortCol: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="sortCols">sortCols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset sorted by the given expressions.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset sorted by the given expressions.
This is an alias of the <code>sort</code> function.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#persist" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="persist(newLevel:org.apache.spark.storage.StorageLevel):Dataset.this.type" class="anchorToMember"></a><a id="persist(StorageLevel):Dataset.this.type" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#persist(newLevel:org.apache.spark.storage.StorageLevel):Dataset.this.type" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">persist</span><span class="params">(<span name="newLevel">newLevel: <a href="../storage/StorageLevel.html" name="org.apache.spark.storage.StorageLevel" id="org.apache.spark.storage.StorageLevel" class="extype">StorageLevel</a></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>.this.type</span></span><p class="shortcomment cmt">Persist this Dataset with the given storage level.</p><div class="fullcomment"><div class="comment cmt"><p>Persist this Dataset with the given storage level.</p></div><dl class="paramcmts block"><dt class="param">newLevel</dt><dd class="cmt"><p>One of: <code>MEMORY_ONLY</code>, <code>MEMORY_AND_DISK</code>, <code>MEMORY_ONLY_SER</code>,
                <code>MEMORY_AND_DISK_SER</code>, <code>DISK_ONLY</code>, <code>MEMORY_ONLY_2</code>,
                <code>MEMORY_AND_DISK_2</code>, etc.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#persist" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="persist():Dataset.this.type" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#persist():Dataset.this.type" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">persist</span><span class="params">()</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>.this.type</span></span><p class="shortcomment cmt">Persist this Dataset with the default storage level (<code>MEMORY_AND_DISK</code>).</p><div class="fullcomment"><div class="comment cmt"><p>Persist this Dataset with the default storage level (<code>MEMORY_AND_DISK</code>).
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#printSchema" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="printSchema(level:Int):Unit" class="anchorToMember"></a><a id="printSchema(Int):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#printSchema(level:Int):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">printSchema</span><span class="params">(<span name="level">level: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Prints the schema up to the given level to the console in a nice tree format.</p><div class="fullcomment"><div class="comment cmt"><p>Prints the schema up to the given level to the console in a nice tree format.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#printSchema" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="printSchema():Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#printSchema():Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">printSchema</span><span class="params">()</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Prints the schema to the console in a nice tree format.</p><div class="fullcomment"><div class="comment cmt"><p>Prints the schema to the console in a nice tree format.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#queryExecution" group="Ungrouped" fullComment="no" data-isabs="false" visbl="pub"><a id="queryExecution:org.apache.spark.sql.execution.QueryExecution" class="anchorToMember"></a><a id="queryExecution:QueryExecution" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#queryExecution:org.apache.spark.sql.execution.QueryExecution" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">val</span></span> <span class="symbol"><span class="name">queryExecution</span><span class="result">: <span name="org.apache.spark.sql.execution.QueryExecution" class="extype">QueryExecution</span></span></span></li><li class="indented0 " name="org.apache.spark.sql.Dataset#randomSplit" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="randomSplit(weights:Array[Double]):Array[org.apache.spark.sql.Dataset[T]]" class="anchorToMember"></a><a id="randomSplit(Array[Double]):Array[Dataset[T]]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#randomSplit(weights:Array[Double]):Array[org.apache.spark.sql.Dataset[T]]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">randomSplit</span><span class="params">(<span name="weights">weights: <span name="scala.Array" class="extype">Array</span>[<span name="scala.Double" class="extype">Double</span>]</span>)</span><span class="result">: <span name="scala.Array" class="extype">Array</span>[<a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]]</span></span><p class="shortcomment cmt">Randomly splits this Dataset with the provided weights.</p><div class="fullcomment"><div class="comment cmt"><p>Randomly splits this Dataset with the provided weights.
</p></div><dl class="paramcmts block"><dt class="param">weights</dt><dd class="cmt"><p>weights for splits, will be normalized if they don't sum to 1.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#randomSplit" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="randomSplit(weights:Array[Double],seed:Long):Array[org.apache.spark.sql.Dataset[T]]" class="anchorToMember"></a><a id="randomSplit(Array[Double],Long):Array[Dataset[T]]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#randomSplit(weights:Array[Double],seed:Long):Array[org.apache.spark.sql.Dataset[T]]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">randomSplit</span><span class="params">(<span name="weights">weights: <span name="scala.Array" class="extype">Array</span>[<span name="scala.Double" class="extype">Double</span>]</span>, <span name="seed">seed: <span name="scala.Long" class="extype">Long</span></span>)</span><span class="result">: <span name="scala.Array" class="extype">Array</span>[<a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]]</span></span><p class="shortcomment cmt">Randomly splits this Dataset with the provided weights.</p><div class="fullcomment"><div class="comment cmt"><p>Randomly splits this Dataset with the provided weights.
</p></div><dl class="paramcmts block"><dt class="param">weights</dt><dd class="cmt"><p>weights for splits, will be normalized if they don't sum to 1.</p></dd><dt class="param">seed</dt><dd class="cmt"><p>Seed for sampling.
For Java API, use <a href="#randomSplitAsList(weights:Array[Double],seed:Long):java.util.List[org.apache.spark.sql.Dataset[T]]" name="org.apache.spark.sql.Dataset#randomSplitAsList" id="org.apache.spark.sql.Dataset#randomSplitAsList" class="extmbr">randomSplitAsList</a>.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#randomSplitAsList" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="randomSplitAsList(weights:Array[Double],seed:Long):java.util.List[org.apache.spark.sql.Dataset[T]]" class="anchorToMember"></a><a id="randomSplitAsList(Array[Double],Long):List[Dataset[T]]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#randomSplitAsList(weights:Array[Double],seed:Long):java.util.List[org.apache.spark.sql.Dataset[T]]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">randomSplitAsList</span><span class="params">(<span name="weights">weights: <span name="scala.Array" class="extype">Array</span>[<span name="scala.Double" class="extype">Double</span>]</span>, <span name="seed">seed: <span name="scala.Long" class="extype">Long</span></span>)</span><span class="result">: <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/List.html#java.util.List" name="java.util.List" id="java.util.List" class="extype">List</a>[<a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]]</span></span><p class="shortcomment cmt">Returns a Java list that contains randomly split Dataset with the provided weights.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a Java list that contains randomly split Dataset with the provided weights.
</p></div><dl class="paramcmts block"><dt class="param">weights</dt><dd class="cmt"><p>weights for splits, will be normalized if they don't sum to 1.</p></dd><dt class="param">seed</dt><dd class="cmt"><p>Seed for sampling.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#rdd" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="rdd:org.apache.spark.rdd.RDD[T]" class="anchorToMember"></a><a id="rdd:RDD[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#rdd:org.apache.spark.rdd.RDD[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">lazy val</span></span> <span class="symbol"><span class="name">rdd</span><span class="result">: <a href="../rdd/RDD.html" name="org.apache.spark.rdd.RDD" id="org.apache.spark.rdd.RDD" class="extype">RDD</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Represents the content of the Dataset as an <code>RDD</code> of <code>T</code>.</p><div class="fullcomment"><div class="comment cmt"><p>Represents the content of the Dataset as an <code>RDD</code> of <code>T</code>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#reduce" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="reduce(func:org.apache.spark.api.java.function.ReduceFunction[T]):T" class="anchorToMember"></a><a id="reduce(ReduceFunction[T]):T" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#reduce(func:org.apache.spark.api.java.function.ReduceFunction[T]):T" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">reduce</span><span class="params">(<span name="func">func: <a href="../api/java/function/ReduceFunction.html" name="org.apache.spark.api.java.function.ReduceFunction" id="org.apache.spark.api.java.function.ReduceFunction" class="extype">ReduceFunction</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.Dataset.T" class="extype">T</span></span></span><p class="shortcomment cmt">(Java-specific)
Reduces the elements of this Dataset using the specified binary function.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific)
Reduces the elements of this Dataset using the specified binary function. The given <code>func</code>
must be commutative and associative or the result may be non-deterministic.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#reduce" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="reduce(func:(T,T)=&gt;T):T" class="anchorToMember"></a><a id="reduce((T,T)=&gt;T):T" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#reduce(func:(T,T)=&gt;T):T" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">reduce</span><span class="params">(<span name="func">func: (<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>) =&gt; <span name="org.apache.spark.sql.Dataset.T" class="extype">T</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.Dataset.T" class="extype">T</span></span></span><p class="shortcomment cmt">(Scala-specific)
Reduces the elements of this Dataset using the specified binary function.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific)
Reduces the elements of this Dataset using the specified binary function. The given <code>func</code>
must be commutative and associative or the result may be non-deterministic.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#repartition" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="repartition(partitionExprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="repartition(Column*):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#repartition(partitionExprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">repartition</span><span class="params">(<span name="partitionExprs">partitionExprs: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset partitioned by the given partitioning expressions, using
<code>spark.sql.shuffle.partitions</code> as number of partitions.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset partitioned by the given partitioning expressions, using
<code>spark.sql.shuffle.partitions</code> as number of partitions.
The resulting Dataset is hash partitioned.</p><p>This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#repartition" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="repartition(numPartitions:Int,partitionExprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="repartition(Int,Column*):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#repartition(numPartitions:Int,partitionExprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">repartition</span><span class="params">(<span name="numPartitions">numPartitions: <span name="scala.Int" class="extype">Int</span></span>, <span name="partitionExprs">partitionExprs: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset partitioned by the given partitioning expressions into
<code>numPartitions</code>.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset partitioned by the given partitioning expressions into
<code>numPartitions</code>. The resulting Dataset is hash partitioned.</p><p>This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#repartition" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="repartition(numPartitions:Int):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="repartition(Int):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#repartition(numPartitions:Int):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">repartition</span><span class="params">(<span name="numPartitions">numPartitions: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset that has exactly <code>numPartitions</code> partitions.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset that has exactly <code>numPartitions</code> partitions.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#repartitionByRange" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="repartitionByRange(partitionExprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="repartitionByRange(Column*):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#repartitionByRange(partitionExprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">repartitionByRange</span><span class="params">(<span name="partitionExprs">partitionExprs: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset partitioned by the given partitioning expressions, using
<code>spark.sql.shuffle.partitions</code> as number of partitions.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset partitioned by the given partitioning expressions, using
<code>spark.sql.shuffle.partitions</code> as number of partitions.
The resulting Dataset is range partitioned.</p><p>At least one partition-by expression must be specified.
When no explicit sort order is specified, "ascending nulls first" is assumed.
Note, the rows are not sorted in each partition of the resulting Dataset.</p><p>Note that due to performance reasons this method uses sampling to estimate the ranges.
Hence, the output may not be consistent, since sampling can return different values.
The sample size can be controlled by the config
<code>spark.sql.execution.rangeExchange.sampleSizePerPartition</code>.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#repartitionByRange" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="repartitionByRange(numPartitions:Int,partitionExprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="repartitionByRange(Int,Column*):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#repartitionByRange(numPartitions:Int,partitionExprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">repartitionByRange</span><span class="params">(<span name="numPartitions">numPartitions: <span name="scala.Int" class="extype">Int</span></span>, <span name="partitionExprs">partitionExprs: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset partitioned by the given partitioning expressions into
<code>numPartitions</code>.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset partitioned by the given partitioning expressions into
<code>numPartitions</code>. The resulting Dataset is range partitioned.</p><p>At least one partition-by expression must be specified.
When no explicit sort order is specified, "ascending nulls first" is assumed.
Note, the rows are not sorted in each partition of the resulting Dataset.</p><p>Note that due to performance reasons this method uses sampling to estimate the ranges.
Hence, the output may not be consistent, since sampling can return different values.
The sample size can be controlled by the config
<code>spark.sql.execution.rangeExchange.sampleSizePerPartition</code>.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#rollup" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="rollup(col1:String,cols:String*):org.apache.spark.sql.RelationalGroupedDataset" class="anchorToMember"></a><a id="rollup(String,String*):RelationalGroupedDataset" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#rollup(col1:String,cols:String*):org.apache.spark.sql.RelationalGroupedDataset" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">rollup</span><span class="params">(<span name="col1">col1: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="cols">cols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.RelationalGroupedDataset" id="org.apache.spark.sql.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a></span></span><p class="shortcomment cmt">Create a multi-dimensional rollup for the current Dataset using the specified columns,
so we can run aggregation on them.</p><div class="fullcomment"><div class="comment cmt"><p>Create a multi-dimensional rollup for the current Dataset using the specified columns,
so we can run aggregation on them.
See <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.RelationalGroupedDataset" id="org.apache.spark.sql.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a> for all the available aggregate functions.</p><p>This is a variant of rollup that can only group by existing columns using column names
(i.e. cannot construct expressions).</p><pre><span class="cmt">// Compute the average for all numeric columns rolled up by department and group.</span>
ds.rollup(<span class="lit">"department"</span>, <span class="lit">"group"</span>).avg()

<span class="cmt">// Compute the max age and average salary, rolled up by department and gender.</span>
ds.rollup($<span class="lit">"department"</span>, $<span class="lit">"gender"</span>).agg(<span class="std">Map</span>(
  <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>,
  <span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>
))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#rollup" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="rollup(cols:org.apache.spark.sql.Column*):org.apache.spark.sql.RelationalGroupedDataset" class="anchorToMember"></a><a id="rollup(Column*):RelationalGroupedDataset" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#rollup(cols:org.apache.spark.sql.Column*):org.apache.spark.sql.RelationalGroupedDataset" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">rollup</span><span class="params">(<span name="cols">cols: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.RelationalGroupedDataset" id="org.apache.spark.sql.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a></span></span><p class="shortcomment cmt">Create a multi-dimensional rollup for the current Dataset using the specified columns,
so we can run aggregation on them.</p><div class="fullcomment"><div class="comment cmt"><p>Create a multi-dimensional rollup for the current Dataset using the specified columns,
so we can run aggregation on them.
See <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.RelationalGroupedDataset" id="org.apache.spark.sql.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a> for all the available aggregate functions.</p><pre><span class="cmt">// Compute the average for all numeric columns rolled up by department and group.</span>
ds.rollup($<span class="lit">"department"</span>, $<span class="lit">"group"</span>).avg()

<span class="cmt">// Compute the max age and average salary, rolled up by department and gender.</span>
ds.rollup($<span class="lit">"department"</span>, $<span class="lit">"gender"</span>).agg(<span class="std">Map</span>(
  <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>,
  <span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>
))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#sameSemantics" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="sameSemantics(other:org.apache.spark.sql.Dataset[T]):Boolean" class="anchorToMember"></a><a id="sameSemantics(Dataset[T]):Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#sameSemantics(other:org.apache.spark.sql.Dataset[T]):Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sameSemantics</span><span class="params">(<span name="other">other: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><p class="shortcomment cmt">Returns <code>true</code> when the logical query plans inside both <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>s are equal and
therefore return same results.</p><div class="fullcomment"><div class="comment cmt"><p>Returns <code>true</code> when the logical query plans inside both <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>s are equal and
therefore return same results.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@DeveloperApi</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.1.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>The equality comparison here is simplified by tolerating the cosmetic differences
      such as attribute names.</p></span>, <span class="cmt"><p>This API can compare both <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>s very fast but can still return <code>false</code> on
      the <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a> that return the same results, for instance, from different plans. Such
      false negative semantic can be useful when caching as an example.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#sample" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="sample(withReplacement:Boolean,fraction:Double):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="sample(Boolean,Double):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#sample(withReplacement:Boolean,fraction:Double):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sample</span><span class="params">(<span name="withReplacement">withReplacement: <span name="scala.Boolean" class="extype">Boolean</span></span>, <span name="fraction">fraction: <span name="scala.Double" class="extype">Double</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a> by sampling a fraction of rows, using a random seed.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a> by sampling a fraction of rows, using a random seed.
</p></div><dl class="paramcmts block"><dt class="param">withReplacement</dt><dd class="cmt"><p>Sample with replacement or not.</p></dd><dt class="param">fraction</dt><dd class="cmt"><p>Fraction of rows to generate, range [0.0, 1.0].</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>This is NOT guaranteed to provide exactly the fraction of the total count
of the given <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#sample" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="sample(withReplacement:Boolean,fraction:Double,seed:Long):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="sample(Boolean,Double,Long):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#sample(withReplacement:Boolean,fraction:Double,seed:Long):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sample</span><span class="params">(<span name="withReplacement">withReplacement: <span name="scala.Boolean" class="extype">Boolean</span></span>, <span name="fraction">fraction: <span name="scala.Double" class="extype">Double</span></span>, <span name="seed">seed: <span name="scala.Long" class="extype">Long</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a> by sampling a fraction of rows, using a user-supplied seed.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a> by sampling a fraction of rows, using a user-supplied seed.
</p></div><dl class="paramcmts block"><dt class="param">withReplacement</dt><dd class="cmt"><p>Sample with replacement or not.</p></dd><dt class="param">fraction</dt><dd class="cmt"><p>Fraction of rows to generate, range [0.0, 1.0].</p></dd><dt class="param">seed</dt><dd class="cmt"><p>Seed for sampling.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>This is NOT guaranteed to provide exactly the fraction of the count
of the given <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#sample" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="sample(fraction:Double):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="sample(Double):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#sample(fraction:Double):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sample</span><span class="params">(<span name="fraction">fraction: <span name="scala.Double" class="extype">Double</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a> by sampling a fraction of rows (without replacement),
using a random seed.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a> by sampling a fraction of rows (without replacement),
using a random seed.
</p></div><dl class="paramcmts block"><dt class="param">fraction</dt><dd class="cmt"><p>Fraction of rows to generate, range [0.0, 1.0].</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.3.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>This is NOT guaranteed to provide exactly the fraction of the count
of the given <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#sample" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="sample(fraction:Double,seed:Long):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="sample(Double,Long):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#sample(fraction:Double,seed:Long):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sample</span><span class="params">(<span name="fraction">fraction: <span name="scala.Double" class="extype">Double</span></span>, <span name="seed">seed: <span name="scala.Long" class="extype">Long</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a> by sampling a fraction of rows (without replacement),
using a user-supplied seed.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a> by sampling a fraction of rows (without replacement),
using a user-supplied seed.
</p></div><dl class="paramcmts block"><dt class="param">fraction</dt><dd class="cmt"><p>Fraction of rows to generate, range [0.0, 1.0].</p></dd><dt class="param">seed</dt><dd class="cmt"><p>Seed for sampling.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.3.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>This is NOT guaranteed to provide exactly the fraction of the count
of the given <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#schema" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="schema:org.apache.spark.sql.types.StructType" class="anchorToMember"></a><a id="schema:StructType" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#schema:org.apache.spark.sql.types.StructType" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">schema</span><span class="result">: <a href="types/StructType.html" name="org.apache.spark.sql.types.StructType" id="org.apache.spark.sql.types.StructType" class="extype">StructType</a></span></span><p class="shortcomment cmt">Returns the schema of this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the schema of this Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#select" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="select[U1,U2,U3,U4,U5](c1:org.apache.spark.sql.TypedColumn[T,U1],c2:org.apache.spark.sql.TypedColumn[T,U2],c3:org.apache.spark.sql.TypedColumn[T,U3],c4:org.apache.spark.sql.TypedColumn[T,U4],c5:org.apache.spark.sql.TypedColumn[T,U5]):org.apache.spark.sql.Dataset[(U1,U2,U3,U4,U5)]" class="anchorToMember"></a><a id="select[U1,U2,U3,U4,U5](TypedColumn[T,U1],TypedColumn[T,U2],TypedColumn[T,U3],TypedColumn[T,U4],TypedColumn[T,U5]):Dataset[(U1,U2,U3,U4,U5)]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#select[U1,U2,U3,U4,U5](c1:org.apache.spark.sql.TypedColumn[T,U1],c2:org.apache.spark.sql.TypedColumn[T,U2],c3:org.apache.spark.sql.TypedColumn[T,U3],c4:org.apache.spark.sql.TypedColumn[T,U4],c5:org.apache.spark.sql.TypedColumn[T,U5]):org.apache.spark.sql.Dataset[(U1,U2,U3,U4,U5)]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">select</span><span class="tparams">[<span name="U1">U1</span>, <span name="U2">U2</span>, <span name="U3">U3</span>, <span name="U4">U4</span>, <span name="U5">U5</span>]</span><span class="params">(<span name="c1">c1: <a href="TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.select.U1" class="extype">U1</span>]</span>, <span name="c2">c2: <a href="TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.select.U2" class="extype">U2</span>]</span>, <span name="c3">c3: <a href="TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.select.U3" class="extype">U3</span>]</span>, <span name="c4">c4: <a href="TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.select.U4" class="extype">U4</span>]</span>, <span name="c5">c5: <a href="TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.select.U5" class="extype">U5</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[(<span name="org.apache.spark.sql.Dataset.select.U1" class="extype">U1</span>, <span name="org.apache.spark.sql.Dataset.select.U2" class="extype">U2</span>, <span name="org.apache.spark.sql.Dataset.select.U3" class="extype">U3</span>, <span name="org.apache.spark.sql.Dataset.select.U4" class="extype">U4</span>, <span name="org.apache.spark.sql.Dataset.select.U5" class="extype">U5</span>)]</span></span><p class="shortcomment cmt">Returns a new Dataset by computing the given <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a> expressions for each element.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by computing the given <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a> expressions for each element.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#select" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="select[U1,U2,U3,U4](c1:org.apache.spark.sql.TypedColumn[T,U1],c2:org.apache.spark.sql.TypedColumn[T,U2],c3:org.apache.spark.sql.TypedColumn[T,U3],c4:org.apache.spark.sql.TypedColumn[T,U4]):org.apache.spark.sql.Dataset[(U1,U2,U3,U4)]" class="anchorToMember"></a><a id="select[U1,U2,U3,U4](TypedColumn[T,U1],TypedColumn[T,U2],TypedColumn[T,U3],TypedColumn[T,U4]):Dataset[(U1,U2,U3,U4)]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#select[U1,U2,U3,U4](c1:org.apache.spark.sql.TypedColumn[T,U1],c2:org.apache.spark.sql.TypedColumn[T,U2],c3:org.apache.spark.sql.TypedColumn[T,U3],c4:org.apache.spark.sql.TypedColumn[T,U4]):org.apache.spark.sql.Dataset[(U1,U2,U3,U4)]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">select</span><span class="tparams">[<span name="U1">U1</span>, <span name="U2">U2</span>, <span name="U3">U3</span>, <span name="U4">U4</span>]</span><span class="params">(<span name="c1">c1: <a href="TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.select.U1" class="extype">U1</span>]</span>, <span name="c2">c2: <a href="TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.select.U2" class="extype">U2</span>]</span>, <span name="c3">c3: <a href="TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.select.U3" class="extype">U3</span>]</span>, <span name="c4">c4: <a href="TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.select.U4" class="extype">U4</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[(<span name="org.apache.spark.sql.Dataset.select.U1" class="extype">U1</span>, <span name="org.apache.spark.sql.Dataset.select.U2" class="extype">U2</span>, <span name="org.apache.spark.sql.Dataset.select.U3" class="extype">U3</span>, <span name="org.apache.spark.sql.Dataset.select.U4" class="extype">U4</span>)]</span></span><p class="shortcomment cmt">Returns a new Dataset by computing the given <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a> expressions for each element.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by computing the given <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a> expressions for each element.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#select" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="select[U1,U2,U3](c1:org.apache.spark.sql.TypedColumn[T,U1],c2:org.apache.spark.sql.TypedColumn[T,U2],c3:org.apache.spark.sql.TypedColumn[T,U3]):org.apache.spark.sql.Dataset[(U1,U2,U3)]" class="anchorToMember"></a><a id="select[U1,U2,U3](TypedColumn[T,U1],TypedColumn[T,U2],TypedColumn[T,U3]):Dataset[(U1,U2,U3)]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#select[U1,U2,U3](c1:org.apache.spark.sql.TypedColumn[T,U1],c2:org.apache.spark.sql.TypedColumn[T,U2],c3:org.apache.spark.sql.TypedColumn[T,U3]):org.apache.spark.sql.Dataset[(U1,U2,U3)]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">select</span><span class="tparams">[<span name="U1">U1</span>, <span name="U2">U2</span>, <span name="U3">U3</span>]</span><span class="params">(<span name="c1">c1: <a href="TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.select.U1" class="extype">U1</span>]</span>, <span name="c2">c2: <a href="TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.select.U2" class="extype">U2</span>]</span>, <span name="c3">c3: <a href="TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.select.U3" class="extype">U3</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[(<span name="org.apache.spark.sql.Dataset.select.U1" class="extype">U1</span>, <span name="org.apache.spark.sql.Dataset.select.U2" class="extype">U2</span>, <span name="org.apache.spark.sql.Dataset.select.U3" class="extype">U3</span>)]</span></span><p class="shortcomment cmt">Returns a new Dataset by computing the given <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a> expressions for each element.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by computing the given <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a> expressions for each element.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#select" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="select[U1,U2](c1:org.apache.spark.sql.TypedColumn[T,U1],c2:org.apache.spark.sql.TypedColumn[T,U2]):org.apache.spark.sql.Dataset[(U1,U2)]" class="anchorToMember"></a><a id="select[U1,U2](TypedColumn[T,U1],TypedColumn[T,U2]):Dataset[(U1,U2)]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#select[U1,U2](c1:org.apache.spark.sql.TypedColumn[T,U1],c2:org.apache.spark.sql.TypedColumn[T,U2]):org.apache.spark.sql.Dataset[(U1,U2)]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">select</span><span class="tparams">[<span name="U1">U1</span>, <span name="U2">U2</span>]</span><span class="params">(<span name="c1">c1: <a href="TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.select.U1" class="extype">U1</span>]</span>, <span name="c2">c2: <a href="TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.select.U2" class="extype">U2</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[(<span name="org.apache.spark.sql.Dataset.select.U1" class="extype">U1</span>, <span name="org.apache.spark.sql.Dataset.select.U2" class="extype">U2</span>)]</span></span><p class="shortcomment cmt">Returns a new Dataset by computing the given <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a> expressions for each element.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by computing the given <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a> expressions for each element.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#select" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="select[U1](c1:org.apache.spark.sql.TypedColumn[T,U1]):org.apache.spark.sql.Dataset[U1]" class="anchorToMember"></a><a id="select[U1](TypedColumn[T,U1]):Dataset[U1]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#select[U1](c1:org.apache.spark.sql.TypedColumn[T,U1]):org.apache.spark.sql.Dataset[U1]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">select</span><span class="tparams">[<span name="U1">U1</span>]</span><span class="params">(<span name="c1">c1: <a href="TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.Dataset.select.U1" class="extype">U1</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.select.U1" class="extype">U1</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset by computing the given <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a> expression for each element.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by computing the given <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a> expression for each element.</p><pre><span class="kw">val</span> ds = <span class="std">Seq</span>(<span class="num">1</span>, <span class="num">2</span>, <span class="num">3</span>).toDS()
<span class="kw">val</span> newDS = ds.select(expr(<span class="lit">"value + 1"</span>).as[<span class="std">Int</span>])</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#select" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="select(col:String,cols:String*):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="select(String,String*):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#select(col:String,cols:String*):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">select</span><span class="params">(<span name="col">col: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="cols">cols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Selects a set of columns.</p><div class="fullcomment"><div class="comment cmt"><p>Selects a set of columns. This is a variant of <code>select</code> that can only select
existing columns using column names (i.e. cannot construct expressions).</p><pre><span class="cmt">// The following two are equivalent:</span>
ds.select(<span class="lit">"colA"</span>, <span class="lit">"colB"</span>)
ds.select($<span class="lit">"colA"</span>, $<span class="lit">"colB"</span>)</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#select" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="select(cols:org.apache.spark.sql.Column*):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="select(Column*):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#select(cols:org.apache.spark.sql.Column*):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">select</span><span class="params">(<span name="cols">cols: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Selects a set of column based expressions.</p><div class="fullcomment"><div class="comment cmt"><p>Selects a set of column based expressions.</p><pre>ds.select($<span class="lit">"colA"</span>, $<span class="lit">"colB"</span> + <span class="num">1</span>)</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#selectExpr" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="selectExpr(exprs:String*):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="selectExpr(String*):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#selectExpr(exprs:String*):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">selectExpr</span><span class="params">(<span name="exprs">exprs: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Selects a set of SQL expressions.</p><div class="fullcomment"><div class="comment cmt"><p>Selects a set of SQL expressions. This is a variant of <code>select</code> that accepts
SQL expressions.</p><pre><span class="cmt">// The following are equivalent:</span>
ds.selectExpr(<span class="lit">"colA"</span>, <span class="lit">"colB as newName"</span>, <span class="lit">"abs(colC)"</span>)
ds.select(expr(<span class="lit">"colA"</span>), expr(<span class="lit">"colB as newName"</span>), expr(<span class="lit">"abs(colC)"</span>))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#selectUntyped" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="prt"><a id="selectUntyped(columns:org.apache.spark.sql.TypedColumn[_,_]*):org.apache.spark.sql.Dataset[_]" class="anchorToMember"></a><a id="selectUntyped(TypedColumn[_,_]*):Dataset[_]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#selectUntyped(columns:org.apache.spark.sql.TypedColumn[_,_]*):org.apache.spark.sql.Dataset[_]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">selectUntyped</span><span class="params">(<span name="columns">columns: <a href="TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[_, _]*</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[_]</span></span><p class="shortcomment cmt">Internal helper function for building typed selects that return tuples.</p><div class="fullcomment"><div class="comment cmt"><p>Internal helper function for building typed selects that return tuples. For simplicity and
code reuse, we do this without the help of the type system and then use helper functions
that cast appropriately for the user facing interface.
</p></div><dl class="attributes block"><dt>Attributes</dt><dd>protected </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#semanticHash" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="semanticHash():Int" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#semanticHash():Int" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">semanticHash</span><span class="params">()</span><span class="result">: <span name="scala.Int" class="extype">Int</span></span></span><p class="shortcomment cmt">Returns a <code>hashCode</code> of the logical query plan against this <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a <code>hashCode</code> of the logical query plan against this <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@DeveloperApi</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.1.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>Unlike the standard <code>hashCode</code>, the hash is calculated against the query plan
      simplified by tolerating the cosmetic differences such as attribute names.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#show" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="show(numRows:Int,truncate:Int,vertical:Boolean):Unit" class="anchorToMember"></a><a id="show(Int,Int,Boolean):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#show(numRows:Int,truncate:Int,vertical:Boolean):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">show</span><span class="params">(<span name="numRows">numRows: <span name="scala.Int" class="extype">Int</span></span>, <span name="truncate">truncate: <span name="scala.Int" class="extype">Int</span></span>, <span name="vertical">vertical: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Displays the Dataset in a tabular form.</p><div class="fullcomment"><div class="comment cmt"><p>Displays the Dataset in a tabular form. For example:</p><pre>year  month AVG('Adj Close) MAX('Adj Close)
<span class="num">1980</span>  <span class="num">12</span>    <span class="num">0.503218</span>        <span class="num">0.595103</span>
<span class="num">1981</span>  <span class="num">01</span>    <span class="num">0.523289</span>        <span class="num">0.570307</span>
<span class="num">1982</span>  <span class="num">02</span>    <span class="num">0.436504</span>        <span class="num">0.475256</span>
<span class="num">1983</span>  <span class="num">03</span>    <span class="num">0.410516</span>        <span class="num">0.442194</span>
<span class="num">1984</span>  <span class="num">04</span>    <span class="num">0.450090</span>        <span class="num">0.483521</span></pre><p>If <code>vertical</code> enabled, this command prints output rows vertically (one line per column value)?</p><pre>-RECORD <span class="num">0</span>-------------------
 year            | <span class="num">1980</span>
 month           | <span class="num">12</span>
 AVG('Adj Close) | <span class="num">0.503218</span>
 AVG('Adj Close) | <span class="num">0.595103</span>
-RECORD <span class="num">1</span>-------------------
 year            | <span class="num">1981</span>
 month           | <span class="num">01</span>
 AVG('Adj Close) | <span class="num">0.523289</span>
 AVG('Adj Close) | <span class="num">0.570307</span>
-RECORD <span class="num">2</span>-------------------
 year            | <span class="num">1982</span>
 month           | <span class="num">02</span>
 AVG('Adj Close) | <span class="num">0.436504</span>
 AVG('Adj Close) | <span class="num">0.475256</span>
-RECORD <span class="num">3</span>-------------------
 year            | <span class="num">1983</span>
 month           | <span class="num">03</span>
 AVG('Adj Close) | <span class="num">0.410516</span>
 AVG('Adj Close) | <span class="num">0.442194</span>
-RECORD <span class="num">4</span>-------------------
 year            | <span class="num">1984</span>
 month           | <span class="num">04</span>
 AVG('Adj Close) | <span class="num">0.450090</span>
 AVG('Adj Close) | <span class="num">0.483521</span></pre></div><dl class="paramcmts block"><dt class="param">numRows</dt><dd class="cmt"><p>Number of rows to show</p></dd><dt class="param">truncate</dt><dd class="cmt"><p>If set to more than 0, truncates strings to <code>truncate</code> characters and
                   all cells will be aligned right.</p></dd><dt class="param">vertical</dt><dd class="cmt"><p>If set to true, prints output rows vertically (one line per column value).</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#show" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="show(numRows:Int,truncate:Int):Unit" class="anchorToMember"></a><a id="show(Int,Int):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#show(numRows:Int,truncate:Int):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">show</span><span class="params">(<span name="numRows">numRows: <span name="scala.Int" class="extype">Int</span></span>, <span name="truncate">truncate: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Displays the Dataset in a tabular form.</p><div class="fullcomment"><div class="comment cmt"><p>Displays the Dataset in a tabular form. For example:</p><pre>year  month AVG('Adj Close) MAX('Adj Close)
<span class="num">1980</span>  <span class="num">12</span>    <span class="num">0.503218</span>        <span class="num">0.595103</span>
<span class="num">1981</span>  <span class="num">01</span>    <span class="num">0.523289</span>        <span class="num">0.570307</span>
<span class="num">1982</span>  <span class="num">02</span>    <span class="num">0.436504</span>        <span class="num">0.475256</span>
<span class="num">1983</span>  <span class="num">03</span>    <span class="num">0.410516</span>        <span class="num">0.442194</span>
<span class="num">1984</span>  <span class="num">04</span>    <span class="num">0.450090</span>        <span class="num">0.483521</span></pre></div><dl class="paramcmts block"><dt class="param">numRows</dt><dd class="cmt"><p>Number of rows to show</p></dd><dt class="param">truncate</dt><dd class="cmt"><p>If set to more than 0, truncates strings to <code>truncate</code> characters and
                   all cells will be aligned right.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#show" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="show(numRows:Int,truncate:Boolean):Unit" class="anchorToMember"></a><a id="show(Int,Boolean):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#show(numRows:Int,truncate:Boolean):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">show</span><span class="params">(<span name="numRows">numRows: <span name="scala.Int" class="extype">Int</span></span>, <span name="truncate">truncate: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Displays the Dataset in a tabular form.</p><div class="fullcomment"><div class="comment cmt"><p>Displays the Dataset in a tabular form. For example:</p><pre>year  month AVG('Adj Close) MAX('Adj Close)
<span class="num">1980</span>  <span class="num">12</span>    <span class="num">0.503218</span>        <span class="num">0.595103</span>
<span class="num">1981</span>  <span class="num">01</span>    <span class="num">0.523289</span>        <span class="num">0.570307</span>
<span class="num">1982</span>  <span class="num">02</span>    <span class="num">0.436504</span>        <span class="num">0.475256</span>
<span class="num">1983</span>  <span class="num">03</span>    <span class="num">0.410516</span>        <span class="num">0.442194</span>
<span class="num">1984</span>  <span class="num">04</span>    <span class="num">0.450090</span>        <span class="num">0.483521</span></pre></div><dl class="paramcmts block"><dt class="param">numRows</dt><dd class="cmt"><p>Number of rows to show</p></dd><dt class="param">truncate</dt><dd class="cmt"><p>Whether truncate long strings. If true, strings more than 20 characters will
             be truncated and all cells will be aligned right</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#show" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="show(truncate:Boolean):Unit" class="anchorToMember"></a><a id="show(Boolean):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#show(truncate:Boolean):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">show</span><span class="params">(<span name="truncate">truncate: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Displays the top 20 rows of Dataset in a tabular form.</p><div class="fullcomment"><div class="comment cmt"><p>Displays the top 20 rows of Dataset in a tabular form.
</p></div><dl class="paramcmts block"><dt class="param">truncate</dt><dd class="cmt"><p>Whether truncate long strings. If true, strings more than 20 characters will
                be truncated and all cells will be aligned right</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#show" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="show():Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#show():Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">show</span><span class="params">()</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Displays the top 20 rows of Dataset in a tabular form.</p><div class="fullcomment"><div class="comment cmt"><p>Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters
will be truncated, and all cells will be aligned right.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#show" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="show(numRows:Int):Unit" class="anchorToMember"></a><a id="show(Int):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#show(numRows:Int):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">show</span><span class="params">(<span name="numRows">numRows: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Displays the Dataset in a tabular form.</p><div class="fullcomment"><div class="comment cmt"><p>Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,
and all cells will be aligned right. For example:</p><pre>year  month AVG('Adj Close) MAX('Adj Close)
<span class="num">1980</span>  <span class="num">12</span>    <span class="num">0.503218</span>        <span class="num">0.595103</span>
<span class="num">1981</span>  <span class="num">01</span>    <span class="num">0.523289</span>        <span class="num">0.570307</span>
<span class="num">1982</span>  <span class="num">02</span>    <span class="num">0.436504</span>        <span class="num">0.475256</span>
<span class="num">1983</span>  <span class="num">03</span>    <span class="num">0.410516</span>        <span class="num">0.442194</span>
<span class="num">1984</span>  <span class="num">04</span>    <span class="num">0.450090</span>        <span class="num">0.483521</span></pre></div><dl class="paramcmts block"><dt class="param">numRows</dt><dd class="cmt"><p>Number of rows to show</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#sort" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="sort(sortExprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="sort(Column*):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#sort(sortExprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sort</span><span class="params">(<span name="sortExprs">sortExprs: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset sorted by the given expressions.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset sorted by the given expressions. For example:</p><pre>ds.sort($<span class="lit">"col1"</span>, $<span class="lit">"col2"</span>.desc)</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#sort" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="sort(sortCol:String,sortCols:String*):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="sort(String,String*):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#sort(sortCol:String,sortCols:String*):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sort</span><span class="params">(<span name="sortCol">sortCol: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="sortCols">sortCols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset sorted by the specified column, all in ascending order.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset sorted by the specified column, all in ascending order.</p><pre><span class="cmt">// The following 3 are equivalent</span>
ds.sort(<span class="lit">"sortcol"</span>)
ds.sort($<span class="lit">"sortcol"</span>)
ds.sort($<span class="lit">"sortcol"</span>.asc)</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#sortWithinPartitions" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="sortWithinPartitions(sortExprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="sortWithinPartitions(Column*):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#sortWithinPartitions(sortExprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sortWithinPartitions</span><span class="params">(<span name="sortExprs">sortExprs: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset with each partition sorted by the given expressions.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with each partition sorted by the given expressions.</p><p>This is the same operation as "SORT BY" in SQL (Hive QL).
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#sortWithinPartitions" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="sortWithinPartitions(sortCol:String,sortCols:String*):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="sortWithinPartitions(String,String*):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#sortWithinPartitions(sortCol:String,sortCols:String*):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sortWithinPartitions</span><span class="params">(<span name="sortCol">sortCol: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="sortCols">sortCols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset with each partition sorted by the given expressions.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with each partition sorted by the given expressions.</p><p>This is the same operation as "SORT BY" in SQL (Hive QL).
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#sparkSession" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="sparkSession:org.apache.spark.sql.SparkSession" class="anchorToMember"></a><a id="sparkSession:SparkSession" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#sparkSession:org.apache.spark.sql.SparkSession" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">lazy val</span></span> <span class="symbol"><span class="name">sparkSession</span><span class="result">: <a href="SparkSession.html" name="org.apache.spark.sql.SparkSession" id="org.apache.spark.sql.SparkSession" class="extype">SparkSession</a></span></span><div class="fullcomment"><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@transient</span><span class="args">()</span> </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#sqlContext" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="sqlContext:org.apache.spark.sql.SQLContext" class="anchorToMember"></a><a id="sqlContext:SQLContext" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#sqlContext:org.apache.spark.sql.SQLContext" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">lazy val</span></span> <span class="symbol"><span class="name">sqlContext</span><span class="result">: <a href="SQLContext.html" name="org.apache.spark.sql.SQLContext" id="org.apache.spark.sql.SQLContext" class="extype">SQLContext</a></span></span><div class="fullcomment"><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@transient</span><span class="args">()</span> </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#stat" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="stat:org.apache.spark.sql.DataFrameStatFunctions" class="anchorToMember"></a><a id="stat:DataFrameStatFunctions" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#stat:org.apache.spark.sql.DataFrameStatFunctions" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">stat</span><span class="result">: <a href="DataFrameStatFunctions.html" name="org.apache.spark.sql.DataFrameStatFunctions" id="org.apache.spark.sql.DataFrameStatFunctions" class="extype">DataFrameStatFunctions</a></span></span><p class="shortcomment cmt">Returns a <a href="DataFrameStatFunctions.html" name="org.apache.spark.sql.DataFrameStatFunctions" id="org.apache.spark.sql.DataFrameStatFunctions" class="extype">DataFrameStatFunctions</a> for working statistic functions support.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a <a href="DataFrameStatFunctions.html" name="org.apache.spark.sql.DataFrameStatFunctions" id="org.apache.spark.sql.DataFrameStatFunctions" class="extype">DataFrameStatFunctions</a> for working statistic functions support.</p><pre><span class="cmt">// Finding frequent items in column with name 'a'.</span>
ds.stat.freqItems(<span class="std">Seq</span>(<span class="lit">"a"</span>))</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#storageLevel" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="storageLevel:org.apache.spark.storage.StorageLevel" class="anchorToMember"></a><a id="storageLevel:StorageLevel" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#storageLevel:org.apache.spark.storage.StorageLevel" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">storageLevel</span><span class="result">: <a href="../storage/StorageLevel.html" name="org.apache.spark.storage.StorageLevel" id="org.apache.spark.storage.StorageLevel" class="extype">StorageLevel</a></span></span><p class="shortcomment cmt">Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.</p><div class="fullcomment"><div class="comment cmt"><p>Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.1.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#summary" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="summary(statistics:String*):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="summary(String*):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#summary(statistics:String*):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">summary</span><span class="params">(<span name="statistics">statistics: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Computes specified statistics for numeric and string columns.</p><div class="fullcomment"><div class="comment cmt"><p>Computes specified statistics for numeric and string columns. Available statistics are:</p><ul><li>count</li><li>mean</li><li>stddev</li><li>min</li><li>max</li><li>arbitrary approximate percentiles specified as a percentage (e.g. 75%)</li><li>count_distinct</li><li>approx_count_distinct</li></ul><p>If no statistics are given, this function computes count, mean, stddev, min,
approximate quartiles (percentiles at 25%, 50%, and 75%), and max.</p><p>This function is meant for exploratory data analysis, as we make no guarantee about the
backward compatibility of the schema of the resulting Dataset. If you want to
programmatically compute summary statistics, use the <code>agg</code> function instead.</p><pre>ds.summary().show()

<span class="cmt">// output:</span>
<span class="cmt">// summary age   height</span>
<span class="cmt">// count   10.0  10.0</span>
<span class="cmt">// mean    53.3  178.05</span>
<span class="cmt">// stddev  11.6  15.7</span>
<span class="cmt">// min     18.0  163.0</span>
<span class="cmt">// 25%     24.0  176.0</span>
<span class="cmt">// 50%     24.0  176.0</span>
<span class="cmt">// 75%     32.0  180.0</span>
<span class="cmt">// max     92.0  192.0</span></pre><pre>ds.summary(<span class="lit">"count"</span>, <span class="lit">"min"</span>, <span class="lit">"25%"</span>, <span class="lit">"75%"</span>, <span class="lit">"max"</span>).show()

<span class="cmt">// output:</span>
<span class="cmt">// summary age   height</span>
<span class="cmt">// count   10.0  10.0</span>
<span class="cmt">// min     18.0  163.0</span>
<span class="cmt">// 25%     24.0  176.0</span>
<span class="cmt">// 75%     32.0  180.0</span>
<span class="cmt">// max     92.0  192.0</span></pre><p>To do a summary for specific columns first select them:</p><pre>ds.select(<span class="lit">"age"</span>, <span class="lit">"height"</span>).summary().show()</pre><p>Specify statistics to output custom summaries:</p><pre>ds.summary(<span class="lit">"count"</span>, <span class="lit">"count_distinct"</span>).show()</pre><p>The distinct count isn't included by default.</p><p>You can also run approximate distinct counts which are faster:</p><pre>ds.summary(<span class="lit">"count"</span>, <span class="lit">"approx_count_distinct"</span>).show()</pre><p>See also <a href="#describe(cols:String*):org.apache.spark.sql.DataFrame" name="org.apache.spark.sql.Dataset#describe" id="org.apache.spark.sql.Dataset#describe" class="extmbr">describe</a> for basic statistics.
</p></div><dl class="paramcmts block"><dt class="param">statistics</dt><dd class="cmt"><p>Statistics from above list to be computed.</p></dd></dl><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.3.0</p></dd></dl></div></li><li class="indented0 " name="scala.AnyRef#synchronized" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="synchronized[T0](x$1:=&gt;T0):T0" class="anchorToMember"></a><a id="synchronized[T0](=&gt;T0):T0" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#synchronized[T0](x$1:=&gt;T0):T0" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">synchronized</span><span class="tparams">[<span name="T0">T0</span>]</span><span class="params">(<span name="arg0">arg0: =&gt; <span name="java.lang.AnyRef.synchronized.T0" class="extype">T0</span></span>)</span><span class="result">: <span name="java.lang.AnyRef.synchronized.T0" class="extype">T0</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef</dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#tail" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="tail(n:Int):Array[T]" class="anchorToMember"></a><a id="tail(Int):Array[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#tail(n:Int):Array[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">tail</span><span class="params">(<span name="n">n: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <span name="scala.Array" class="extype">Array</span>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns the last <code>n</code> rows in the Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the last <code>n</code> rows in the Dataset.</p><p>Running tail requires moving data into the application's driver process, and doing so with
a very large <code>n</code> can crash the driver process with OutOfMemoryError.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#take" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="take(n:Int):Array[T]" class="anchorToMember"></a><a id="take(Int):Array[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#take(n:Int):Array[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">take</span><span class="params">(<span name="n">n: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <span name="scala.Array" class="extype">Array</span>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns the first <code>n</code> rows in the Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the first <code>n</code> rows in the Dataset.</p><p>Running take requires moving data into the application's driver process, and doing so with
a very large <code>n</code> can crash the driver process with OutOfMemoryError.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#takeAsList" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="takeAsList(n:Int):java.util.List[T]" class="anchorToMember"></a><a id="takeAsList(Int):List[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#takeAsList(n:Int):java.util.List[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">takeAsList</span><span class="params">(<span name="n">n: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/List.html#java.util.List" name="java.util.List" id="java.util.List" class="extype">List</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns the first <code>n</code> rows in the Dataset as a list.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the first <code>n</code> rows in the Dataset as a list.</p><p>Running take requires moving data into the application's driver process, and doing so with
a very large <code>n</code> can crash the driver process with OutOfMemoryError.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#to" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="to(schema:org.apache.spark.sql.types.StructType):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="to(StructType):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#to(schema:org.apache.spark.sql.types.StructType):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">to</span><span class="params">(<span name="schema">schema: <a href="types/StructType.html" name="org.apache.spark.sql.types.StructType" id="org.apache.spark.sql.types.StructType" class="extype">StructType</a></span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Returns a new DataFrame where each row is reconciled to match the specified schema.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new DataFrame where each row is reconciled to match the specified schema. Spark will:</p><ul><li>Reorder columns and/or inner fields by name to match the specified schema.</li><li>Project away columns and/or inner fields that are not needed by the specified schema.
  Missing columns and/or inner fields (present in the specified schema but not input DataFrame)
  lead to failures.</li><li>Cast the columns and/or inner fields to match the data types in the specified schema, if
  the types are compatible, e.g., numeric to numeric (error if overflows), but not string to
  int.</li><li>Carry over the metadata from the specified schema, while the columns and/or inner fields
  still keep their own metadata if not overwritten by the specified schema.</li><li>Fail if the nullability is not compatible. For example, the column and/or inner field is
  nullable but the specified schema requires them to be not nullable.</li></ul></div><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#toDF" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="toDF(colNames:String*):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="toDF(String*):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#toDF(colNames:String*):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">toDF</span><span class="params">(<span name="colNames">colNames: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Converts this strongly typed collection of data to generic <code>DataFrame</code> with columns renamed.</p><div class="fullcomment"><div class="comment cmt"><p>Converts this strongly typed collection of data to generic <code>DataFrame</code> with columns renamed.
This can be quite convenient in conversion from an RDD of tuples into a <code>DataFrame</code> with
meaningful names. For example:</p><pre><span class="kw">val</span> rdd: RDD[(<span class="std">Int</span>, <span class="std">String</span>)] = ...
rdd.toDF()  <span class="cmt">// this implicit conversion creates a DataFrame with column name `_1` and `_2`</span>
rdd.toDF(<span class="lit">"id"</span>, <span class="lit">"name"</span>)  <span class="cmt">// this creates a DataFrame with column name "id" and "name"</span></pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#toDF" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="toDF():org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="toDF():DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#toDF():org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">toDF</span><span class="params">()</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Converts this strongly typed collection of data to generic Dataframe.</p><div class="fullcomment"><div class="comment cmt"><p>Converts this strongly typed collection of data to generic Dataframe. In contrast to the
strongly typed objects that Dataset operations work on, a Dataframe returns generic <a href="Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>
objects that allow fields to be accessed by ordinal or name.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#toJSON" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="toJSON:org.apache.spark.sql.Dataset[String]" class="anchorToMember"></a><a id="toJSON:Dataset[String]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#toJSON:org.apache.spark.sql.Dataset[String]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">toJSON</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="scala.Predef.String" class="extype">String</span>]</span></span><p class="shortcomment cmt">Returns the content of the Dataset as a Dataset of JSON strings.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the content of the Dataset as a Dataset of JSON strings.</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#toJavaRDD" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="toJavaRDD:org.apache.spark.api.java.JavaRDD[T]" class="anchorToMember"></a><a id="toJavaRDD:JavaRDD[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#toJavaRDD:org.apache.spark.api.java.JavaRDD[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">toJavaRDD</span><span class="result">: <a href="../api/java/JavaRDD.html" name="org.apache.spark.api.java.JavaRDD" id="org.apache.spark.api.java.JavaRDD" class="extype">JavaRDD</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns the content of the Dataset as a <code>JavaRDD</code> of <code>T</code>s.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the content of the Dataset as a <code>JavaRDD</code> of <code>T</code>s.</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#toLocalIterator" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="toLocalIterator():java.util.Iterator[T]" class="anchorToMember"></a><a id="toLocalIterator():Iterator[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#toLocalIterator():java.util.Iterator[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">toLocalIterator</span><span class="params">()</span><span class="result">: <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/Iterator.html#java.util.Iterator" name="java.util.Iterator" id="java.util.Iterator" class="extype">Iterator</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns an iterator that contains all rows in this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns an iterator that contains all rows in this Dataset.</p><p>The iterator will consume as much memory as the largest partition in this Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>this results in multiple Spark jobs, and if the input Dataset is the result
of a wide transformation (e.g. join with different partitioners), to avoid
recomputing the input Dataset should be cached first.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#toString" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="toString():String" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#toString():String" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">toString</span><span class="params">()</span><span class="result">: <span name="scala.Predef.String" class="extype">String</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a> → AnyRef → Any</dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#transform" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="transform[U](t:org.apache.spark.sql.Dataset[T]=&gt;org.apache.spark.sql.Dataset[U]):org.apache.spark.sql.Dataset[U]" class="anchorToMember"></a><a id="transform[U]((Dataset[T])=&gt;Dataset[U]):Dataset[U]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#transform[U](t:org.apache.spark.sql.Dataset[T]=&gt;org.apache.spark.sql.Dataset[U]):org.apache.spark.sql.Dataset[U]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">transform</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="t">t: (<a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]) =&gt; <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.transform.U" class="extype">U</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.transform.U" class="extype">U</span>]</span></span><p class="shortcomment cmt">Concise syntax for chaining custom transformations.</p><div class="fullcomment"><div class="comment cmt"><p>Concise syntax for chaining custom transformations.</p><pre><span class="kw">def</span> featurize(ds: Dataset[T]): Dataset[U] = ...

ds
  .transform(featurize)
  .transform(...)</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#union" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="union(other:org.apache.spark.sql.Dataset[T]):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="union(Dataset[T]):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#union(other:org.apache.spark.sql.Dataset[T]):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">union</span><span class="params">(<span name="other">other: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset containing union of rows in this Dataset and another Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset containing union of rows in this Dataset and another Dataset.</p><p>This is equivalent to <code>UNION ALL</code> in SQL. To do a SQL-style set union (that does
deduplication of elements), use this function followed by a <a href="#distinct():org.apache.spark.sql.Dataset[T]" name="org.apache.spark.sql.Dataset#distinct" id="org.apache.spark.sql.Dataset#distinct" class="extmbr">distinct</a>.</p><p>Also as standard in SQL, this function resolves columns by position (not by name):</p><pre><span class="kw">val</span> df1 = <span class="std">Seq</span>((<span class="num">1</span>, <span class="num">2</span>, <span class="num">3</span>)).toDF(<span class="lit">"col0"</span>, <span class="lit">"col1"</span>, <span class="lit">"col2"</span>)
<span class="kw">val</span> df2 = <span class="std">Seq</span>((<span class="num">4</span>, <span class="num">5</span>, <span class="num">6</span>)).toDF(<span class="lit">"col1"</span>, <span class="lit">"col2"</span>, <span class="lit">"col0"</span>)
df1.union(df2).show

<span class="cmt">// output:</span>
<span class="cmt">// +----+----+----+</span>
<span class="cmt">// |col0|col1|col2|</span>
<span class="cmt">// +----+----+----+</span>
<span class="cmt">// |   1|   2|   3|</span>
<span class="cmt">// |   4|   5|   6|</span>
<span class="cmt">// +----+----+----+</span></pre><p>Notice that the column positions in the schema aren't necessarily matched with the
fields in the strongly typed objects in a Dataset. This function resolves columns
by their positions in the schema, not the fields in the strongly typed objects. Use
<a href="#unionByName(other:org.apache.spark.sql.Dataset[T],allowMissingColumns:Boolean):org.apache.spark.sql.Dataset[T]" name="org.apache.spark.sql.Dataset#unionByName" id="org.apache.spark.sql.Dataset#unionByName" class="extmbr">unionByName</a> to resolve columns by field name in the typed objects.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#unionAll" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="unionAll(other:org.apache.spark.sql.Dataset[T]):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="unionAll(Dataset[T]):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#unionAll(other:org.apache.spark.sql.Dataset[T]):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">unionAll</span><span class="params">(<span name="other">other: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset containing union of rows in this Dataset and another Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset containing union of rows in this Dataset and another Dataset.
This is an alias for <code>union</code>.</p><p>This is equivalent to <code>UNION ALL</code> in SQL. To do a SQL-style set union (that does
deduplication of elements), use this function followed by a <a href="#distinct():org.apache.spark.sql.Dataset[T]" name="org.apache.spark.sql.Dataset#distinct" id="org.apache.spark.sql.Dataset#distinct" class="extmbr">distinct</a>.</p><p>Also as standard in SQL, this function resolves columns by position (not by name).
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#unionByName" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="unionByName(other:org.apache.spark.sql.Dataset[T],allowMissingColumns:Boolean):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="unionByName(Dataset[T],Boolean):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#unionByName(other:org.apache.spark.sql.Dataset[T],allowMissingColumns:Boolean):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">unionByName</span><span class="params">(<span name="other">other: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span>, <span name="allowMissingColumns">allowMissingColumns: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset containing union of rows in this Dataset and another Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset containing union of rows in this Dataset and another Dataset.</p><p>The difference between this function and <a href="#union(other:org.apache.spark.sql.Dataset[T]):org.apache.spark.sql.Dataset[T]" name="org.apache.spark.sql.Dataset#union" id="org.apache.spark.sql.Dataset#union" class="extmbr">union</a> is that this function
resolves columns by name (not by position).</p><p>When the parameter <code>allowMissingColumns</code> is <code>true</code>, the set of column names
in this and other <code>Dataset</code> can differ; missing columns will be filled with null.
Further, the missing columns of this <code>Dataset</code> will be added at the end
in the schema of the union result:</p><pre><span class="kw">val</span> df1 = <span class="std">Seq</span>((<span class="num">1</span>, <span class="num">2</span>, <span class="num">3</span>)).toDF(<span class="lit">"col0"</span>, <span class="lit">"col1"</span>, <span class="lit">"col2"</span>)
<span class="kw">val</span> df2 = <span class="std">Seq</span>((<span class="num">4</span>, <span class="num">5</span>, <span class="num">6</span>)).toDF(<span class="lit">"col1"</span>, <span class="lit">"col0"</span>, <span class="lit">"col3"</span>)
df1.unionByName(df2, <span class="kw">true</span>).show

<span class="cmt">// output: "col3" is missing at left df1 and added at the end of schema.</span>
<span class="cmt">// +----+----+----+----+</span>
<span class="cmt">// |col0|col1|col2|col3|</span>
<span class="cmt">// +----+----+----+----+</span>
<span class="cmt">// |   1|   2|   3|NULL|</span>
<span class="cmt">// |   5|   4|NULL|   6|</span>
<span class="cmt">// +----+----+----+----+</span>

df2.unionByName(df1, <span class="kw">true</span>).show

<span class="cmt">// output: "col2" is missing at left df2 and added at the end of schema.</span>
<span class="cmt">// +----+----+----+----+</span>
<span class="cmt">// |col1|col0|col3|col2|</span>
<span class="cmt">// +----+----+----+----+</span>
<span class="cmt">// |   4|   5|   6|NULL|</span>
<span class="cmt">// |   2|   1|NULL|   3|</span>
<span class="cmt">// +----+----+----+----+</span></pre><p>Note that this supports nested columns in struct and array types. With <code>allowMissingColumns</code>,
missing nested columns of struct columns with the same name will also be filled with null
values and added to the end of struct. Nested columns in map types are not currently
supported.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.1.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#unionByName" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="unionByName(other:org.apache.spark.sql.Dataset[T]):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="unionByName(Dataset[T]):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#unionByName(other:org.apache.spark.sql.Dataset[T]):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">unionByName</span><span class="params">(<span name="other">other: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset containing union of rows in this Dataset and another Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset containing union of rows in this Dataset and another Dataset.</p><p>This is different from both <code>UNION ALL</code> and <code>UNION DISTINCT</code> in SQL. To do a SQL-style set
union (that does deduplication of elements), use this function followed by a <a href="#distinct():org.apache.spark.sql.Dataset[T]" name="org.apache.spark.sql.Dataset#distinct" id="org.apache.spark.sql.Dataset#distinct" class="extmbr">distinct</a>.</p><p>The difference between this function and <a href="#union(other:org.apache.spark.sql.Dataset[T]):org.apache.spark.sql.Dataset[T]" name="org.apache.spark.sql.Dataset#union" id="org.apache.spark.sql.Dataset#union" class="extmbr">union</a> is that this function
resolves columns by name (not by position):</p><pre><span class="kw">val</span> df1 = <span class="std">Seq</span>((<span class="num">1</span>, <span class="num">2</span>, <span class="num">3</span>)).toDF(<span class="lit">"col0"</span>, <span class="lit">"col1"</span>, <span class="lit">"col2"</span>)
<span class="kw">val</span> df2 = <span class="std">Seq</span>((<span class="num">4</span>, <span class="num">5</span>, <span class="num">6</span>)).toDF(<span class="lit">"col1"</span>, <span class="lit">"col2"</span>, <span class="lit">"col0"</span>)
df1.unionByName(df2).show

<span class="cmt">// output:</span>
<span class="cmt">// +----+----+----+</span>
<span class="cmt">// |col0|col1|col2|</span>
<span class="cmt">// +----+----+----+</span>
<span class="cmt">// |   1|   2|   3|</span>
<span class="cmt">// |   6|   4|   5|</span>
<span class="cmt">// +----+----+----+</span></pre><p>Note that this supports nested columns in struct and array types. Nested columns in map types
are not currently supported.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#unpersist" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="unpersist():Dataset.this.type" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#unpersist():Dataset.this.type" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">unpersist</span><span class="params">()</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>.this.type</span></span><p class="shortcomment cmt">Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.</p><div class="fullcomment"><div class="comment cmt"><p>Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
This will not un-persist any cached data that is built upon this Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#unpersist" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="unpersist(blocking:Boolean):Dataset.this.type" class="anchorToMember"></a><a id="unpersist(Boolean):Dataset.this.type" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#unpersist(blocking:Boolean):Dataset.this.type" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">unpersist</span><span class="params">(<span name="blocking">blocking: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>.this.type</span></span><p class="shortcomment cmt">Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.</p><div class="fullcomment"><div class="comment cmt"><p>Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
This will not un-persist any cached data that is built upon this Dataset.
</p></div><dl class="paramcmts block"><dt class="param">blocking</dt><dd class="cmt"><p>Whether to block until all blocks are deleted.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#unpivot" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="unpivot(ids:Array[org.apache.spark.sql.Column],variableColumnName:String,valueColumnName:String):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="unpivot(Array[Column],String,String):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#unpivot(ids:Array[org.apache.spark.sql.Column],variableColumnName:String,valueColumnName:String):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">unpivot</span><span class="params">(<span name="ids">ids: <span name="scala.Array" class="extype">Array</span>[<a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>, <span name="variableColumnName">variableColumnName: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="valueColumnName">valueColumnName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns set.</p><div class="fullcomment"><div class="comment cmt"><p>Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns set.
This is the reverse to <code>groupBy(...).pivot(...).agg(...)</code>, except for the aggregation,
which cannot be reversed.
</p></div><dl class="paramcmts block"><dt class="param">ids</dt><dd class="cmt"><p>Id columns</p></dd><dt class="param">variableColumnName</dt><dd class="cmt"><p>Name of the variable column</p></dd><dt class="param">valueColumnName</dt><dd class="cmt"><p>Name of the value column</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd><dt>See also</dt><dd><span class="cmt"><p><code>org.apache.spark.sql.Dataset.unpivot(Array, Array, String, String)</code>
This is equivalent to calling <code>Dataset#unpivot(Array, Array, String, String)</code>
where <code>values</code> is set to all non-id columns that exist in the DataFrame.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#unpivot" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="unpivot(ids:Array[org.apache.spark.sql.Column],values:Array[org.apache.spark.sql.Column],variableColumnName:String,valueColumnName:String):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="unpivot(Array[Column],Array[Column],String,String):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#unpivot(ids:Array[org.apache.spark.sql.Column],values:Array[org.apache.spark.sql.Column],variableColumnName:String,valueColumnName:String):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">unpivot</span><span class="params">(<span name="ids">ids: <span name="scala.Array" class="extype">Array</span>[<a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>, <span name="values">values: <span name="scala.Array" class="extype">Array</span>[<a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>, <span name="variableColumnName">variableColumnName: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="valueColumnName">valueColumnName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns set.</p><div class="fullcomment"><div class="comment cmt"><p>Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns set.
This is the reverse to <code>groupBy(...).pivot(...).agg(...)</code>, except for the aggregation,
which cannot be reversed.</p><p>This function is useful to massage a DataFrame into a format where some
columns are identifier columns ("ids"), while all other columns ("values")
are "unpivoted" to the rows, leaving just two non-id columns, named as given
by <code>variableColumnName</code> and <code>valueColumnName</code>.</p><pre><span class="kw">val</span> df = <span class="std">Seq</span>((<span class="num">1</span>, <span class="num">11</span>, <span class="num">12</span>L), (<span class="num">2</span>, <span class="num">21</span>, <span class="num">22</span>L)).toDF(<span class="lit">"id"</span>, <span class="lit">"int"</span>, <span class="lit">"long"</span>)
df.show()
<span class="cmt">// output:</span>
<span class="cmt">// +---+---+----+</span>
<span class="cmt">// | id|int|long|</span>
<span class="cmt">// +---+---+----+</span>
<span class="cmt">// |  1| 11|  12|</span>
<span class="cmt">// |  2| 21|  22|</span>
<span class="cmt">// +---+---+----+</span>

df.unpivot(<span class="std">Array</span>($<span class="lit">"id"</span>), <span class="std">Array</span>($<span class="lit">"int"</span>, $<span class="lit">"long"</span>), <span class="lit">"variable"</span>, <span class="lit">"value"</span>).show()
<span class="cmt">// output:</span>
<span class="cmt">// +---+--------+-----+</span>
<span class="cmt">// | id|variable|value|</span>
<span class="cmt">// +---+--------+-----+</span>
<span class="cmt">// |  1|     int|   11|</span>
<span class="cmt">// |  1|    long|   12|</span>
<span class="cmt">// |  2|     int|   21|</span>
<span class="cmt">// |  2|    long|   22|</span>
<span class="cmt">// +---+--------+-----+</span>
<span class="cmt">// schema:</span>
<span class="cmt">//root</span>
<span class="cmt">// |-- id: integer (nullable = false)</span>
<span class="cmt">// |-- variable: string (nullable = false)</span>
<span class="cmt">// |-- value: long (nullable = true)</span></pre><p>When no "id" columns are given, the unpivoted DataFrame consists of only the
"variable" and "value" columns.</p><p>All "value" columns must share a least common data type. Unless they are the same data type,
all "value" columns are cast to the nearest common data type. For instance,
types <code>IntegerType</code> and <code>LongType</code> are cast to <code>LongType</code>, while <code>IntegerType</code> and <code>StringType</code>
do not have a common data type and <code>unpivot</code> fails with an <code>AnalysisException</code>.
</p></div><dl class="paramcmts block"><dt class="param">ids</dt><dd class="cmt"><p>Id columns</p></dd><dt class="param">values</dt><dd class="cmt"><p>Value columns to unpivot</p></dd><dt class="param">variableColumnName</dt><dd class="cmt"><p>Name of the variable column</p></dd><dt class="param">valueColumnName</dt><dd class="cmt"><p>Name of the value column</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd></dl></div></li><li class="indented0 " name="scala.AnyRef#wait" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="wait(x$1:Long,x$2:Int):Unit" class="anchorToMember"></a><a id="wait(Long,Int):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#wait(x$1:Long,x$2:Int):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">wait</span><span class="params">(<span name="arg0">arg0: <span name="scala.Long" class="extype">Long</span></span>, <span name="arg1">arg1: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd><span class="name">@throws</span><span class="args">(<span><span class="defval">classOf[java.lang.InterruptedException]</span></span>)</span> </dd></dl></div></li><li class="indented0 " name="scala.AnyRef#wait" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="wait(x$1:Long):Unit" class="anchorToMember"></a><a id="wait(Long):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#wait(x$1:Long):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">wait</span><span class="params">(<span name="arg0">arg0: <span name="scala.Long" class="extype">Long</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd><span class="name">@throws</span><span class="args">(<span><span class="defval">classOf[java.lang.InterruptedException]</span></span>)</span> <span class="name">@native</span><span class="args">()</span> </dd></dl></div></li><li class="indented0 " name="scala.AnyRef#wait" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="wait():Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#wait():Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">wait</span><span class="params">()</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd><span class="name">@throws</span><span class="args">(<span><span class="defval">classOf[java.lang.InterruptedException]</span></span>)</span> </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#where" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="where(conditionExpr:String):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="where(String):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#where(conditionExpr:String):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">where</span><span class="params">(<span name="conditionExpr">conditionExpr: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Filters rows using the given SQL expression.</p><div class="fullcomment"><div class="comment cmt"><p>Filters rows using the given SQL expression.</p><pre>peopleDs.where(<span class="lit">"age &gt; 15"</span>)</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#where" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="where(condition:org.apache.spark.sql.Column):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="where(Column):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#where(condition:org.apache.spark.sql.Column):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">where</span><span class="params">(<span name="condition">condition: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Filters rows using the given condition.</p><div class="fullcomment"><div class="comment cmt"><p>Filters rows using the given condition. This is an alias for <code>filter</code>.</p><pre><span class="cmt">// The following are equivalent:</span>
peopleDs.filter($<span class="lit">"age"</span> &gt; <span class="num">15</span>)
peopleDs.where($<span class="lit">"age"</span> &gt; <span class="num">15</span>)</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#withColumn" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="withColumn(colName:String,col:org.apache.spark.sql.Column):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="withColumn(String,Column):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#withColumn(colName:String,col:org.apache.spark.sql.Column):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">withColumn</span><span class="params">(<span name="colName">colName: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="col">col: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Returns a new Dataset by adding a column or replacing the existing column that has
the same name.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by adding a column or replacing the existing column that has
the same name.</p><p><code>column</code>'s expression must only refer to attributes supplied by this Dataset. It is an
error to add a column that refers to some other Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>this method introduces a projection internally. Therefore, calling it multiple times,
for instance, via loops in order to add multiple columns can generate big plans which
can cause performance issues and even <code>StackOverflowException</code>. To avoid this,
use <code>select</code> with the multiple columns at once.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#withColumnRenamed" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="withColumnRenamed(existingName:String,newName:String):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="withColumnRenamed(String,String):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#withColumnRenamed(existingName:String,newName:String):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">withColumnRenamed</span><span class="params">(<span name="existingName">existingName: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="newName">newName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Returns a new Dataset with a column renamed.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with a column renamed.
This is a no-op if schema doesn't contain existingName.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#withColumns" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="withColumns(colsMap:java.util.Map[String,org.apache.spark.sql.Column]):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="withColumns(Map[String,Column]):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#withColumns(colsMap:java.util.Map[String,org.apache.spark.sql.Column]):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">withColumns</span><span class="params">(<span name="colsMap">colsMap: <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/Map.html#java.util.Map" name="java.util.Map" id="java.util.Map" class="extype">Map</a>[<span name="scala.Predef.String" class="extype">String</span>, <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">(Java-specific) Returns a new Dataset by adding columns or replacing the existing columns
that has the same names.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific) Returns a new Dataset by adding columns or replacing the existing columns
that has the same names.</p><p><code>colsMap</code> is a map of column name and column, the column must only refer to attribute
supplied by this Dataset. It is an error to add columns that refers to some other Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#withColumns" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="withColumns(colsMap:Map[String,org.apache.spark.sql.Column]):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="withColumns(Map[String,Column]):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#withColumns(colsMap:Map[String,org.apache.spark.sql.Column]):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">withColumns</span><span class="params">(<span name="colsMap">colsMap: <span name="scala.Predef.Map" class="extype">Map</span>[<span name="scala.Predef.String" class="extype">String</span>, <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">(Scala-specific) Returns a new Dataset by adding columns or replacing the existing columns
that has the same names.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Returns a new Dataset by adding columns or replacing the existing columns
that has the same names.</p><p><code>colsMap</code> is a map of column name and column, the column must only refer to attributes
supplied by this Dataset. It is an error to add columns that refers to some other Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#withColumnsRenamed" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="withColumnsRenamed(colsMap:java.util.Map[String,String]):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="withColumnsRenamed(Map[String,String]):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#withColumnsRenamed(colsMap:java.util.Map[String,String]):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">withColumnsRenamed</span><span class="params">(<span name="colsMap">colsMap: <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/Map.html#java.util.Map" name="java.util.Map" id="java.util.Map" class="extype">Map</a>[<span name="scala.Predef.String" class="extype">String</span>, <span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">(Java-specific)
Returns a new Dataset with a columns renamed.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific)
Returns a new Dataset with a columns renamed.
This is a no-op if schema doesn't contain existingName.</p><p><code>colsMap</code> is a map of existing column name and new column name.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#withColumnsRenamed" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="withColumnsRenamed(colsMap:Map[String,String]):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="withColumnsRenamed(Map[String,String]):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#withColumnsRenamed(colsMap:Map[String,String]):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">withColumnsRenamed</span><span class="params">(<span name="colsMap">colsMap: <span name="scala.Predef.Map" class="extype">Map</span>[<span name="scala.Predef.String" class="extype">String</span>, <span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">(Scala-specific)
Returns a new Dataset with a columns renamed.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific)
Returns a new Dataset with a columns renamed.
This is a no-op if schema doesn't contain existingName.</p><p><code>colsMap</code> is a map of existing column name and new column name.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@throws</span><span class="args">(<span><span class="defval">scala.this.throws.&lt;init&gt;$default$1[org.apache.spark.sql.AnalysisException]</span></span>)</span> </dd><dt>Since</dt><dd><p>3.4.0</p></dd><dt>Exceptions thrown</dt><dd><span class="cmt"><p><a href="AnalysisException.html" name="org.apache.spark.sql.AnalysisException" id="org.apache.spark.sql.AnalysisException" class="extype"><code>AnalysisException</code></a> if there are duplicate names in resulting projection</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#withMetadata" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="withMetadata(columnName:String,metadata:org.apache.spark.sql.types.Metadata):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="withMetadata(String,Metadata):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#withMetadata(columnName:String,metadata:org.apache.spark.sql.types.Metadata):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">withMetadata</span><span class="params">(<span name="columnName">columnName: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="metadata">metadata: <a href="types/Metadata.html" name="org.apache.spark.sql.types.Metadata" id="org.apache.spark.sql.types.Metadata" class="extype">Metadata</a></span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">Returns a new Dataset by updating an existing column with metadata.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by updating an existing column with metadata.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#withWatermark" group="streaming" fullComment="yes" data-isabs="false" visbl="pub"><a id="withWatermark(eventTime:String,delayThreshold:String):org.apache.spark.sql.Dataset[T]" class="anchorToMember"></a><a id="withWatermark(String,String):Dataset[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#withWatermark(eventTime:String,delayThreshold:String):org.apache.spark.sql.Dataset[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">withWatermark</span><span class="params">(<span name="eventTime">eventTime: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="delayThreshold">delayThreshold: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Defines an event time watermark for this <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>.</p><div class="fullcomment"><div class="comment cmt"><p>Defines an event time watermark for this <a href="" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a>. A watermark tracks a point in time
before which we assume no more late data is going to arrive.</p><p>Spark will use this watermark for several purposes:</p><ul><li>To know when a given time window aggregation can be finalized and thus can be emitted
  when using output modes that do not allow updates.</li><li>To minimize the amount of state that we need to keep for on-going aggregations,
   <code>mapGroupsWithState</code> and <code>dropDuplicates</code> operators.</li></ul><p> The current watermark is computed by looking at the <code>MAX(eventTime)</code> seen across
 all of the partitions in the query minus a user specified <code>delayThreshold</code>.  Due to the cost
 of coordinating this value across partitions, the actual watermark used is only guaranteed
 to be at least <code>delayThreshold</code> behind the actual event time.  In some cases we may still
 process records that arrive more than <code>delayThreshold</code> late.
</p></div><dl class="paramcmts block"><dt class="param">eventTime</dt><dd class="cmt"><p>the name of the column that contains the event time of the row.</p></dd><dt class="param">delayThreshold</dt><dd class="cmt"><p>the minimum delay to wait to data to arrive late, relative to the latest
                      record that has been processed in the form of an interval
                      (e.g. "1 minute" or "5 hours"). NOTE: This should not be negative.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.1.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#write" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="write:org.apache.spark.sql.DataFrameWriter[T]" class="anchorToMember"></a><a id="write:DataFrameWriter[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#write:org.apache.spark.sql.DataFrameWriter[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">write</span><span class="result">: <a href="DataFrameWriter.html" name="org.apache.spark.sql.DataFrameWriter" id="org.apache.spark.sql.DataFrameWriter" class="extype">DataFrameWriter</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Interface for saving the content of the non-streaming Dataset out into external storage.</p><div class="fullcomment"><div class="comment cmt"><p>Interface for saving the content of the non-streaming Dataset out into external storage.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#writeStream" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="writeStream:org.apache.spark.sql.streaming.DataStreamWriter[T]" class="anchorToMember"></a><a id="writeStream:DataStreamWriter[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#writeStream:org.apache.spark.sql.streaming.DataStreamWriter[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">writeStream</span><span class="result">: <a href="streaming/DataStreamWriter.html" name="org.apache.spark.sql.streaming.DataStreamWriter" id="org.apache.spark.sql.streaming.DataStreamWriter" class="extype">DataStreamWriter</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Interface for saving the content of the streaming Dataset out into external storage.</p><div class="fullcomment"><div class="comment cmt"><p>Interface for saving the content of the streaming Dataset out into external storage.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#writeTo" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="writeTo(table:String):org.apache.spark.sql.DataFrameWriterV2[T]" class="anchorToMember"></a><a id="writeTo(String):DataFrameWriterV2[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#writeTo(table:String):org.apache.spark.sql.DataFrameWriterV2[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">writeTo</span><span class="params">(<span name="table">table: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="DataFrameWriterV2.html" name="org.apache.spark.sql.DataFrameWriterV2" id="org.apache.spark.sql.DataFrameWriterV2" class="extype">DataFrameWriterV2</a>[<span name="org.apache.spark.sql.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Create a write configuration builder for v2 sources.</p><div class="fullcomment"><div class="comment cmt"><p>Create a write configuration builder for v2 sources.</p><p>This builder is used to configure and execute write operations. For example, to append to an
existing table, run:</p><pre>df.writeTo(<span class="lit">"catalog.db.table"</span>).append()</pre><p>This can also be used to create or replace existing tables:</p><pre>df.writeTo(<span class="lit">"catalog.db.table"</span>).partitionedBy($<span class="lit">"col"</span>).createOrReplace()</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>3.0.0</p></dd></dl></div></li></ol></div><div class="values members"><h3>Deprecated Value Members</h3><ol><li class="indented0 " name="org.apache.spark.sql.Dataset#explode" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="explode[A,B](inputColumn:String,outputColumn:String)(f:A=&gt;IterableOnce[B])(implicitevidence$5:reflect.runtime.universe.TypeTag[B]):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="explode[A,B](String,String)((A)=&gt;IterableOnce[B])(scala.reflect.api.JavaUniverse.TypeTag[B]):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#explode[A,B](inputColumn:String,outputColumn:String)(f:A=&gt;IterableOnce[B])(implicitevidence$5:reflect.runtime.universe.TypeTag[B]):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name deprecated" title="Deprecated: (Since version 2.0.0) use flatMap() or select() with functions.explode() instead">explode</span><span class="tparams">[<span name="A">A</span>, <span name="B">B</span>]</span><span class="params">(<span name="inputColumn">inputColumn: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="outputColumn">outputColumn: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="params">(<span name="f">f: (<span name="org.apache.spark.sql.Dataset.explode.A" class="extype">A</span>) =&gt; <span name="scala.IterableOnce" class="extype">IterableOnce</span>[<span name="org.apache.spark.sql.Dataset.explode.B" class="extype">B</span>]</span>)</span><span class="params">(<span class="implicit">implicit </span><span name="arg0">arg0: <span name="scala.reflect.api.TypeTags.TypeTag" class="extype">scala.reflect.api.JavaUniverse.TypeTag</span>[<span name="org.apache.spark.sql.Dataset.explode.B" class="extype">B</span>]</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">(Scala-specific) Returns a new Dataset where a single column has been expanded to zero
or more rows by the provided function.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Returns a new Dataset where a single column has been expanded to zero
or more rows by the provided function. This is similar to a <code>LATERAL VIEW</code> in HiveQL. All
columns of the input row are implicitly joined with each value that is output by the function.</p><p>Given that this is deprecated, as an alternative, you can explode columns either using
<code>functions.explode()</code>:</p><pre>ds.select(explode(split($<span class="lit">"words"</span>, <span class="lit">" "</span>)).as(<span class="lit">"word"</span>))</pre><p>or <code>flatMap()</code>:</p><pre>ds.flatMap(_.words.split(<span class="lit">" "</span>))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@deprecated</span> </dd><dt>Deprecated</dt><dd class="cmt"><p><i>(Since version 2.0.0)</i> use flatMap() or select() with functions.explode() instead</p></dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#explode" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="explode[A&lt;:Product](input:org.apache.spark.sql.Column*)(f:org.apache.spark.sql.Row=&gt;IterableOnce[A])(implicitevidence$4:reflect.runtime.universe.TypeTag[A]):org.apache.spark.sql.DataFrame" class="anchorToMember"></a><a id="explode[A&lt;:Product](Column*)((Row)=&gt;IterableOnce[A])(scala.reflect.api.JavaUniverse.TypeTag[A]):DataFrame" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#explode[A&lt;:Product](input:org.apache.spark.sql.Column*)(f:org.apache.spark.sql.Row=&gt;IterableOnce[A])(implicitevidence$4:reflect.runtime.universe.TypeTag[A]):org.apache.spark.sql.DataFrame" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name deprecated" title="Deprecated: (Since version 2.0.0) use flatMap() or select() with functions.explode() instead">explode</span><span class="tparams">[<span name="A">A &lt;: <span name="scala.Product" class="extype">Product</span></span>]</span><span class="params">(<span name="input">input: <a href="Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="params">(<span name="f">f: (<a href="Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>) =&gt; <span name="scala.IterableOnce" class="extype">IterableOnce</span>[<span name="org.apache.spark.sql.Dataset.explode.A" class="extype">A</span>]</span>)</span><span class="params">(<span class="implicit">implicit </span><span name="arg0">arg0: <span name="scala.reflect.api.TypeTags.TypeTag" class="extype">scala.reflect.api.JavaUniverse.TypeTag</span>[<span name="org.apache.spark.sql.Dataset.explode.A" class="extype">A</span>]</span>)</span><span class="result">: <a href="index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" name="org.apache.spark.sql.DataFrame" id="org.apache.spark.sql.DataFrame" class="extmbr">DataFrame</a></span></span><p class="shortcomment cmt">(Scala-specific) Returns a new Dataset where each row has been expanded to zero or more
rows by the provided function.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Returns a new Dataset where each row has been expanded to zero or more
rows by the provided function. This is similar to a <code>LATERAL VIEW</code> in HiveQL. The columns of
the input row are implicitly joined with each row that is output by the function.</p><p>Given that this is deprecated, as an alternative, you can explode columns either using
<code>functions.explode()</code> or <code>flatMap()</code>. The following example uses these alternatives to count
the number of books that contain a given word:</p><pre><span class="kw">case</span> <span class="kw">class</span> Book(title: <span class="std">String</span>, words: <span class="std">String</span>)
<span class="kw">val</span> ds: Dataset[Book]

<span class="kw">val</span> allWords = ds.select($<span class="lit">"title"</span>, explode(split($<span class="lit">"words"</span>, <span class="lit">" "</span>)).as(<span class="lit">"word"</span>))

<span class="kw">val</span> bookCountPerWord = allWords.groupBy(<span class="lit">"word"</span>).agg(count_distinct(<span class="lit">"title"</span>))</pre><p>Using <code>flatMap()</code> this can similarly be exploded as:</p><pre>ds.flatMap(_.words.split(<span class="lit">" "</span>))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@deprecated</span> </dd><dt>Deprecated</dt><dd class="cmt"><p><i>(Since version 2.0.0)</i> use flatMap() or select() with functions.explode() instead</p></dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="scala.AnyRef#finalize" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="prt"><a id="finalize():Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#finalize():Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name deprecated" title="Deprecated: (Since version 9)">finalize</span><span class="params">()</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Attributes</dt><dd>protected[<span name="java.lang" class="extype">lang</span>] </dd><dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd><span class="name">@throws</span><span class="args">(<span><span class="symbol">classOf[java.lang.Throwable]</span></span>)</span> <span class="name">@Deprecated</span> </dd><dt>Deprecated</dt><dd class="cmt"><p><i>(Since version 9)</i></p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.Dataset#registerTempTable" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="registerTempTable(tableName:String):Unit" class="anchorToMember"></a><a id="registerTempTable(String):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../org/apache/spark/sql/Dataset.html#registerTempTable(tableName:String):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name deprecated" title="Deprecated: (Since version 2.0.0) Use createOrReplaceTempView(viewName) instead.">registerTempTable</span><span class="params">(<span name="tableName">tableName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Registers this Dataset as a temporary table using the given name.</p><div class="fullcomment"><div class="comment cmt"><p>Registers this Dataset as a temporary table using the given name. The lifetime of this
temporary table is tied to the <a href="SparkSession.html" name="org.apache.spark.sql.SparkSession" id="org.apache.spark.sql.SparkSession" class="extype">SparkSession</a> that was used to create this Dataset.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@deprecated</span> </dd><dt>Deprecated</dt><dd class="cmt"><p><i>(Since version 2.0.0)</i> Use createOrReplaceTempView(viewName) instead.</p></dd><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li></ol></div></div><div id="inheritedMembers"><div name="java.io.Serializable" class="parent"><h3>Inherited from <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/io/Serializable.html#java.io.Serializable" name="java.io.Serializable" id="java.io.Serializable" class="extype">Serializable</a></h3></div><div name="scala.AnyRef" class="parent"><h3>Inherited from <span name="scala.AnyRef" class="extype">AnyRef</span></h3></div><div name="scala.Any" class="parent"><h3>Inherited from <span name="scala.Any" class="extype">Any</span></h3></div></div><div id="groupedMembers"><div name="action" class="group"><h3>Actions</h3></div><div name="basic" class="group"><h3>Basic Dataset functions</h3></div><div name="streaming" class="group"><h3>streaming</h3></div><div name="typedrel" class="group"><h3>Typed transformations</h3></div><div name="untypedrel" class="group"><h3>Untyped transformations</h3></div><div name="Ungrouped" class="group"><h3>Ungrouped</h3></div></div></div><div id="tooltip"></div><div id="footer"></div></body></div></div></div></body></html>
